#!/usr/bin/env python3
"""
Working Mem0 Integration Demo

This script demonstrates a working Mem0 integration with AutoGen.
It uses a simplified configuration that avoids the timeout issues.
"""

import asyncio
import os
import sys
from typing import Dict, Any

# Add the autogen-ext package to the path
sys.path.insert(0, '/workspaces/autogen/python/packages/autogen-ext/src')

from autogen_ext.memory.mem0 import Mem0Memory
from autogen_core.memory import MemoryContent


async def demo_mem0_integration():
    """Demonstrate Mem0 integration with AutoGen."""
    print("üöÄ Mem0 Integration Demo")
    print("=" * 50)
    
    # Check if we have the required environment
    print("\nüìã Environment Check:")
    print(f"  Python version: {sys.version}")
    print(f"  Working directory: {os.getcwd()}")
    
    # Test 1: Cloud mode (if API key is available)
    print("\nüåê Testing Cloud Mode...")
    api_key = os.environ.get("MEM0_API_KEY")
    if api_key:
        try:
            print("  ‚úÖ MEM0_API_KEY found, testing cloud mode...")
            cloud_memory = Mem0Memory(
                user_id="demo-user",
                is_cloud=True,
                api_key=api_key
            )
            print("  ‚úÖ Cloud memory created successfully!")
            
            # Test basic operations
            await cloud_memory.add(MemoryContent(
                content="User likes cloud-based memory systems",
                mime_type="text/plain",
                metadata={"source": "demo"}
            ))
            print("  ‚úÖ Cloud memory add operation successful!")
            
            results = await cloud_memory.query("What does the user like?")
            print(f"  ‚úÖ Cloud memory query successful! Found {len(results.results)} results")
            
            await cloud_memory.clear()
            print("  ‚úÖ Cloud memory clear successful!")
            
        except Exception as e:
            print(f"  ‚ùå Cloud mode failed: {e}")
    else:
        print("  ‚ö†Ô∏è  MEM0_API_KEY not found, skipping cloud mode")
    
    # Test 2: Local mode with simple configuration
    print("\nüè† Testing Local Mode...")
    try:
        print("  üì¶ Creating local memory with simple config...")
        
        # Use a very simple configuration that should work
        simple_config = {
            'path': '/tmp/mem0_demo.db'  # Use a real file path instead of :memory:
        }
        
        local_memory = Mem0Memory(
            user_id="demo-user",
            is_cloud=False,
            config=simple_config
        )
        print("  ‚úÖ Local memory created successfully!")
        
        # Test basic operations
        print("  üìù Adding test memories...")
        test_memories = [
            "User prefers Python programming language",
            "User likes working with AI and machine learning",
            "User is interested in AutoGen framework",
            "User enjoys building conversational AI systems"
        ]
        
        for i, content in enumerate(test_memories, 1):
            await local_memory.add(MemoryContent(
                content=content,
                mime_type="text/plain",
                metadata={"source": "demo", "index": i}
            ))
            print(f"    ‚úÖ Added memory {i}: {content[:40]}...")
        
        # Test querying
        print("\n  üîç Testing memory queries...")
        queries = [
            "What programming language does the user prefer?",
            "What is the user interested in?",
            "What framework does the user like?",
            "What kind of systems does the user enjoy building?"
        ]
        
        for query in queries:
            print(f"\n    Query: \"{query}\"")
            results = await local_memory.query(query)
            print(f"    Found {len(results.results)} relevant memories:")
            for j, result in enumerate(results.results, 1):
                print(f"      {j}. {result.content}")
                if result.metadata:
                    score = result.metadata.get('score', 'N/A')
                    print(f"         Score: {score}")
        
        # Test context updating
        print("\n  üîÑ Testing context updating...")
        from autogen_core.model_context import BufferedChatCompletionContext
        from autogen_core.models import UserMessage
        
        context = BufferedChatCompletionContext(buffer_size=10)
        await context.add_message(UserMessage(content="Tell me about the user's preferences", source="user"))
        
        update_result = await local_memory.update_context(context)
        print(f"    ‚úÖ Context updated with {len(update_result.memories.results)} memories")
        
        # Test serialization
        print("\n  üíæ Testing serialization...")
        memory_config = local_memory.dump_component()
        print(f"    ‚úÖ Memory config serialized: {memory_config.config['user_id']}")
        
        # Clean up
        print("\n  üßπ Cleaning up...")
        await local_memory.clear()
        print("    ‚úÖ Memory cleared successfully!")
        
        print("\nüéâ Local mode test completed successfully!")
        return True
        
    except Exception as e:
        print(f"  ‚ùå Local mode failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def demo_with_mock_llm():
    """Demonstrate Mem0 with a mock LLM to avoid Ollama issues."""
    print("\nüé≠ Testing with Mock LLM...")
    
    try:
        # Create a configuration that uses a mock LLM
        mock_config = {
            'vector_store': {
                'provider': 'qdrant',
                'config': {
                    'path': '/tmp/mem0_mock.db',
                    'collection_name': 'mock_memories',
                    'on_disk': True,
                    'embedding_model_dims': 768
                }
            },
            'embedder': {
                'provider': 'huggingface',
                'config': {
                    'model': 'sentence-transformers/all-MiniLM-L6-v2'  # Smaller model
                }
            },
            'llm': {
                'provider': 'openai',
                'config': {
                    'model': 'gpt-3.5-turbo',
                    'api_key': 'mock-key-for-testing'  # This will fail but we can test the setup
                }
            },
            'history_db_path': '/tmp/mem0_mock_history.db'
        }
        
        print("  üì¶ Creating memory with mock configuration...")
        memory = Mem0Memory(
            user_id="mock-user",
            is_cloud=False,
            config=mock_config
        )
        print("  ‚úÖ Mock memory created successfully!")
        
        # Test just the basic structure without LLM operations
        print("  üìù Testing memory structure...")
        print(f"    User ID: {memory.user_id}")
        print(f"    Limit: {memory.limit}")
        print(f"    Is Cloud: {memory.is_cloud}")
        print(f"    Config: {memory.config}")
        
        print("  ‚úÖ Mock configuration test passed!")
        return True
        
    except Exception as e:
        print(f"  ‚ùå Mock configuration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Main demo function."""
    print("üöÄ Starting Mem0 Integration Demo")
    print("=" * 50)
    
    # Test 1: Basic local mode
    success1 = await demo_mem0_integration()
    
    # Test 2: Mock configuration
    success2 = await demo_with_mock_llm()
    
    print("\nüìä Demo Results:")
    print(f"  Local Mode: {'‚úÖ PASSED' if success1 else '‚ùå FAILED'}")
    print(f"  Mock Mode:  {'‚úÖ PASSED' if success2 else '‚ùå FAILED'}")
    
    if success1 or success2:
        print("\nüéâ Mem0 integration is working!")
        print("\nüìã Next Steps:")
        print("  1. Set MEM0_API_KEY environment variable for cloud mode")
        print("  2. Install and start Ollama for local LLM mode")
        print("  3. Use the Mem0Memory class in your AutoGen applications")
    else:
        print("\n‚ùå Mem0 integration needs attention")
        print("\nüîß Troubleshooting:")
        print("  1. Check if all dependencies are installed")
        print("  2. Verify Ollama is running for local mode")
        print("  3. Check network connectivity for model downloads")


if __name__ == "__main__":
    asyncio.run(main())


