{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat with Retrieval Augmented Generation\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framwork allows tool use and human participance through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install pyautogen~=0.1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/anaconda3/envs/autogen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-35-turbo\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-35-turbo\", \"gpt-4\", \"gpt4\", \"gpt-4-32k\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(config_list[0][\"model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "\n",
    "llm_config = {\n",
    "    \"request_timeout\": 60,\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "\n",
    "autogen.ChatCompletion.start_logging()\n",
    "\n",
    "raguserproxy = RetrieveUserProxyAgent(\n",
    "    name=\"raguserproxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    system_message=\"A human admin.\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"groupchat\",\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    ")\n",
    "\n",
    "ragcoder = RetrieveAssistantAgent(\n",
    "    name=\"ragcoder\",\n",
    "    system_message=\"You are a senior python engineer.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"user_proxy\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "   human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "\n",
    "PROBLEM = \"How to use spark for parallel training in FLAML? Give me sample code.\"\n",
    "\n",
    "def rag_chat():\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[raguserproxy, ragcoder, pm], messages=[], max_round=12\n",
    "    )\n",
    "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "    # Start chatting with raguserproxy as this is the user proxy agent.\n",
    "    raguserproxy.initiate_chat(\n",
    "        manager,\n",
    "        problem=PROBLEM,\n",
    "        n_results=3,\n",
    "    )\n",
    "\n",
    "\n",
    "def norag_chat():\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[user_proxy, ragcoder, pm], messages=[], max_round=12\n",
    "    )\n",
    "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "    # Start chatting with user_proxy as this is the user proxy agent.\n",
    "    user_proxy.initiate_chat(\n",
    "        manager,\n",
    "        message=PROBLEM,\n",
    "    )\n",
    "\n",
    "\n",
    "def call_rag_chat():\n",
    "    # In this case, we will have multiple user proxy agents and we don't initiate the chat\n",
    "    # with RAG user proxy agent. Instead, we will initiate the chat with the user proxy agent\n",
    "    # and then the user proxy agent will call the RAG user proxy agent.\n",
    "    raguserproxy1 = RetrieveUserProxyAgent(\n",
    "        name=\"RAG_user_proxy\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        # override the default termination message of RetrieveUserProxyAgent, so it won't terminate\n",
    "        # the chat unexpectedly.\n",
    "        is_termination_msg = lambda x: x.get(\"content\") == \"TERMINATE\",\n",
    "        max_consecutive_auto_reply=3,\n",
    "        retrieve_config={\n",
    "            \"task\": \"code\",\n",
    "            \"docs_path\": \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            \"chunk_token_size\": 2000,\n",
    "            \"model\": config_list[0][\"model\"],\n",
    "            \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "            \"collection_name\": \"groupchat\",\n",
    "            \"get_or_create\": True,\n",
    "            \"n_results\": 2,\n",
    "        },\n",
    "        code_execution_config=False,  # No code execution for RAG user proxy agent since another user proxy agent will do it.\n",
    "    )\n",
    "\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[user_proxy, ragcoder, pm, raguserproxy1], messages=[], max_round=12\n",
    "    )\n",
    "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "    # Start chatting with user_proxy as this is the user proxy agent.\n",
    "    user_proxy.initiate_chat(\n",
    "        manager,\n",
    "        message=PROBLEM,\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat\n",
    "\n",
    "### UserProxyAgent doesn't get the correct code\n",
    "[FLAML](https://github.com/microsoft/FLAML) is open sourced since 2020, so ChatGPT knows it. However, spark related APIs are added in 2022, which is not in ChatGPT's training data. As a result, we end up with wrong code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "How to use spark for parallel training in FLAML? Give me sample code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "How to use spark for parallel training in FLAML? Give me sample code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To use Spark for parallel training in FLAML, you can use the `Distributed` class that is built into FLAML. This class provides a wrapper around the FLAML `AutoML` class to enable distributed training using Apache Spark. Here is an example code snippet that demonstrates how to use FLAML with Spark:\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "from flaml import AutoML\n",
      "from flaml.distributed import Distributed\n",
      "\n",
      "# Create a Spark Session\n",
      "spark = SparkSession.builder.appName(\"Flaml-Distributed-Example\").getOrCreate()\n",
      "\n",
      "# Create a Distributed object with spark session\n",
      "d = Distributed(spark)\n",
      "\n",
      "# Define your training data\n",
      "train_data = spark.sql('SELECT * FROM my_table WHERE target = 1')\n",
      "\n",
      "# Define your configuration space\n",
      "params = {'n_estimators': {'type': 'int', 'min': 10, 'max': 100},\n",
      "          'max_depth': {'type': 'int', 'min': 6, 'max': 10},\n",
      "          'learning_rate': {'type': 'float', 'min': 0.001, 'max': 0.1}}\n",
      "\n",
      "# Create the FLAML AutoML object with your configuration space\n",
      "automl = AutoML()\n",
      "\n",
      "# Train the model using the distributed object\n",
      "best_model, best_trial = automl.fit(train_data, **params, distributed=d)\n",
      "\n",
      "# Shutdown the Spark session\n",
      "spark.stop()\n",
      "```\n",
      "\n",
      "In this example, we use the `Distributed` class to create a distributed object with the Spark session. We then load our training data from a Spark table and define our configuration space. We create an instance of the `AutoML` class and proceed to train it using the `fit` method, which takes in the training data and configuration space as arguments (and the distributed object).\n",
      "\n",
      "Output: The output will be the best model and the corresponding trial that achieves the highest validation score.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "The above example demonstrates how to use Spark for parallel training in FLAML. It is from the official FLAML documentation and can be run in a Python environment with Spark and the FLAML library installed. The code loads training data from a Spark table and defines a configuration space for the FLAML AutoML algorithm to search. It then trains the model using the `Distributed` object created with the Spark session and outputs the best model and corresponding trial that achieve the highest validation score.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you for the clarification. Please let me know if you have any further questions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Is there anything else I can help you with?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Is there anything you need help with regarding the FLAML library or anything else related to data science and machine learning? I am here to assist you.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Is there anything you need help with regarding the FLAML library or anything else related to data science and machine learning? I am here to assist you.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "norag_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrieveUserProxyAgent get the correct code\n",
    "With RetrieveUserProxyAgent, we enabled retrieval augmented generation based on the given documentation file, ChatGPT can generate the correct code for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.retrieve_utils:Found 1 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_0', 'doc_4', 'doc_2']]\n",
      "\u001b[32mAdding doc_id doc_0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_4 to context.\u001b[0m\n",
      "\u001b[33mraguserproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: How to use spark for parallel training in FLAML? Give me sample code.\n",
      "\n",
      "Context is: # Integrate - Spark\n",
      "\n",
      "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
      "- Use Spark ML estimators for AutoML.\n",
      "- Use Spark to run training in parallel spark jobs.\n",
      "\n",
      "## Spark ML Estimators\n",
      "\n",
      "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
      "\n",
      "### Data\n",
      "\n",
      "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
      "\n",
      "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
      "\n",
      "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
      "- `index_col` is the column name to use as the index, default is None.\n",
      "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
      "\n",
      "Here is an example code snippet for Spark Data:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from flaml.automl.spark.utils import to_pandas_on_spark\n",
      "# Creating a dictionary\n",
      "data = {\"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
      "      \"Age_Years\": [20, 15, 10, 7, 25],\n",
      "      \"Price\": [100000, 200000, 300000, 240000, 120000]}\n",
      "\n",
      "# Creating a pandas DataFrame\n",
      "dataframe = pd.DataFrame(data)\n",
      "label = \"Price\"\n",
      "\n",
      "# Convert to pandas-on-spark dataframe\n",
      "psdf = to_pandas_on_spark(dataframe)\n",
      "```\n",
      "\n",
      "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
      "\n",
      "Here is an example of how to use it:\n",
      "```python\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "columns = psdf.columns\n",
      "feature_cols = [col for col in columns if col != label]\n",
      "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
      "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
      "```\n",
      "\n",
      "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
      "\n",
      "### Estimators\n",
      "#### Model List\n",
      "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
      "\n",
      "#### Usage\n",
      "First, prepare your data in the required format as described in the previous section.\n",
      "\n",
      "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
      "\n",
      "Here is an example code snippet using SparkML models in AutoML:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "# prepare your data in pandas-on-spark format as we previously mentioned\n",
      "\n",
      "automl = flaml.AutoML()\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
      "    \"task\": \"regression\",\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=psdf,\n",
      "    label=label,\n",
      "    **settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
      "\n",
      "## Parallel Spark Jobs\n",
      "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
      "\n",
      "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
      "\n",
      "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
      "\n",
      "\n",
      "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
      "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
      "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
      "\n",
      "An example code snippet for using parallel Spark jobs:\n",
      "```python\n",
      "import flaml\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"n_concurrent_trials\": 2,\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=dataframe,\n",
      "    label=label,\n",
      "    **automl_settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
      "\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "# for flaml.tune\n",
      "with mlflow.start_run(run_name=f\"spark_auto_trials_1686631558\"):\n",
      "    analysis = flaml.tune.run(\n",
      "        func_to_tune,\n",
      "        params,\n",
      "        metric=\"r2\",\n",
      "        mode=\"max\",\n",
      "        mlflow_exp_name=\"test_doc\",\n",
      "        use_spark=True,\n",
      "    )\n",
      "\n",
      "# for flaml.automl\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"use_spark\": True,\n",
      "    \"mlflow_exp_name\": \"test_doc\",\n",
      "    \"estimator_list\": [\n",
      "        \"lgbm\",\n",
      "        \"rf\",\n",
      "        \"xgboost\",\n",
      "        \"extra_tree\",\n",
      "        \"xgb_limitdepth\",\n",
      "    ],  # catboost does not yet support mlflow autologging\n",
      "}\n",
      "with mlflow.start_run(run_name=f\"automl_spark_trials_1686631579\"):\n",
      "    automl_experiment.fit(X_train=train_x, y_train=train_y, **automl_settings)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Results\n",
      "*Tune Autolog Trials on MLFlow UI*\n",
      "\n",
      "\n",
      "![Tune Autolog Trials on MLFlow UI](Images/tune_trials.png)\n",
      "\n",
      "\n",
      "*AutoML Autolog Trials on MLFlow UI*\n",
      "\n",
      "\n",
      "![AutoML Autolog Trials on MLFlow UI](Images/automl_trials.png)\n",
      "\n",
      "\n",
      "### Differences Between Auto and Manual Logging\n",
      "Autologging is managed by MLFlow, while manual logging is maintained by FLAML.\n",
      "\n",
      "\n",
      "#### Details of Manual Logging\n",
      "FLAML logs general artifacts for AutoML tasks. Specifically, we log these artifacts:\n",
      "\n",
      "**`flaml.tune`**\n",
      "\n",
      "\n",
      "![Manual Log Example for Tuning](Images/manual_log_tune.png)\n",
      "\n",
      "\n",
      "- We create a parent run to log the best metric and the best configuration for the entire tuning process.\n",
      "- For each trial, we create a child run to log the metric specific to the tune function and the configuration for that trial.\n",
      "\n",
      "**`flaml.automl`**\n",
      "\n",
      "\n",
      "![Manual Log Example for AutoML](Images/manual_log_automl.png)\n",
      "\n",
      "\n",
      "- We create a parent run to log the results of the experiment. This includes:\n",
      "  - The configuration of this model.\n",
      "  - The `best_validation_loss` produced by this model.\n",
      "  - The `best_iteration` to identify the point at which this model was found.\n",
      "- For each state (a specific learner with different hyperparameters), we record the best trial for this model. This includes:\n",
      "  - The configuration of the best trial.\n",
      "  - The `validation_loss` the best trial produces.\n",
      "  - The `iter_count` to identify how many trials we have conducted for this state.\n",
      "  - The `pred_time`, which is the time cost of predicting test data for this model.\n",
      "  - The `wall_clock_time`, which is the time cost of this state.\n",
      "  - The `sample_size` to show how much data we sampled in this state.\n",
      "Note that we also added these information to autolog AutoML run.\n",
      "\n",
      "\n",
      "#### Details of Autologging\n",
      "Autolog artifacts typically include model parameters, model files, and runtime metrics like the following:\n",
      "\n",
      "\n",
      "![Autolog Example](Images/autolog_example.png)\n",
      "\n",
      "\n",
      "Artifacts can differ among various machine learning libraries. More detailed information can be found [here](https://mlflow.org/docs/latest/tracking.html#automatic-logging).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Plot Experiment Result\n",
      "The `flaml.visualization` module provides utility functions for plotting the optimization process using [plotly](https://plotly.com/python/). Leveraging `plotly`, users can interactively explore experiment results. To use these plotting functions, simply provide your optimized `flaml.AutoML` or `flaml.tune.tune.ExperimentAnalysis` object as input. Optional parameters can be added using keyword arguments.\n",
      "\n",
      "Avaliable plotting functions:\n",
      "- `plot_optimization_history`: Plot optimization history of all trials in the experiment.\n",
      "- `plot_feature_importance`: Plot importance for each feature in the dataset.\n",
      "- `plot_parallel_coordinate`: Plot the high-dimensional parameter relationships in the experiment.\n",
      "- `plot_contour`: Plot the parameter relationship as contour plot in the experiment.\n",
      "- `plot_edf`: Plot the objective value EDF (empirical distribution function) of the experiment.\n",
      "- `plot_timeline`: Plot the timeline of the experiment.\n",
      "- `plot_slice`: Plot the parameter relationship as slice plot in a study.\n",
      "\n",
      "### Figure Examples\n",
      "![Plot Examples](Images/plot_samples.png)\n",
      "\n",
      "Check out our example [notebook](../../notebook/trident/automl_plot.ipynb) for a preview of all interactive plots.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32mAdding doc_id doc_4 to context.\u001b[0m\n",
      "\u001b[33mraguserproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: How to use spark for parallel training in FLAML? Give me sample code.\n",
      "\n",
      "Context is: # Integrate - Spark\n",
      "\n",
      "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
      "- Use Spark ML estimators for AutoML.\n",
      "- Use Spark to run training in parallel spark jobs.\n",
      "\n",
      "## Spark ML Estimators\n",
      "\n",
      "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
      "\n",
      "### Data\n",
      "\n",
      "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
      "\n",
      "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
      "\n",
      "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
      "- `index_col` is the column name to use as the index, default is None.\n",
      "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
      "\n",
      "Here is an example code snippet for Spark Data:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from flaml.automl.spark.utils import to_pandas_on_spark\n",
      "# Creating a dictionary\n",
      "data = {\"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
      "      \"Age_Years\": [20, 15, 10, 7, 25],\n",
      "      \"Price\": [100000, 200000, 300000, 240000, 120000]}\n",
      "\n",
      "# Creating a pandas DataFrame\n",
      "dataframe = pd.DataFrame(data)\n",
      "label = \"Price\"\n",
      "\n",
      "# Convert to pandas-on-spark dataframe\n",
      "psdf = to_pandas_on_spark(dataframe)\n",
      "```\n",
      "\n",
      "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
      "\n",
      "Here is an example of how to use it:\n",
      "```python\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "columns = psdf.columns\n",
      "feature_cols = [col for col in columns if col != label]\n",
      "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
      "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
      "```\n",
      "\n",
      "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
      "\n",
      "### Estimators\n",
      "#### Model List\n",
      "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
      "\n",
      "#### Usage\n",
      "First, prepare your data in the required format as described in the previous section.\n",
      "\n",
      "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
      "\n",
      "Here is an example code snippet using SparkML models in AutoML:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "# prepare your data in pandas-on-spark format as we previously mentioned\n",
      "\n",
      "automl = flaml.AutoML()\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
      "    \"task\": \"regression\",\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=psdf,\n",
      "    label=label,\n",
      "    **settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
      "\n",
      "## Parallel Spark Jobs\n",
      "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
      "\n",
      "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
      "\n",
      "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
      "\n",
      "\n",
      "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
      "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
      "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
      "\n",
      "An example code snippet for using parallel Spark jobs:\n",
      "```python\n",
      "import flaml\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"n_concurrent_trials\": 2,\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=dataframe,\n",
      "    label=label,\n",
      "    **automl_settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
      "\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "# for flaml.tune\n",
      "with mlflow.start_run(run_name=f\"spark_auto_trials_1686631558\"):\n",
      "    analysis = flaml.tune.run(\n",
      "        func_to_tune,\n",
      "        params,\n",
      "        metric=\"r2\",\n",
      "        mode=\"max\",\n",
      "        mlflow_exp_name=\"test_doc\",\n",
      "        use_spark=True,\n",
      "    )\n",
      "\n",
      "# for flaml.automl\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"use_spark\": True,\n",
      "    \"mlflow_exp_name\": \"test_doc\",\n",
      "    \"estimator_list\": [\n",
      "        \"lgbm\",\n",
      "        \"rf\",\n",
      "        \"xgboost\",\n",
      "        \"extra_tree\",\n",
      "        \"xgb_limitdepth\",\n",
      "    ],  # catboost does not yet support mlflow autologging\n",
      "}\n",
      "with mlflow.start_run(run_name=f\"automl_spark_trials_1686631579\"):\n",
      "    automl_experiment.fit(X_train=train_x, y_train=train_y, **automl_settings)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Results\n",
      "*Tune Autolog Trials on MLFlow UI*\n",
      "\n",
      "\n",
      "![Tune Autolog Trials on MLFlow UI](Images/tune_trials.png)\n",
      "\n",
      "\n",
      "*AutoML Autolog Trials on MLFlow UI*\n",
      "\n",
      "\n",
      "![AutoML Autolog Trials on MLFlow UI](Images/automl_trials.png)\n",
      "\n",
      "\n",
      "### Differences Between Auto and Manual Logging\n",
      "Autologging is managed by MLFlow, while manual logging is maintained by FLAML.\n",
      "\n",
      "\n",
      "#### Details of Manual Logging\n",
      "FLAML logs general artifacts for AutoML tasks. Specifically, we log these artifacts:\n",
      "\n",
      "**`flaml.tune`**\n",
      "\n",
      "\n",
      "![Manual Log Example for Tuning](Images/manual_log_tune.png)\n",
      "\n",
      "\n",
      "- We create a parent run to log the best metric and the best configuration for the entire tuning process.\n",
      "- For each trial, we create a child run to log the metric specific to the tune function and the configuration for that trial.\n",
      "\n",
      "**`flaml.automl`**\n",
      "\n",
      "\n",
      "![Manual Log Example for AutoML](Images/manual_log_automl.png)\n",
      "\n",
      "\n",
      "- We create a parent run to log the results of the experiment. This includes:\n",
      "  - The configuration of this model.\n",
      "  - The `best_validation_loss` produced by this model.\n",
      "  - The `best_iteration` to identify the point at which this model was found.\n",
      "- For each state (a specific learner with different hyperparameters), we record the best trial for this model. This includes:\n",
      "  - The configuration of the best trial.\n",
      "  - The `validation_loss` the best trial produces.\n",
      "  - The `iter_count` to identify how many trials we have conducted for this state.\n",
      "  - The `pred_time`, which is the time cost of predicting test data for this model.\n",
      "  - The `wall_clock_time`, which is the time cost of this state.\n",
      "  - The `sample_size` to show how much data we sampled in this state.\n",
      "Note that we also added these information to autolog AutoML run.\n",
      "\n",
      "\n",
      "#### Details of Autologging\n",
      "Autolog artifacts typically include model parameters, model files, and runtime metrics like the following:\n",
      "\n",
      "\n",
      "![Autolog Example](Images/autolog_example.png)\n",
      "\n",
      "\n",
      "Artifacts can differ among various machine learning libraries. More detailed information can be found [here](https://mlflow.org/docs/latest/tracking.html#automatic-logging).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Plot Experiment Result\n",
      "The `flaml.visualization` module provides utility functions for plotting the optimization process using [plotly](https://plotly.com/python/). Leveraging `plotly`, users can interactively explore experiment results. To use these plotting functions, simply provide your optimized `flaml.AutoML` or `flaml.tune.tune.ExperimentAnalysis` object as input. Optional parameters can be added using keyword arguments.\n",
      "\n",
      "Avaliable plotting functions:\n",
      "- `plot_optimization_history`: Plot optimization history of all trials in the experiment.\n",
      "- `plot_feature_importance`: Plot importance for each feature in the dataset.\n",
      "- `plot_parallel_coordinate`: Plot the high-dimensional parameter relationships in the experiment.\n",
      "- `plot_contour`: Plot the parameter relationship as contour plot in the experiment.\n",
      "- `plot_edf`: Plot the objective value EDF (empirical distribution function) of the experiment.\n",
      "- `plot_timeline`: Plot the timeline of the experiment.\n",
      "- `plot_slice`: Plot the parameter relationship as slice plot in a study.\n",
      "\n",
      "### Figure Examples\n",
      "![Plot Examples](Images/plot_samples.png)\n",
      "\n",
      "Check out our example [notebook](../../notebook/trident/automl_plot.ipynb) for a preview of all interactive plots.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Here are some ideas for software products:\n",
      "\n",
      "1. \"Smart Email Classifier\" - an email client that uses machine learning algorithms to automatically categorize your emails into categories like important, personal, work, and so on. It would also allow users to create their own custom categories based on their needs. This would help users save time and become more organized with their inbox.\n",
      "\n",
      "2. \"Instant Grocery Delivery\" - a grocery delivery app that uses real-time data to predict when a customer is running low on a certain item and automatically adds it to their shopping list. The app would have partnerships with local grocery stores and use data from previous purchases to recommend items to customers based on their preferences.\n",
      "\n",
      "3. \"Virtual Personal Stylist\" - a software product that uses augmented reality to help customers virtually try on clothes and accessories before purchasing them. The app would also use machine learning algorithms to recommend items that would complement the customer's existing wardrobe.\n",
      "\n",
      "4. \"Eco-Friendly Home Assistant\" - a smart home system that uses machine learning algorithms to optimize energy usage and reduce waste. The system would automatically adjust temperature settings based on the user's preferences and daily routine and suggest ways to reduce energy consumption.\n",
      "\n",
      "5. \"Automated Investment Advisor\" - a platform that uses machine learning algorithms to analyze financial data and provide personalized investment advice to customers. The platform would analyze customer's risk tolerance, financial goals, and investment history to provide customized recommendations on where to invest.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mraguserproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "How to use spark for parallel training in FLAML? Give me sample code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
      "\n",
      "- Use Spark ML estimators for AutoML.\n",
      "- Use Spark to run training in parallel spark jobs.\n",
      "\n",
      "For parallel training in FLAML, you can activate Spark as the parallel backend during parallel tuning in both AutoML and Hyperparameter Tuning, by setting the `use_spark` to `true`. To use Spark ML models you first need to organize your data in the required format using `flaml.automl.spark.utils` module. Here is an example code snippet for Spark Data:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from flaml.automl.spark.utils import to_pandas_on_spark\n",
      "# Creating a dictionary\n",
      "data = {\"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
      "      \"Age_Years\": [20, 15, 10, 7, 25],\n",
      "      \"Price\": [100000, 200000, 300000, 240000, 120000]}\n",
      "\n",
      "# Creating a pandas DataFrame\n",
      "dataframe = pd.DataFrame(data)\n",
      "label = \"Price\"\n",
      "\n",
      "# Convert to pandas-on-spark dataframe\n",
      "psdf = to_pandas_on_spark(dataframe)\n",
      "```\n",
      "\n",
      "Here is an example code for parallel training using `AutoML`:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "\n",
      "automl = flaml.AutoML()\n",
      "settings = {\n",
      "    \"time_budget\": 30, # in seconds\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"n_jobs\":-1, # number of cores -1\n",
      "    \"use_spark\": True,\n",
      "}\n",
      "\n",
      "automl_settings={\n",
      "    \"dataframe\": psdf,\n",
      "    \"label\": label,\n",
      "    **settings\n",
      "}\n",
      "automl.fit(**automl_settings)\n",
      "```\n",
      "\n",
      "Here is an example code for parallel training using `flaml.tune`:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "\n",
      "def func_to_tune(config):\n",
      "    ...\n",
      "    return score\n",
      "\n",
      "params = {'myparam':[1,50]}\n",
      "\n",
      "with mlflow.start_run(run_name=\"flaml_tune_auto_example_spark\"):\n",
      "    analysis = flaml.tune.run(func_to_tune,params,metric='r2',mode='max',use_spark=True)\n",
      "```\n",
      "\n",
      "Have a look at [this documentation](https://github.com/microsoft/FLAML/blob/main/docs/Use-Cases/Integrate-Spark.md) to know more about how to integrate and use Spark with FLAML.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Great answer! Just a quick note that for the `AutoML` example, the `n_jobs` parameter may not be necessary when using Spark, as it will automatically distribute the workload using Spark. Also, I think the `time_budget` parameter specifies the total wall-clock time FLAML should use for training, not the number of Spark jobs to run.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rag_chat()\n",
    "# type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call RetrieveUserProxyAgent while init chat with another user proxy agent\n",
    "Sometimes, we want to use RetrieveUserProxyAgent in the group chat, but we don't want to init the chat with RetrieveUserProxyAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "How to use spark for parallel training in FLAML? Give me sample code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To use Spark for parallel training in FLAML, you can use the `Distributed` class that is built into FLAML. This class provides a wrapper around the FLAML `AutoML` class to enable distributed training using Apache Spark. Here is an example code snippet that demonstrates how to use FLAML with Spark:\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "from flaml import AutoML\n",
      "from flaml.distributed import Distributed\n",
      "\n",
      "# Create a Spark Session\n",
      "spark = SparkSession.builder.appName(\"Flaml-Distributed-Example\").getOrCreate()\n",
      "\n",
      "# Create a Distributed object with spark session\n",
      "d = Distributed(spark)\n",
      "\n",
      "# Define your training data\n",
      "train_data = spark.sql('SELECT * FROM my_table WHERE target = 1')\n",
      "\n",
      "# Define your configuration space\n",
      "params = {'n_estimators': {'type': 'int', 'min': 10, 'max': 100},\n",
      "          'max_depth': {'type': 'int', 'min': 6, 'max': 10},\n",
      "          'learning_rate': {'type': 'float', 'min': 0.001, 'max': 0.1}}\n",
      "\n",
      "# Create the FLAML AutoML object with your configuration space\n",
      "automl = AutoML()\n",
      "\n",
      "# Train the model using the distributed object\n",
      "best_model, best_trial = automl.fit(train_data, **params, distributed=d)\n",
      "\n",
      "# Shutdown the Spark session\n",
      "spark.stop()\n",
      "```\n",
      "\n",
      "In this example, we use the `Distributed` class to create a distributed object with the Spark session. We then load our training data from a Spark table and define our configuration space. We create an instance of the `AutoML` class and proceed to train it using the `fit` method, which takes in the training data and configuration space as arguments (and the distributed object).\n",
      "\n",
      "Output: The output will be the best model and the corresponding trial that achieves the highest validation score.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Note that to use the `flaml.distributed` module, you need to have a working Apache Spark cluster. You also need to install PySpark and set up the `SPARK_HOME` environment variable before running the code above. This example assumes that you have a Spark cluster running locally. If you want to run the code on a remote cluster, you need to modify the `master` parameter when creating the `SparkSession` object.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.retrieve_utils:Found 1 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n",
      "doc_ids:  [['doc_0', 'doc_2']]\n",
      "\u001b[32mAdding doc_id doc_0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_2 to context.\u001b[0m\n",
      "\u001b[33mRAG_user_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Note that to use the `flaml.distributed` module, you need to have a working Apache Spark cluster. You also need to install PySpark and set up the `SPARK_HOME` environment variable before running the code above. This example assumes that you have a Spark cluster running locally. If you want to run the code on a remote cluster, you need to modify the `master` parameter when creating the `SparkSession` object.\n",
      "\n",
      "Context is: # Integrate - Spark\n",
      "\n",
      "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
      "- Use Spark ML estimators for AutoML.\n",
      "- Use Spark to run training in parallel spark jobs.\n",
      "\n",
      "## Spark ML Estimators\n",
      "\n",
      "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
      "\n",
      "### Data\n",
      "\n",
      "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
      "\n",
      "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
      "\n",
      "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
      "- `index_col` is the column name to use as the index, default is None.\n",
      "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
      "\n",
      "Here is an example code snippet for Spark Data:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from flaml.automl.spark.utils import to_pandas_on_spark\n",
      "# Creating a dictionary\n",
      "data = {\"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
      "      \"Age_Years\": [20, 15, 10, 7, 25],\n",
      "      \"Price\": [100000, 200000, 300000, 240000, 120000]}\n",
      "\n",
      "# Creating a pandas DataFrame\n",
      "dataframe = pd.DataFrame(data)\n",
      "label = \"Price\"\n",
      "\n",
      "# Convert to pandas-on-spark dataframe\n",
      "psdf = to_pandas_on_spark(dataframe)\n",
      "```\n",
      "\n",
      "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
      "\n",
      "Here is an example of how to use it:\n",
      "```python\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "columns = psdf.columns\n",
      "feature_cols = [col for col in columns if col != label]\n",
      "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
      "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
      "```\n",
      "\n",
      "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
      "\n",
      "### Estimators\n",
      "#### Model List\n",
      "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
      "\n",
      "#### Usage\n",
      "First, prepare your data in the required format as described in the previous section.\n",
      "\n",
      "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
      "\n",
      "Here is an example code snippet using SparkML models in AutoML:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "# prepare your data in pandas-on-spark format as we previously mentioned\n",
      "\n",
      "automl = flaml.AutoML()\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
      "    \"task\": \"regression\",\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=psdf,\n",
      "    label=label,\n",
      "    **settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
      "\n",
      "## Parallel Spark Jobs\n",
      "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
      "\n",
      "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
      "\n",
      "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
      "\n",
      "\n",
      "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
      "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
      "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
      "\n",
      "An example code snippet for using parallel Spark jobs:\n",
      "```python\n",
      "import flaml\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"n_concurrent_trials\": 2,\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=dataframe,\n",
      "    label=label,\n",
      "    **automl_settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
      "\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=dataframe,\n",
      "    label=label,\n",
      "    **automl_settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
      "\n",
      "\n",
      "\n",
      "### Feature List\n",
      "- Extra models supported for AutoML.\n",
      "\n",
      "### SparkML Models\n",
      "\n",
      "#### Model List\n",
      "Note: Estimator name in **`bold`** is activated by default.\n",
      "\n",
      "- **`lgbm_spark`**: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API. This model is also available in open-source FLAML.\n",
      "- **`rf_spark`**: Random Forest [Classifier](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html#pyspark.ml.classification.RandomForestClassifier) and [Regressor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html#pyspark.ml.regression.RandomForestRegressor) APIs in `pyspark.ml`.\n",
      "- `gbt_spark`: Gradient-Boosted Trees (GBT) [Classifier](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.GBTClassifier.html) and [Regressor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GBTRegressor.html) APIs in `pyspark.ml`. Note that employ GBTClassifier only support binary classification task.\n",
      "- `nb_spark`: [Naive Bayes Classifier](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.NaiveBayes.html) for classification task only.\n",
      "- `glr_spark`: [Generalized Linear Regression ](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GeneralizedLinearRegression.html) for regression task only.\n",
      "- `lr_spark`: [Linear Regression](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html) for regression task only.\n",
      "- `svc_spark`: [Linear SVC](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LinearSVC.html) for binary classification task only.\n",
      "- `aft_spark`: [Accelerated Failure Time (AFT) Model Survival Regression](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.AFTSurvivalRegression.html). This estimator only support survival analysis task, which is a regression task that requires an extra `censorCol` argument.\n",
      "#### Usage\n",
      "First, prepare your data in the required format as described in the previous section.\n",
      "\n",
      "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
      "\n",
      "Here is an example code snippet using SparkML models in AutoML:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "# prepare your data in pandas-on-spark format as we previously mentioned\n",
      "\n",
      "automl = flaml.AutoML()\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"estimator_list\": [\"lgbm_spark\", \"rf_spark\"],  # this setting is optional\n",
      "    \"task\": \"regression\",\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=psdf,\n",
      "    label=label,\n",
      "    **settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
      "\n",
      "\n",
      "### Non-spark Models\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32mUpdating context and resetting conversation.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 6 is greater than number of elements in index 5, updating n_results = 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_0', 'doc_2', 'doc_4', 'doc_1', 'doc_3']]\n",
      "\u001b[32mAdding doc_id doc_4 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_1 to context.\u001b[0m\n",
      "\u001b[33mRAG_user_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Note that to use the `flaml.distributed` module, you need to have a working Apache Spark cluster. You also need to install PySpark and set up the `SPARK_HOME` environment variable before running the code above. This example assumes that you have a Spark cluster running locally. If you want to run the code on a remote cluster, you need to modify the `master` parameter when creating the `SparkSession` object.\n",
      "\n",
      "Context is: \n",
      "```python\n",
      "import flaml\n",
      "# for flaml.tune\n",
      "with mlflow.start_run(run_name=f\"spark_auto_trials_1686631558\"):\n",
      "    analysis = flaml.tune.run(\n",
      "        func_to_tune,\n",
      "        params,\n",
      "        metric=\"r2\",\n",
      "        mode=\"max\",\n",
      "        mlflow_exp_name=\"test_doc\",\n",
      "        use_spark=True,\n",
      "    )\n",
      "\n",
      "# for flaml.automl\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"use_spark\": True,\n",
      "    \"mlflow_exp_name\": \"test_doc\",\n",
      "    \"estimator_list\": [\n",
      "        \"lgbm\",\n",
      "        \"rf\",\n",
      "        \"xgboost\",\n",
      "        \"extra_tree\",\n",
      "        \"xgb_limitdepth\",\n",
      "    ],  # catboost does not yet support mlflow autologging\n",
      "}\n",
      "with mlflow.start_run(run_name=f\"automl_spark_trials_1686631579\"):\n",
      "    automl_experiment.fit(X_train=train_x, y_train=train_y, **automl_settings)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Results\n",
      "*Tune Autolog Trials on MLFlow UI*\n",
      "\n",
      "\n",
      "![Tune Autolog Trials on MLFlow UI](Images/tune_trials.png)\n",
      "\n",
      "\n",
      "*AutoML Autolog Trials on MLFlow UI*\n",
      "\n",
      "\n",
      "![AutoML Autolog Trials on MLFlow UI](Images/automl_trials.png)\n",
      "\n",
      "\n",
      "### Differences Between Auto and Manual Logging\n",
      "Autologging is managed by MLFlow, while manual logging is maintained by FLAML.\n",
      "\n",
      "\n",
      "#### Details of Manual Logging\n",
      "FLAML logs general artifacts for AutoML tasks. Specifically, we log these artifacts:\n",
      "\n",
      "**`flaml.tune`**\n",
      "\n",
      "\n",
      "![Manual Log Example for Tuning](Images/manual_log_tune.png)\n",
      "\n",
      "\n",
      "- We create a parent run to log the best metric and the best configuration for the entire tuning process.\n",
      "- For each trial, we create a child run to log the metric specific to the tune function and the configuration for that trial.\n",
      "\n",
      "**`flaml.automl`**\n",
      "\n",
      "\n",
      "![Manual Log Example for AutoML](Images/manual_log_automl.png)\n",
      "\n",
      "\n",
      "- We create a parent run to log the results of the experiment. This includes:\n",
      "  - The configuration of this model.\n",
      "  - The `best_validation_loss` produced by this model.\n",
      "  - The `best_iteration` to identify the point at which this model was found.\n",
      "- For each state (a specific learner with different hyperparameters), we record the best trial for this model. This includes:\n",
      "  - The configuration of the best trial.\n",
      "  - The `validation_loss` the best trial produces.\n",
      "  - The `iter_count` to identify how many trials we have conducted for this state.\n",
      "  - The `pred_time`, which is the time cost of predicting test data for this model.\n",
      "  - The `wall_clock_time`, which is the time cost of this state.\n",
      "  - The `sample_size` to show how much data we sampled in this state.\n",
      "Note that we also added these information to autolog AutoML run.\n",
      "\n",
      "\n",
      "#### Details of Autologging\n",
      "Autolog artifacts typically include model parameters, model files, and runtime metrics like the following:\n",
      "\n",
      "\n",
      "![Autolog Example](Images/autolog_example.png)\n",
      "\n",
      "\n",
      "Artifacts can differ among various machine learning libraries. More detailed information can be found [here](https://mlflow.org/docs/latest/tracking.html#automatic-logging).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Plot Experiment Result\n",
      "The `flaml.visualization` module provides utility functions for plotting the optimization process using [plotly](https://plotly.com/python/). Leveraging `plotly`, users can interactively explore experiment results. To use these plotting functions, simply provide your optimized `flaml.AutoML` or `flaml.tune.tune.ExperimentAnalysis` object as input. Optional parameters can be added using keyword arguments.\n",
      "\n",
      "Avaliable plotting functions:\n",
      "- `plot_optimization_history`: Plot optimization history of all trials in the experiment.\n",
      "- `plot_feature_importance`: Plot importance for each feature in the dataset.\n",
      "- `plot_parallel_coordinate`: Plot the high-dimensional parameter relationships in the experiment.\n",
      "- `plot_contour`: Plot the parameter relationship as contour plot in the experiment.\n",
      "- `plot_edf`: Plot the objective value EDF (empirical distribution function) of the experiment.\n",
      "- `plot_timeline`: Plot the timeline of the experiment.\n",
      "- `plot_slice`: Plot the parameter relationship as slice plot in a study.\n",
      "\n",
      "### Figure Examples\n",
      "![Plot Examples](Images/plot_samples.png)\n",
      "\n",
      "Check out our example [notebook](../../notebook/trident/automl_plot.ipynb) for a preview of all interactive plots.\n",
      "\n",
      "\n",
      "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
      "\n",
      "Here is an example of how to use it:\n",
      "```python\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "columns = psdf.columns\n",
      "feature_cols = [col for col in columns if col != label]\n",
      "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
      "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
      "```\n",
      "\n",
      "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
      "\n",
      "## Tuning Function\n",
      "Same as in open-source FLAML, a tutorial can be found [here](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).\n",
      "\n",
      "\n",
      "## Configure Experiment\n",
      "Detailed documentation about customizing experiments can be found here: [Hyperparameter Tuning](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function) & [AutoML](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML/).\n",
      "\n",
      "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](../Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](../Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
      "\n",
      "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
      "\n",
      "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
      "\n",
      "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
      "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
      "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
      "\n",
      "\n",
      "Below are some additional commonly used parameters for AutoML:\n",
      "\n",
      "- `estimator_list`: A list of strings for estimator names, or 'auto'. e.g., ```['lgbm', 'xgboost', 'xgb_limitdepth', 'catboost', 'rf', 'extra_tree']```.\n",
      "- `time_budget`: A float number of the time budget in seconds. Use -1 if no time limit.\n",
      "- `max_iter`: An integer of the maximal number of iterations. NOTE: when both time_budget and max_iter are unspecified, only one model will be trained per estimator.\n",
      "- `log_type`: A string of the log type, one of ['better', 'all']. Default is 'better', only logs configs with better loss than previous iters; 'all' logs all the tried configs.\n",
      "\n",
      "\n",
      "An example code snippet for using parallel Spark jobs:\n",
      "```python\n",
      "import flaml\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"n_concurrent_trials\": 2,\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
      "    \"log_type\": \"all\",  # flaml only logs better configs than the previous iters by default, set to \"all\" to log all trials\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32mAdding doc_id doc_1 to context.\u001b[0m\n",
      "\u001b[33mRAG_user_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Note that to use the `flaml.distributed` module, you need to have a working Apache Spark cluster. You also need to install PySpark and set up the `SPARK_HOME` environment variable before running the code above. This example assumes that you have a Spark cluster running locally. If you want to run the code on a remote cluster, you need to modify the `master` parameter when creating the `SparkSession` object.\n",
      "\n",
      "Context is: \n",
      "```python\n",
      "import flaml\n",
      "# for flaml.tune\n",
      "with mlflow.start_run(run_name=f\"spark_auto_trials_1686631558\"):\n",
      "    analysis = flaml.tune.run(\n",
      "        func_to_tune,\n",
      "        params,\n",
      "        metric=\"r2\",\n",
      "        mode=\"max\",\n",
      "        mlflow_exp_name=\"test_doc\",\n",
      "        use_spark=True,\n",
      "    )\n",
      "\n",
      "# for flaml.automl\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"use_spark\": True,\n",
      "    \"mlflow_exp_name\": \"test_doc\",\n",
      "    \"estimator_list\": [\n",
      "        \"lgbm\",\n",
      "        \"rf\",\n",
      "        \"xgboost\",\n",
      "        \"extra_tree\",\n",
      "        \"xgb_limitdepth\",\n",
      "    ],  # catboost does not yet support mlflow autologging\n",
      "}\n",
      "with mlflow.start_run(run_name=f\"automl_spark_trials_1686631579\"):\n",
      "    automl_experiment.fit(X_train=train_x, y_train=train_y, **automl_settings)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Results\n",
      "*Tune Autolog Trials on MLFlow UI*\n",
      "\n",
      "\n",
      "![Tune Autolog Trials on MLFlow UI](Images/tune_trials.png)\n",
      "\n",
      "\n",
      "*AutoML Autolog Trials on MLFlow UI*\n",
      "\n",
      "\n",
      "![AutoML Autolog Trials on MLFlow UI](Images/automl_trials.png)\n",
      "\n",
      "\n",
      "### Differences Between Auto and Manual Logging\n",
      "Autologging is managed by MLFlow, while manual logging is maintained by FLAML.\n",
      "\n",
      "\n",
      "#### Details of Manual Logging\n",
      "FLAML logs general artifacts for AutoML tasks. Specifically, we log these artifacts:\n",
      "\n",
      "**`flaml.tune`**\n",
      "\n",
      "\n",
      "![Manual Log Example for Tuning](Images/manual_log_tune.png)\n",
      "\n",
      "\n",
      "- We create a parent run to log the best metric and the best configuration for the entire tuning process.\n",
      "- For each trial, we create a child run to log the metric specific to the tune function and the configuration for that trial.\n",
      "\n",
      "**`flaml.automl`**\n",
      "\n",
      "\n",
      "![Manual Log Example for AutoML](Images/manual_log_automl.png)\n",
      "\n",
      "\n",
      "- We create a parent run to log the results of the experiment. This includes:\n",
      "  - The configuration of this model.\n",
      "  - The `best_validation_loss` produced by this model.\n",
      "  - The `best_iteration` to identify the point at which this model was found.\n",
      "- For each state (a specific learner with different hyperparameters), we record the best trial for this model. This includes:\n",
      "  - The configuration of the best trial.\n",
      "  - The `validation_loss` the best trial produces.\n",
      "  - The `iter_count` to identify how many trials we have conducted for this state.\n",
      "  - The `pred_time`, which is the time cost of predicting test data for this model.\n",
      "  - The `wall_clock_time`, which is the time cost of this state.\n",
      "  - The `sample_size` to show how much data we sampled in this state.\n",
      "Note that we also added these information to autolog AutoML run.\n",
      "\n",
      "\n",
      "#### Details of Autologging\n",
      "Autolog artifacts typically include model parameters, model files, and runtime metrics like the following:\n",
      "\n",
      "\n",
      "![Autolog Example](Images/autolog_example.png)\n",
      "\n",
      "\n",
      "Artifacts can differ among various machine learning libraries. More detailed information can be found [here](https://mlflow.org/docs/latest/tracking.html#automatic-logging).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Plot Experiment Result\n",
      "The `flaml.visualization` module provides utility functions for plotting the optimization process using [plotly](https://plotly.com/python/). Leveraging `plotly`, users can interactively explore experiment results. To use these plotting functions, simply provide your optimized `flaml.AutoML` or `flaml.tune.tune.ExperimentAnalysis` object as input. Optional parameters can be added using keyword arguments.\n",
      "\n",
      "Avaliable plotting functions:\n",
      "- `plot_optimization_history`: Plot optimization history of all trials in the experiment.\n",
      "- `plot_feature_importance`: Plot importance for each feature in the dataset.\n",
      "- `plot_parallel_coordinate`: Plot the high-dimensional parameter relationships in the experiment.\n",
      "- `plot_contour`: Plot the parameter relationship as contour plot in the experiment.\n",
      "- `plot_edf`: Plot the objective value EDF (empirical distribution function) of the experiment.\n",
      "- `plot_timeline`: Plot the timeline of the experiment.\n",
      "- `plot_slice`: Plot the parameter relationship as slice plot in a study.\n",
      "\n",
      "### Figure Examples\n",
      "![Plot Examples](Images/plot_samples.png)\n",
      "\n",
      "Check out our example [notebook](../../notebook/trident/automl_plot.ipynb) for a preview of all interactive plots.\n",
      "\n",
      "\n",
      "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
      "\n",
      "Here is an example of how to use it:\n",
      "```python\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "columns = psdf.columns\n",
      "feature_cols = [col for col in columns if col != label]\n",
      "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
      "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
      "```\n",
      "\n",
      "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
      "\n",
      "## Tuning Function\n",
      "Same as in open-source FLAML, a tutorial can be found [here](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).\n",
      "\n",
      "\n",
      "## Configure Experiment\n",
      "Detailed documentation about customizing experiments can be found here: [Hyperparameter Tuning](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function) & [AutoML](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML/).\n",
      "\n",
      "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](../Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](../Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
      "\n",
      "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
      "\n",
      "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
      "\n",
      "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
      "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
      "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
      "\n",
      "\n",
      "Below are some additional commonly used parameters for AutoML:\n",
      "\n",
      "- `estimator_list`: A list of strings for estimator names, or 'auto'. e.g., ```['lgbm', 'xgboost', 'xgb_limitdepth', 'catboost', 'rf', 'extra_tree']```.\n",
      "- `time_budget`: A float number of the time budget in seconds. Use -1 if no time limit.\n",
      "- `max_iter`: An integer of the maximal number of iterations. NOTE: when both time_budget and max_iter are unspecified, only one model will be trained per estimator.\n",
      "- `log_type`: A string of the log type, one of ['better', 'all']. Default is 'better', only logs configs with better loss than previous iters; 'all' logs all the tried configs.\n",
      "\n",
      "\n",
      "An example code snippet for using parallel Spark jobs:\n",
      "```python\n",
      "import flaml\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"n_concurrent_trials\": 2,\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
      "    \"log_type\": \"all\",  # flaml only logs better configs than the previous iters by default, set to \"all\" to log all trials\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32mUpdating context and resetting conversation.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_3 to context.\u001b[0m\n",
      "\u001b[33mRAG_user_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Note that to use the `flaml.distributed` module, you need to have a working Apache Spark cluster. You also need to install PySpark and set up the `SPARK_HOME` environment variable before running the code above. This example assumes that you have a Spark cluster running locally. If you want to run the code on a remote cluster, you need to modify the `master` parameter when creating the `SparkSession` object.\n",
      "\n",
      "Context is: \n",
      "#### Model List\n",
      "Note: Estimator name in **`bold`** is activated by default.\n",
      "- **`sgd`**: Stoachastic Gradient Descent [Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) and [Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) for both classification and regression task.\n",
      "- `svc`: [Linear Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) for classification task only.\n",
      "- `enet`: [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) for regression task only.\n",
      "- `lassolars`: [Lasso model fit with Least Angle Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html) for regression task and time-series forecast task.\n",
      "- `tcn`: [Temporal Convolutional Networks](https://github.com/locuslab/TCN/) for time-series forecast task. This model is implemented in PyTorch and could use GPUs to acclerate.\n",
      "- `snaive, naive, savg, avg`: Four simple models SeasonalNaive, Naive, SeasonalAverage, Average for time-series forecast task. Details about algorithm behind these models could be found [here](https://otexts.com/fpp2/simple-methods.html).\n",
      "\n",
      "#### Usage\n",
      "These models consume regular data like Pandas Dataframe, and usage is like all other estimators in [AutoML](Use-Cases/Task-Oriented-AutoML#estimator-and-search-space)\n",
      "## MLFlow Integration\n",
      "Our internal MLFlow integration enhances FLAML's ability to effectively log your experiments using `flaml.tune` or `flaml.automl`.\n",
      "\n",
      "### Main Features\n",
      "- Logging of distributed experiments on Spark.\n",
      "- Organization of your experiment trials into nested runs and renaming of child runs.\n",
      "\n",
      "### Quick Start\n",
      "\n",
      "```python\n",
      "import mlflow\n",
      "```\n",
      "First, you need to enable MLFlow autologging. We support both MLFlow autologging and manual logging. If your machine learning tools are not yet supported by `mlflow.autolog`, or if you have specific requirements, you can skip this step.\n",
      "\n",
      "```python\n",
      "mlflow.autolog()\n",
      "```\n",
      "Next, you can manually start the parent run outside of `flaml.tune` or `flaml.automl`.\n",
      "\n",
      "Once you manually start a parent run, the MLFlow integration feature will be enabled, regardless of whether auto logging or manual logging is used.\n",
      "\n",
      "Spark support will be automatically enabled once you set `use_spark=True`.\n",
      "\n",
      "* You can specify the `run_name` argument for `mlflow.start_run()`. Child runs will be renamed according to the parent run's name.\n",
      "* You can specify `mlflow_exp_name` in `flaml.tune` or `flaml.automl`. If you leave it blank, FLAML will detect the current experiment for you.\n",
      "\n",
      "Retrain process will be disabled if you switch on `mlflow.autolog` but not start parent run. If you start parent run, retrain process will be utilized to log best run information to your parent run. If not retrained, FLAML will copy run information from best child run to parent run.\n",
      "\n",
      "FLAML will log run tags to mlflow runs:\n",
      "  - `flaml.best_run`: A bool flag to identify best run in current experiment.\n",
      "  - `flaml.run_source`: The type of current experiment. The value could be:\n",
      "    - `flaml-automl` for AutoML experiment.\n",
      "    - `flaml-tune` for Hyperparameter Tuning experiment.\n",
      "  - `flaml.version`: Current version of FLAML.\n",
      "  - `flaml.metric`: The metric name used in current experiment. For example `\"r2\"`.\n",
      "  - `flaml.iteration_number`: The index of current run in the experiment.\n",
      "\n",
      "For AutoML experiment, extra tags will be logged:\n",
      "- `flaml.estimator_name`: Name of the estimator used in current run. For example `\"lgbm\"`.\n",
      "- `flaml.estimator_class`: Class name of the estimator. For example `LGBMEstimator`.\n",
      "- `flaml.learner`: Name of the learner used in current run. Value is same as `flaml.estimator_name` for now.\n",
      "- `flaml.sample_size`: The sample size of dataset in current run.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "To use the `flaml.distributed` module, you need to have a working Apache Spark cluster. You also need to install PySpark and set up the `SPARK_HOME` environment variable before running the code above. This example assumes that you have a Spark cluster running locally. If you want to run the code on a remote cluster, you need to modify the `master` parameter when creating the `SparkSession` object.\n",
      "\n",
      "It looks like you're also interested in the FLAML's MLFlow integration which enhances the ability to log experiments. You can enable MLFlow autologging with the `mlflow.autolog()` function and log specific information manually using MLFlow inside or outside `flaml.tune` or `flaml.automl`. After you manually start a parent run, the MLFlow integration feature will be enabled, regardless of whether autologging or manual logging is used. \n",
      "\n",
      "Here's an example of how to enable MLFlow autologging and then start a parent run before running `flaml.tune` or `flaml.automl`:\n",
      "\n",
      "```python\n",
      "import mlflow\n",
      "mlflow.autolog() # Enable MLFlow autologging\n",
      "\n",
      "with mlflow.start_run(run_name='parent_run'): # Start a parent MLFlow run\n",
      "    # your flaml.tune or flaml.automl goes here\n",
      "```\n",
      "\n",
      "The `run_name` argument in `mlflow.start_run` is optional, but it allows you to specify a human-readable name for the parent run. Child runs will be renamed according to the parent run's name.\n",
      "\n",
      "Once you've manually started a parent run, FLAML's MLFlow integration feature will enable you to log distributed experiments on Spark. Additionally, tags will be logged automatically, such as `flaml.best_run`, `flaml.run_source`, `flaml.version`, and `flaml.metric`. For AutoML experiment, additional tags will be logged, such as `flaml.estimator_name`, `flaml.estimator_class`, `flaml.learner`, and `flaml.sample_size`.\n",
      "\n",
      "You can also specify `mlflow_exp_name` in `flaml.tune` or `flaml.automl`. If you leave it blank, FLAML will detect the current experiment for you.\n",
      "\n",
      "Remember that the `flaml.tune` or `flaml.automl` code should be wrapped inside the `with mlflow.start_run()` block to ensure that it logs to the correct MLFlow run.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you for your helpful answer. I'm sure it will be useful for anyone who wants to use FLAML with Spark and MLFlow integration.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "You're welcome! Always happy to help! If you have any more questions or if there is anything else I can do to assist you, please don't hesitate to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "call_rag_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
