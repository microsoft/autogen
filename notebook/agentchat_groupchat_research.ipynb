{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Research with Multi-Agent Group Chat\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gpt4,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin.\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=\"\"\"Engineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.\n",
    "Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.\n",
    "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
    "\"\"\",\n",
    ")\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=\"\"\"Scientist. You follow an approved plan. You are able to categorize papers after seeing their abstracts printed. You don't write code.\"\"\",\n",
    ")\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=\"\"\"Planner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.\n",
    "The plan may involve an engineer who can write code and a scientist who doesn't write code.\n",
    "Explain the plan first. Be clear which step is performed by an engineer, and which step is performed by a scientist.\n",
    "\"\"\",\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"paper\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")\n",
    "critic = autogen.AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    system_message=\"Critic. Double check plan, claims, code from other agents and provide feedback. Check whether the plan includes adding verifiable info such as source URL.\",\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor, critic], messages=[], max_round=50\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape the arXiv website for papers related to LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Review the scraped data to identify the different domains in which LLM is applied. This could be based on keywords in the title or abstract, or the scientist's knowledge of the field.\n",
      "\n",
      "3. Engineer: Modify the script to categorize the papers based on the domains identified by the scientist. The script should output a markdown table with columns for the domain, title, authors, abstract, and link.\n",
      "\n",
      "4. Scientist: Review the markdown table to ensure the papers are correctly categorized and the information is accurate.\n",
      "\n",
      "5. Engineer: Make any necessary revisions to the script based on the scientist's feedback.\n",
      "\n",
      "6. Scientist: Give final approval of the markdown table.\n",
      "\n",
      "7. Engineer: Submit the final markdown table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to chat_manager):\n",
      "\n",
      "The plan seems solid and well-structured. However, it lacks the inclusion of verifiable information such as source URLs. Here's a revised version:\n",
      "\n",
      "1. Engineer: Write a script to scrape the arXiv website for papers related to LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Review the scraped data to identify the different domains in which LLM is applied. This could be based on keywords in the title or abstract, or the scientist's knowledge of the field.\n",
      "\n",
      "3. Engineer: Modify the script to categorize the papers based on the domains identified by the scientist. The script should output a markdown table with columns for the domain, title, authors, abstract, and link (source URL).\n",
      "\n",
      "4. Scientist: Review the markdown table to ensure the papers are correctly categorized and the information is accurate. Check the source URLs to verify the information.\n",
      "\n",
      "5. Engineer: Make any necessary revisions to the script based on the scientist's feedback.\n",
      "\n",
      "6. Scientist: Give final approval of the markdown table, ensuring all entries have a valid source URL.\n",
      "\n",
      "7. Engineer: Submit the final markdown table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "I'm sorry for the confusion, but as an AI model developed by OpenAI, I don't have the ability to directly scrape websites or access real-time data from external databases or websites, including arXiv. However, I can help you write a Python script that uses the arXiv API to fetch the data you need.\n",
      "\n",
      "Here's a Python script that uses the `arxiv` package to search for papers related to \"LLM applications\" from the last week. This script will print out the title, authors, summary, and URL of each paper.\n",
      "\n",
      "```python\n",
      "import arxiv\n",
      "import datetime\n",
      "\n",
      "# Calculate the date one week ago\n",
      "one_week_ago = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime('%Y%m%d%H%M%S')\n",
      "\n",
      "# Search for papers on LLM applications\n",
      "search = arxiv.Search(\n",
      "  query=\"LLM applications\",\n",
      "  max_results=100,\n",
      "  sort_by=arxiv.SortCriterion.SubmittedDate,\n",
      "  sort_order=arxiv.SortOrder.Descending\n",
      ")\n",
      "\n",
      "for result in search.get():\n",
      "  # Check if the paper was submitted in the last week\n",
      "  if result.submitted.strftime('%Y%m%d%H%M%S') > one_week_ago:\n",
      "    print(f\"Title: {result.title}\")\n",
      "    print(f\"Authors: {', '.join(author.name for author in result.authors)}\")\n",
      "    print(f\"Summary: {result.summary}\")\n",
      "    print(f\"URL: {result.entry_id}\")\n",
      "    print(\"\\n\")\n",
      "```\n",
      "\n",
      "You can install the `arxiv` package using pip:\n",
      "\n",
      "```shell\n",
      "pip install arxiv\n",
      "```\n",
      "\n",
      "Please note that you'll need to replace `\"LLM applications\"` with the actual search term you're interested in. The arXiv API doesn't support searching by the submission date, so the script fetches the 100 most recent results and filters them manually. You may need to increase `max_results` if there are more than 100 papers submitted per week on your topic.\n",
      "\n",
      "To create a markdown table, you'll need to format the output like this:\n",
      "\n",
      "```markdown\n",
      "| Title | Authors | Summary | URL |\n",
      "|-------|---------|---------|-----|\n",
      "| ...   | ...     | ...     | ... |\n",
      "```\n",
      "\n",
      "You can do this in Python using f-strings:\n",
      "\n",
      "```python\n",
      "print(\"| Title | Authors | Summary | URL |\")\n",
      "print(\"|-------|---------|---------|-----|\")\n",
      "for result in search.get():\n",
      "  # Check if the paper was submitted in the last week\n",
      "  if result.submitted.strftime('%Y%m%d%H%M%S') > one_week_ago:\n",
      "    print(f\"| {result.title} | {', '.join(author.name for author in result.authors)} | {result.summary} | {result.entry_id} |\")\n",
      "```\n",
      "\n",
      "This will print a markdown table to the console. If you want to write the table to a file, you can open a file and use the `write` method instead of `print`.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      ":15: DeprecationWarning: The 'get' method is deprecated, use 'results' instead\n",
      "  for result in search.get():\n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 17, in <module>\n",
      "    if result.submitted.strftime('%Y%m%d%H%M%S') > one_week_ago:\n",
      "AttributeError: 'Result' object has no attribute 'submitted'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion. It seems there was a mistake in the code. The `arxiv` package's `Result` object doesn't have a `submitted` attribute. Instead, we should use the `published` attribute to get the date when the paper was published.\n",
      "\n",
      "Here's the corrected code:\n",
      "\n",
      "```python\n",
      "import arxiv\n",
      "import datetime\n",
      "\n",
      "# Calculate the date one week ago\n",
      "one_week_ago = (datetime.datetime.now() - datetime.timedelta(days=7)).strftime('%Y%m%d%H%M%S')\n",
      "\n",
      "# Search for papers on LLM applications\n",
      "search = arxiv.Search(\n",
      "  query=\"LLM applications\",\n",
      "  max_results=100,\n",
      "  sort_by=arxiv.SortCriterion.Relevance,\n",
      "  sort_order=arxiv.SortOrder.Descending\n",
      ")\n",
      "\n",
      "print(\"| Title | Authors | Summary | URL |\")\n",
      "print(\"|-------|---------|---------|-----|\")\n",
      "for result in search.results():\n",
      "  # Check if the paper was published in the last week\n",
      "  if result.published.strftime('%Y%m%d%H%M%S') > one_week_ago:\n",
      "    print(f\"| {result.title} | {', '.join(author.name for author in result.authors)} | {result.summary} | {result.entry_id} |\")\n",
      "```\n",
      "\n",
      "This script will print a markdown table of papers related to \"LLM applications\" that were published in the last week. The table includes the title, authors, summary, and URL of each paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "| Title | Authors | Summary | URL |\n",
      "|-------|---------|---------|-----|\n",
      "| Large Language Models as Data Preprocessors | Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada | Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's\n",
      "LLaMA variants, have marked a significant advancement in artificial\n",
      "intelligence. Trained on vast amounts of text data, LLMs are capable of\n",
      "understanding and generating human-like text across a diverse range of topics.\n",
      "This study expands on the applications of LLMs, exploring their potential in\n",
      "data preprocessing, a critical stage in data mining and analytics applications.\n",
      "We delve into the applicability of state-of-the-art LLMs such as GPT-3.5,\n",
      "GPT-4, and Vicuna-13B for error detection, data imputation, schema matching,\n",
      "and entity matching tasks. Alongside showcasing the inherent capabilities of\n",
      "LLMs, we highlight their limitations, particularly in terms of computational\n",
      "expense and inefficiency. We propose an LLM-based framework for data\n",
      "preprocessing, which integrates cutting-edge prompt engineering techniques,\n",
      "coupled with traditional methods like contextualization and feature selection,\n",
      "to improve the performance and efficiency of these models. The effectiveness of\n",
      "LLMs in data preprocessing is evaluated through an experimental study spanning\n",
      "12 datasets. GPT-4 emerged as a standout, achieving 100\\% accuracy or F1 score\n",
      "on 4 datasets, suggesting LLMs' immense potential in these tasks. Despite\n",
      "certain limitations, our study underscores the promise of LLMs in this domain\n",
      "and anticipates future developments to overcome current hurdles. | http://arxiv.org/abs/2308.16361v1 |\n",
      "| Large language models in medicine: the potentials and pitfalls | Jesutofunmi A. Omiye, Haiwen Gui, Shawheen J. Rezaei, James Zou, Roxana Daneshjou | Large language models (LLMs) have been applied to tasks in healthcare,\n",
      "ranging from medical exam questions to responding to patient questions. With\n",
      "increasing institutional partnerships between companies producing LLMs and\n",
      "healthcare systems, real world clinical application is coming closer to\n",
      "reality. As these models gain traction, it is essential for healthcare\n",
      "practitioners to understand what LLMs are, their development, their current and\n",
      "potential applications, and the associated pitfalls when utilized in medicine.\n",
      "This review and accompanying tutorial aim to give an overview of these topics\n",
      "to aid healthcare practitioners in understanding the rapidly changing landscape\n",
      "of LLMs as applied to medicine. | http://arxiv.org/abs/2309.00087v1 |\n",
      "| Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following | Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, Pheng-Ann Heng | We introduce Point-Bind, a 3D multi-modality model aligning point clouds with\n",
      "2D image, language, audio, and video. Guided by ImageBind, we construct a joint\n",
      "embedding space between 3D and multi-modalities, enabling many promising\n",
      "applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D\n",
      "open-world understanding. On top of this, we further present Point-LLM, the\n",
      "first 3D large language model (LLM) following 3D multi-modal instructions. By\n",
      "parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of\n",
      "Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction\n",
      "data, but exhibits superior 3D and multi-modal question-answering capacity. We\n",
      "hope our work may cast a light on the community for extending 3D point clouds\n",
      "to multi-modality applications. Code is available at\n",
      "https://github.com/ZiyuGuo99/Point-Bind_Point-LLM. | http://arxiv.org/abs/2309.00615v1 |\n",
      "| Where Would I Go Next? Large Language Models as Human Mobility Predictors | Xinglei Wang, Meng Fang, Zichao Zeng, Tao Cheng | Accurate human mobility prediction underpins many important applications\n",
      "across a variety of domains, including epidemic modelling, transport planning,\n",
      "and emergency responses. Due to the sparsity of mobility data and the\n",
      "stochastic nature of people's daily activities, achieving precise predictions\n",
      "of people's locations remains a challenge. While recently developed large\n",
      "language models (LLMs) have demonstrated superior performance across numerous\n",
      "language-related tasks, their applicability to human mobility studies remains\n",
      "unexplored. Addressing this gap, this article delves into the potential of LLMs\n",
      "for human mobility prediction tasks. We introduce a novel method, LLM-Mob,\n",
      "which leverages the language understanding and reasoning capabilities of LLMs\n",
      "for analysing human mobility data. We present concepts of historical stays and\n",
      "context stays to capture both long-term and short-term dependencies in human\n",
      "movement and enable time-aware prediction by using time information of the\n",
      "prediction target. Additionally, we design context-inclusive prompts that\n",
      "enable LLMs to generate more accurate predictions. Comprehensive evaluations of\n",
      "our method reveal that LLM-Mob excels in providing accurate and interpretable\n",
      "predictions, highlighting the untapped potential of LLMs in advancing human\n",
      "mobility prediction techniques. We posit that our research marks a significant\n",
      "paradigm shift in human mobility modelling, transitioning from building complex\n",
      "domain-specific models to harnessing general-purpose LLMs that yield accurate\n",
      "predictions through language instructions. The code for this work is available\n",
      "at https://github.com/xlwang233/LLM-Mob. | http://arxiv.org/abs/2308.15197v1 |\n",
      "| Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model | Kazuki Hori, Kanata Suzuki, Tetsuya Ogata | The application of the Large Language Model (LLM) to robot action planning\n",
      "has been actively studied. The instructions given to the LLM by natural\n",
      "language may include ambiguity and lack of information depending on the task\n",
      "context. It is possible to adjust the output of LLM by making the instruction\n",
      "input more detailed; however, the design cost is high. In this paper, we\n",
      "propose the interactive robot action planning method that allows the LLM to\n",
      "analyze and gather missing information by asking questions to humans. The\n",
      "method can minimize the design cost of generating precise robot instructions.\n",
      "We demonstrated the effectiveness of our method through concrete examples in\n",
      "cooking tasks. However, our experiments also revealed challenges in robot\n",
      "action planning with LLM, such as asking unimportant questions and assuming\n",
      "crucial information without asking. Shedding light on these issues provides\n",
      "valuable insights for future research on utilizing LLM for robotics. | http://arxiv.org/abs/2308.15684v1 |\n",
      "| AskIt: Unified Programming Interface for Programming with Large Language Models | Katsumi Okuda, Saman Amarasinghe | In the evolving landscape of software development, Large Language Models\n",
      "(LLMs) exhibit a unique phenomenon known as emergent abilities, demonstrating\n",
      "adeptness across numerous tasks, from text summarization to code generation.\n",
      "While these abilities open up novel avenues in software design and crafting,\n",
      "their incorporation presents substantial challenges. Developers grapple with\n",
      "decisions surrounding the direct embedding of LLMs within applications versus\n",
      "employing them for code generation. Moreover, effective prompt design becomes a\n",
      "critical concern, given the necessity of data extraction from natural language\n",
      "outputs. To address these intricacies, this paper introduces AskIt, a\n",
      "domain-specific language (DSL) specifically designed for LLMs. AskIt simplifies\n",
      "LLM integration, offering type-guided output control, template-based function\n",
      "definitions, and a unified interface that diminishes the distinction between\n",
      "LLM-based code generation and application integration. Furthermore, through\n",
      "Programming by Example (PBE), AskIt harnesses the power of few-shot learning at\n",
      "the programming language level. Our evaluations underscore AskIt's potency.\n",
      "Across 50 tasks, AskIt generated concise prompts for the given tasks, achieving\n",
      "a 16.14% reduction in prompt length relative to benchmarks. Additionally, by\n",
      "enabling the transition from direct LLM application usage to function\n",
      "generation, AskIt achieved significant speedups, as observed in our GSM8K\n",
      "benchmark experiments. Through these advancements, AskIt streamlines the\n",
      "integration of LLMs in software development, offering a more efficient,\n",
      "versatile approach for leveraging emergent abilities. The implementations of\n",
      "AskIt in TypeScript and Python are available at\n",
      "https://github.com/katsumiok/ts-askit and https://github.com/katsumiok/pyaskit,\n",
      "respectively. | http://arxiv.org/abs/2308.15645v1 |\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mScientist\u001b[0m (to chat_manager):\n",
      "\n",
      "Here are the papers related to LLM applications published in the last week:\n",
      "\n",
      "| Domain | Title | Authors | Summary | URL |\n",
      "|-------|---------|---------|-----|-----|\n",
      "| Data Preprocessing | [Large Language Models as Data Preprocessors](http://arxiv.org/abs/2308.16361v1) | Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada | This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. | [Link](http://arxiv.org/abs/2308.16361v1) |\n",
      "| Medicine | [Large language models in medicine: the potentials and pitfalls](http://arxiv.org/abs/2309.00087v1) | Jesutofunmi A. Omiye, Haiwen Gui, Shawheen J. Rezaei, James Zou, Roxana Daneshjou | This review and accompanying tutorial aim to give an overview of these topics to aid healthcare practitioners in understanding the rapidly changing landscape of LLMs as applied to medicine. | [Link](http://arxiv.org/abs/2309.00087v1) |\n",
      "| 3D Understanding, Generation, and Instruction Following | [Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following](http://arxiv.org/abs/2309.00615v1) | Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, Pheng-Ann Heng | We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. | [Link](http://arxiv.org/abs/2309.00615v1) |\n",
      "| Human Mobility Prediction | [Where Would I Go Next? Large Language Models as Human Mobility Predictors](http://arxiv.org/abs/2308.15197v1) | Xinglei Wang, Meng Fang, Zichao Zeng, Tao Cheng | This article delves into the potential of LLMs for human mobility prediction tasks. | [Link](http://arxiv.org/abs/2308.15197v1) |\n",
      "| Robotics | [Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model](http://arxiv.org/abs/2308.15684v1) | Kazuki Hori, Kanata Suzuki, Tetsuya Ogata | In this paper, we propose the interactive robot action planning method that allows the LLM to analyze and gather missing information by asking questions to humans. | [Link](http://arxiv.org/abs/2308.15684v1) |\n",
      "| Software Development | [AskIt: Unified Programming Interface for Programming with Large Language Models](http://arxiv.org/abs/2308.15645v1) | Katsumi Okuda, Saman Amarasinghe | This paper introduces AskIt, a domain-specific language (DSL) specifically designed for LLMs. | [Link](http://arxiv.org/abs/2308.15645v1) |\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to chat_manager):\n",
      "\n",
      "The scientist has done a good job categorizing the papers into different domains and providing a summary for each. The markdown table is correctly formatted and includes the source URL for each paper, which allows for verification of the information. The domains identified are diverse, indicating a broad range of applications for Large Language Models (LLMs). This review and categorization should provide a useful resource for anyone interested in the recent applications of LLMs.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"\n",
    "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Group Chat without Critic for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Plan:\n",
      "\n",
      "1. Engineer: Write a script to scrape the arXiv website for papers related to LLM (Language Model) applications published in the last week. The script should extract the title, authors, abstract, and link to the paper.\n",
      "\n",
      "2. Scientist: Review the scraped data to identify the different domains in which LLM is applied. This could be based on keywords in the title or abstract, or the scientist's knowledge of the field.\n",
      "\n",
      "3. Engineer: Modify the script to categorize the papers based on the domains identified by the scientist. The script should output a markdown table with columns for the domain, title, authors, abstract, and link.\n",
      "\n",
      "4. Scientist: Review the markdown table to ensure the papers are correctly categorized and the information is accurate.\n",
      "\n",
      "5. Engineer: Make any necessary revisions to the script based on the scientist's feedback.\n",
      "\n",
      "6. Scientist: Give final approval of the markdown table.\n",
      "\n",
      "7. Engineer: Submit the final markdown table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "groupchat_nocritic = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor], messages=[], max_round=50\n",
    ")\n",
    "for agent in groupchat.agents:\n",
    "    agent.reset()\n",
    "manager_nocritic = autogen.GroupChatManager(groupchat=groupchat_nocritic, llm_config=gpt4_config)\n",
    "user_proxy.initiate_chat(\n",
    "    manager_nocritic,\n",
    "    message=\"\"\"\n",
    "find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.\n",
    "\"\"\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "front_matter": {
    "tags": ["group chat"],
    "description": "Perform research using a group chat with a number of specialized agents"
  },
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
