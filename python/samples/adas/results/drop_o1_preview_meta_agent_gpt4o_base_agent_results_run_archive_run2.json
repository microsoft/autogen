[
    {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import logging\n    import json\n    from dataclasses import dataclass\n    import sys\n    from autogen_core import SingleThreadedAgentRuntime, DefaultTopicId, RoutedAgent, message_handler, ClosureAgent, ClosureContext, DefaultSubscription\n    from autogen_core.base import AgentId, AgentRuntime, MessageContext\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        LLMMessage,\n        SystemMessage,\n        UserMessage,\n    )\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from typing import List\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n\n    # Define message types as data classes\n    @dataclass\n    class ChainOfThoughtTask:\n        task: str\n\n\n    @dataclass\n    class FinalResult:\n        result: str\n\n\n    # Define the Chain-of-Thought Agent\n    class ChainOfThoughtAgent(RoutedAgent):\n        def __init__(self, description: str,\n                    model_client: ChatCompletionClient,\n                    system_prompt: str,\n                    instruction: str,\n            ) -> None:\n            super().__init__(description)\n            self._system_messages: List[LLMMessage] = [\n                SystemMessage(\n                    content=system_prompt,\n                )\n            ]\n            self._model_client = model_client\n            self._instruction = instruction\n\n        @message_handler\n        async def handle_task(self, message: ChainOfThoughtTask, ctx: MessageContext) -> None:\n\n            logging.info(f\"{self._description} received message: {message.task}\")\n            user_prompt = message.task + \"\\n\" + self._instruction\n            msgs = self._system_messages + [UserMessage(content=user_prompt, source=self.metadata[\"type\"])]\n            model_result = await self._model_client.create(msgs)\n            assert isinstance(model_result.content, str)\n\n            await self.publish_message(\n                message=FinalResult(model_result.content),\n                topic_id=DefaultTopicId(),\n            )\n\n\n    # Define the main function to set up and run the agent system\n    async def main():\n\n        # Create a queue to collect final answer\n        queue = asyncio.Queue[FinalResult]()\n        async def output_result(_agent: ClosureContext, message: FinalResult, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Create the chain-of-thought agent\n        agent_id = AgentId(\"COTAgent\", \"default\")\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        await ChainOfThoughtAgent.register(\n            runtime, \"COTAgent\", lambda: ChainOfThoughtAgent(\n                description='Chain-of-Thought Agent',\n                model_client=model_client,\n                system_prompt=\"You are a helpful assistant. Directly answer the question. Keep it very concise.\",\n                instruction=cot_instruction,\n            )\n        )\n        # Create closure agent to collect final output result\n        await ClosureAgent.register_closure(runtime, \"output_result\", output_result, subscriptions=lambda: [DefaultSubscription()])\n\n        # Start the runtime, and publish the first message\n        runtime.start()\n        initial_message = ChainOfThoughtTask(task=task)\n        await runtime.send_message(initial_message, agent_id) # publish_message\n\n        # Keep processing messages until idle.\n        await runtime.stop_when_idle()\n\n        # Return the first answer from the queue\n        return (await queue.get()).result\n\n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 7.8%), Median: 26.8%"
    },
    {
        "thought": "While an LLM can arrive at the correct answer, its reasoning may vary. By repeatedly asking the same question with high temperature settings, we can generate different reasoning paths. We then combine multiple answers from these Chain-of-Thought (CoT) agents to produce a more accurate final answer through ensembling.",
        "name": "Self-Consistency with Chain-of-Thought",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import logging\n    import json\n    from dataclasses import dataclass\n    import sys\n    from autogen_core import SingleThreadedAgentRuntime, DefaultTopicId, RoutedAgent, message_handler, ClosureAgent, ClosureContext, DefaultSubscription\n    from autogen_core.base import AgentId, AgentRuntime, MessageContext\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        LLMMessage,\n        SystemMessage,\n        UserMessage,\n    )\n    from typing import List\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n\n    @dataclass\n    class WorkerTask:\n        task: str\n        previous_results: List[str]\n\n\n    @dataclass\n    class WorkerTaskResult:\n        result: str\n\n\n    @dataclass\n    class UserTask:\n        task: str\n\n\n    @dataclass\n    class FinalResult:\n        result: str\n\n\n    class WorkerAgent(RoutedAgent):\n        def __init__(\n            self,\n            model_client: ChatCompletionClient,\n            instruction: str,\n        ) -> None:\n            super().__init__(description=\"Worker Agent\")\n            self._model_client = model_client\n            self._instruction = instruction\n\n        @message_handler\n        async def handle_task(self, message: WorkerTask, ctx: MessageContext) -> WorkerTaskResult:\n            user_prompt = message.task + \"\\n\" + self._instruction\n\n            if message.previous_results:\n                # If previous results are provided, we need to synthesize them to create a single prompt.\n                # system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n                system_prompt = \"Given all the solutions, reason over them carefully and provide a final answer.\"\n                system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(message.previous_results)])\n                model_result = await self._model_client.create(\n                    [SystemMessage(content=system_prompt), UserMessage(content=user_prompt, source=\"user\")]\n                )\n            else:\n                # If no previous results are provided, we can simply pass the user query to the model.\n                model_result = await self._model_client.create([UserMessage(content=user_prompt, source=\"user\")])\n            assert isinstance(model_result.content, str)\n            print(f\"{'-'*80}\\nWorker-{self.id}:\\n{model_result.content}\")\n            return WorkerTaskResult(result=model_result.content)\n\n\n    class OrchestratorAgent(RoutedAgent):\n        def __init__(\n            self,\n            model_client: ChatCompletionClient,\n            worker_agent_types: List[str],\n            num_layers: int,\n        ) -> None:\n            super().__init__(description=\"Aggregator Agent\")\n            self._model_client = model_client\n            self._worker_agent_types = worker_agent_types\n            self._num_layers = num_layers\n\n\n        @message_handler\n        async def handle_task(self, message: UserTask, ctx: MessageContext) -> FinalResult:\n            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived task: {message.task}\")\n            # Create task for the first layer.\n            worker_task = WorkerTask(task=message.task, previous_results=[])\n            # Iterate over layers.\n            for i in range(self._num_layers):\n                # Assign workers for this layer.\n                worker_ids = [\n                    AgentId(worker_type, f\"{self.id.key}/layer_{i}/worker_{j}\")\n                    for j, worker_type in enumerate(self._worker_agent_types)\n                ]\n                # Dispatch tasks to workers.\n                print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nDispatch to workers at layer {i}\")\n                results = await asyncio.gather(*[self.send_message(worker_task, worker_id) for worker_id in worker_ids])\n                print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived results from workers at layer {i}\")\n                # Prepare task for the next layer.\n                worker_task = WorkerTask(task=message.task, previous_results=[r.result for r in results])\n            # Perform final aggregation.\n            print(\"6666\")\n            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nPerforming final aggregation\")\n            print(\"1234\")\n            system_prompt = \"Given all the above solutions, reason over them carefully and provide a final answer.\"\n            print(\"aaasdf\")\n            system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(worker_task.previous_results)])\n            print(\"aasdf\")\n            model_result = await self._model_client.create(\n                [SystemMessage(content=system_prompt), UserMessage(content=message.task, source=\"user\")]\n            )\n            print(\"asdf\")\n            assert isinstance(model_result.content, str)\n            return FinalResult(result=model_result.content)\n            # await self.publish_message(\n            #     message=FinalResult(result=model_result.content),\n            #     topic_id=DefaultTopicId(),\n            # )\n            \n    # Define the main function to set up and run the agent system\n    async def main():\n        # Create a queue to collect final answer\n        queue = asyncio.Queue[FinalResult]()\n        async def output_result(_agent: ClosureContext, message: FinalResult, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Create the agents\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        await WorkerAgent.register(\n            runtime, \"worker\", lambda: WorkerAgent(model_client=model_client, instruction=cot_instruction)\n        )\n        await OrchestratorAgent.register(\n            runtime,\n            \"orchestrator\",\n            lambda: OrchestratorAgent(\n                model_client=model_client, worker_agent_types=[\"worker\"] * 5, num_layers=1\n            ),\n        )\n        # Create closure agent to collect final output result\n        await ClosureAgent.register_closure(runtime, \"output_result\", output_result, subscriptions=lambda: [DefaultSubscription()])\n\n        # Start the runtime, and publish the first message\n        runtime.start()\n        result = await runtime.send_message(UserTask(task=task), AgentId(\"orchestrator\", \"default\"))\n\n        # Return the result\n        print(\"fdsa\")\n        return result.result\n\n        \n\n\n        \n        # await runtime.send_message(UserTask(task=task), AgentId(\"orchestrator\", \"default\"))\n        # await runtime.stop_when_idle()\n\n        # # Return the first answer from the queue\n        # print(\"final\")\n        # return (await queue.get()).result\n\n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (3.0%, 9.3%), Median: 29.4%"
    },
    {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import json\n    import logging\n    import re\n    import sys\n    import uuid\n    from dataclasses import dataclass\n    from typing import Dict, List, Union\n    from autogen_core.base import MessageContext, TopicId, AgentId, AgentRuntime\n    from autogen_core import SingleThreadedAgentRuntime, DefaultTopicId, RoutedAgent, TypeSubscription, DefaultSubscription, ClosureAgent, ClosureContext, message_handler, default_subscription\n    from autogen_core.components.models import (\n        AssistantMessage,\n        ChatCompletionClient,\n        LLMMessage,\n        SystemMessage,\n        UserMessage,\n    )\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n\n    @dataclass\n    class WritingTask:\n        task: str\n\n\n    @dataclass\n    class WritingResult:\n        task: str\n        answer: str\n        review: str\n\n\n    @dataclass\n    class ReviewTask:\n        session_id: str\n        writing_task: str\n        answer_scratchpad: str\n        answer: str\n\n\n    @dataclass\n    class ReviewResult:\n        review: str\n        session_id: str\n        approved: bool\n\n\n    @default_subscription\n    class WorkerAgent(RoutedAgent):\n        \"An agent that performs writing tasks.\"\n\n        def __init__(self,\n                    model_client: ChatCompletionClient,\n                    instruction: str,\n        ) -> None:\n            super().__init__(\"A helpful assistant\")\n            self._system_messages: List[LLMMessage] = [\n                SystemMessage(\n                    content=\"\"\"You are a helpful assistant. Work with the critic to improve your answer.\n                    Make sure to directly answer the question. Keep it very concise.\n                    Respond using the following format:\n\n    Thoughts: <Your comments>\n    Answer: <Your answer>\n    \"\"\",\n                )\n            ]\n            self._model_client = model_client\n            self._session_memory: Dict[str, List[WritingTask | ReviewTask | ReviewResult]] = {}\n            self._instruction = instruction\n\n        @message_handler\n        async def handle_writing_task(self, message: WritingTask, ctx: MessageContext) -> None:\n            # Store the messages in a temporary memory for this request only.\n            session_id = str(uuid.uuid4())\n            self._session_memory.setdefault(session_id, []).append(message)\n            # Generate a response using the chat completion API.\n            response = await self._model_client.create(\n                self._system_messages + [UserMessage(content=message.task + self._instruction, source=self.metadata[\"type\"])],\n                cancellation_token=ctx.cancellation_token,\n            )\n            assert isinstance(response.content, str)\n            # Extract the answer from the response.\n            answer = self._extract_answer(response.content)\n            # Create a review task.\n            review_task = ReviewTask(\n                session_id=session_id,\n                writing_task=message.task,\n                answer_scratchpad=response.content,\n                answer=answer,\n            )\n            # Store the review task in the session memory.\n            self._session_memory[session_id].append(review_task)\n            # Publish a review task.\n            await self.publish_message(review_task, topic_id=TopicId(\"default\", self.id.key))\n\n        @message_handler\n        async def handle_review_result(self, message: ReviewResult, ctx: MessageContext) -> None:\n            # Store the review result in the session memory.\n            self._session_memory[message.session_id].append(message)\n            # Obtain the request from previous messages.\n            review_request = next(\n                m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, ReviewTask)\n            )\n            assert review_request is not None\n            # Check if the is approved.\n            if message.approved:\n                # Publish the writing result.\n                await self.publish_message(\n                    WritingResult(\n                        answer=review_request.answer,\n                        task=review_request.writing_task,\n                        review=message.review,\n                    ),\n                    topic_id=TopicId(\"result\", self.id.key),\n                )\n                print(\"Writing Result:\")\n                print(\"-\" * 80)\n                print(f\"Task:\\n{review_request.writing_task}\")\n                print(\"-\" * 80)\n                print(f\"Answer:\\n{review_request.answer}\")\n                print(\"-\" * 80)\n                print(f\"Review:\\n{message.review}\")\n                print(\"-\" * 80)\n            else:\n                # Create a list of LLM messages to send to the model.\n                messages: List[LLMMessage] = [*self._system_messages]\n                for m in self._session_memory[message.session_id]:\n                    if isinstance(m, ReviewResult):\n                        messages.append(UserMessage(content=m.review, source=\"Reviewer\"))\n                    elif isinstance(m, ReviewTask):\n                        messages.append(AssistantMessage(content=m.answer_scratchpad, source=\"Worker\"))\n                    elif isinstance(m, WritingTask):\n                        messages.append(UserMessage(content=m.task, source=\"User\"))\n                    else:\n                        raise ValueError(f\"Unexpected message type: {m}\")\n                # Generate a revision using the chat completion API.\n                response = await self._model_client.create(messages, cancellation_token=ctx.cancellation_token)\n                assert isinstance(response.content, str)\n                # Extract the answer from the response.\n                answer = self._extract_answer(response.content)\n                # Create a new review task.\n                review_task = ReviewTask(\n                    session_id=message.session_id,\n                    writing_task=review_request.writing_task,\n                    answer_scratchpad=response.content,\n                    answer=answer,\n                )\n                # Store the review task in the session memory.\n                self._session_memory[message.session_id].append(review_task)\n                # Publish a new review task.\n                await self.publish_message(review_task, topic_id=TopicId(\"default\", self.id.key))\n\n\n        def _extract_answer(self, text: str) -> Union[str, None]:\n            pattern = \"(?<=Answer: ).*\"\n            # Search for the pattern in the markdown text\n            match = re.search(pattern, text, re.DOTALL)\n            # Extract the language and code block if a match is found\n            if match:\n                return match.group(0)\n            return None\n\n    @default_subscription\n    class ReviewerAgent(RoutedAgent):\n        \"\"\"An agent that critiques tasks.\"\"\"\n\n        def __init__(self, model_client: ChatCompletionClient) -> None:\n            super().__init__(\"A critic agent.\")\n            self._system_messages: List[LLMMessage] = [\n                SystemMessage(\n                    content=\"\"\"You are a critic. Review answers and criticize on where it might be wrong.\n    Respond using the following JSON format:\n    {\n        \"correctness\": \"<Your comments>\",\n        \"approval\": \"<APPROVE or REVISE>\",\n        \"suggested_changes\": \"<Your comments>\"\n    }\n    \"\"\",\n                )\n            ]\n            self._session_memory: Dict[str, List[ReviewTask | ReviewResult]] = {}\n            self._model_client = model_client\n\n        @message_handler\n        async def handle_review_task(self, message: ReviewTask, ctx: MessageContext) -> None:\n            # Format the prompt for the review.\n            # Gather the previous feedback if available.\n            previous_feedback = \"\"\n            if message.session_id in self._session_memory:\n                previous_review = next(\n                    (m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, ReviewResult)),\n                    None,\n                )\n                if previous_review is not None:\n                    previous_feedback = previous_review.review\n            # Store the messages in a temporary memory for this request only.\n            self._session_memory.setdefault(message.session_id, []).append(message)\n            prompt = f\"\"\"The problem statement is: {message.writing_task}\n    The answer is:\n    ```\n    {message.answer}\n    ```\n\n    Previous feedback:\n    {previous_feedback}\n\n    Please review the answer. If previous feedback was provided, see if it was addressed.\n    \"\"\"\n            # Generate a response using the chat completion API.\n            response = await self._model_client.create(\n                self._system_messages + [UserMessage(content=prompt, source=self.metadata[\"type\"])],\n                cancellation_token=ctx.cancellation_token,\n                json_output=True,\n            )\n            assert isinstance(response.content, str)\n            # TODO: use structured generation library e.g. guidance to ensure the response is in the expected format.\n            # Parse the response JSON.\n            review = json.loads(response.content)\n            # Construct the review text.\n            review_text = \"Review:\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in review.items()])\n            approved = review[\"approval\"].lower().strip() == \"approve\"\n            result = ReviewResult(\n                review=review_text,\n                session_id=message.session_id,\n                approved=approved,\n            )\n            # Store the review result in the session memory.\n            self._session_memory[message.session_id].append(result)\n            # Publish the review result.\n            await self.publish_message(result, topic_id=TopicId(\"default\", self.id.key))\n\n\n    # Define the main function to set up and run the agent system\n    async def main():\n        # Create a queue to collect final answer\n        queue = asyncio.Queue[WritingResult]()\n        async def output_result(_agent: ClosureContext, message: WritingResult, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Create agents\n        await ReviewerAgent.register(\n            runtime, \"ReviewerAgent\", lambda: ReviewerAgent(model_client=model_client)\n        )\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        await WorkerAgent.register(\n            runtime, \"WorkerAgent\", lambda: WorkerAgent(model_client=model_client, instruction=cot_instruction)\n        )\n        # Create closure agent to collect final output result\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(runtime, \"output_result\", output_result, subscriptions=lambda: [result_topic])\n\n        # Start the runtime, and publish the first message\n        runtime.start()\n        await runtime.publish_message(\n            message=WritingTask(task=task),\n            topic_id=DefaultTopicId(),\n        )\n\n        # Keep processing messages until idle.\n        await runtime.stop_when_idle()\n\n        # Return the first answer from the queue\n        print(f\"queue {queue}\")\n        return (await queue.get()).answer\n    \n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 20.0%), Median: 55.5%"
    },
    {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import json\n    import logging\n    import re\n    import sys\n    import uuid\n    from dataclasses import dataclass\n    from typing import Dict, List, Union\n    from autogen_core.base import MessageContext, TopicId, AgentId, AgentRuntime\n    from autogen_core import SingleThreadedAgentRuntime, RoutedAgent, default_subscription, message_handler, TypeSubscription, ClosureAgent, ClosureContext, DefaultTopicId\n    from autogen_core.components.models import (\n        AssistantMessage,\n        ChatCompletionClient,\n        LLMMessage,\n        SystemMessage,\n        UserMessage,\n    )\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n    \n    @dataclass\n    class Question:\n        content: str\n\n\n    @dataclass\n    class Answer:\n        content: str\n\n\n    @dataclass\n    class SolverRequest:\n        content: str\n        question: str\n\n\n    @dataclass\n    class IntermediateSolverResponse:\n        content: str\n        question: str\n        answer: str\n        round: int\n\n\n    @dataclass\n    class FinalSolverResponse:\n        answer: str\n\n    @default_subscription\n    class Solver(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient, topic_type: str, num_neighbors: int, max_round: int) -> None:\n            super().__init__(\"A debator.\")\n            self._topic_type = topic_type\n            self._model_client = model_client\n            self._num_neighbors = num_neighbors\n            self._history: List[LLMMessage] = []\n            self._buffer: Dict[int, List[IntermediateSolverResponse]] = {}\n            self._system_messages = [\n                SystemMessage(content=\n                    (\n                        \"You are a helpful assistant with expertise in reasoning. \"\n                        \"Your task is to assist in solving a reasoning problem by providing \"\n                        \"a clear and detailed solution. Limit your output within 100 words, \"\n                        \"and your final answer should be a single string.\"\n                    )\n                )\n            ]\n            self._round = 0\n            self._max_round = max_round\n\n        @message_handler\n        async def handle_request(self, message: SolverRequest, ctx: MessageContext) -> None:\n            # Add the question to the memory.\n            self._history.append(UserMessage(content=message.content, source=\"user\"))\n            # Make an inference using the model.\n            model_result = await self._model_client.create(self._system_messages + self._history)\n            assert isinstance(model_result.content, str)\n            # Add the response to the memory.\n            self._history.append(AssistantMessage(content=model_result.content, source=self.metadata[\"type\"]))\n            print(f\"{'-'*80}\\nSolver {self.id} round {self._round}:\\n{model_result.content}\")\n            # Increment the counter.\n            self._round += 1\n            if self._round == self._max_round:\n                # If the counter reaches the maximum round, publishes a final response.\n                await self.publish_message(FinalSolverResponse(answer=model_result.content), topic_id=DefaultTopicId())\n            else:\n                # Publish intermediate response to the topic associated with this solver.\n                print(\"publish IntermediateSolverResponse\")\n                await self.publish_message(\n                    IntermediateSolverResponse(\n                        content=model_result.content,\n                        question=message.question,\n                        answer=model_result.content,\n                        round=self._round,\n                    ),\n                    topic_id=DefaultTopicId(type=self._topic_type),\n                )\n\n        @message_handler\n        async def handle_response(self, message: IntermediateSolverResponse, ctx: MessageContext) -> None:\n            # Add neighbor's response to the buffer.\n            self._buffer.setdefault(message.round, []).append(message)\n            # Check if all neighbors have responded.\n            if len(self._buffer[message.round]) == self._num_neighbors:\n                print(\n                    f\"{'-'*80}\\nSolver {self.id} round {message.round}:\\nReceived all responses from {self._num_neighbors} neighbors.\"\n                )\n                # Prepare the prompt for the next question.\n                prompt = \"These are the solutions to the problem from other agents:\\n\"\n                for resp in self._buffer[message.round]:\n                    prompt += f\"One agent solution: {resp.content}\\n\"\n                prompt += (\n                    \"Using the solutions from other agents as additional information, \"\n                    \"can you provide your answer to the problem? \"\n                    f\"The original problem is {message.question}. \"\n                    \"Your final answer should be a single string.\"\n                )\n                # Send the question to the agent itself to solve.\n                await self.send_message(SolverRequest(content=prompt, question=message.question), self.id)\n                # Clear the buffer.\n                self._buffer.pop(message.round)\n\n\n    @default_subscription\n    class Aggregator(RoutedAgent):\n        def __init__(self, num_solvers: int) -> None:\n            super().__init__(\"Aggregator\")\n            self._num_solvers = num_solvers\n            self._buffer: List[FinalSolverResponse] = []\n\n        @message_handler\n        async def handle_question(self, message: Question, ctx: MessageContext) -> None:\n            print(f\"{'-'*80}\\nAggregator {self.id} received question:\\n{message.content}\")\n            prompt = (\n                f\"Can you solve the following problem?\\n{message.content}\\n\"\n                \"Explain your reasoning. Your final answer should be a single string.\"\n            )\n            print(f\"{'-'*80}\\nAggregator {self.id} publishes initial solver request.\")\n            await self.publish_message(SolverRequest(content=prompt, question=message.content), topic_id=DefaultTopicId())\n\n        @message_handler\n        async def handle_final_solver_response(self, message: FinalSolverResponse, ctx: MessageContext) -> None:\n            self._buffer.append(message)\n            if len(self._buffer) == self._num_solvers:\n                print(f\"{'-'*80}\\nAggregator {self.id} received all final answers from {self._num_solvers} solvers.\")\n                # Find the majority answer.\n                answers = [resp.answer for resp in self._buffer]\n                majority_answer = max(set(answers), key=answers.count)\n                # Publish the aggregated response.\n                await self.publish_message(Answer(content=majority_answer), topic_id=TopicId(\"result\", self.id.key))\n                # Clear the responses.\n                self._buffer.clear()\n                print(f\"{'-'*80}\\nAggregator {self.id} publishes final answer:\\n{majority_answer}\")\n\n\n    # Define the main function to set up and run the agent system\n    async def main():\n        queue = asyncio.Queue[Answer]()\n        async def output_result(_agent: ClosureContext, message: Answer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        runtime = SingleThreadedAgentRuntime()\n        await Solver.register(\n            runtime,\n            \"SolverA\",\n            lambda: Solver(\n                model_client=model_client,\n                topic_type=\"SolverA\",\n                num_neighbors=2,\n                max_round=3,\n            ),\n        )\n        await Solver.register(\n            runtime,\n            \"SolverB\",\n            lambda: Solver(\n                model_client=model_client,\n                topic_type=\"SolverB\",\n                num_neighbors=2,\n                max_round=3,\n            ),\n        )\n        await Solver.register(\n            runtime,\n            \"SolverC\",\n            lambda: Solver(\n                model_client=model_client,\n                topic_type=\"SolverC\",\n                num_neighbors=2,\n                max_round=3,\n            ),\n        )\n        await Solver.register(\n            runtime,\n            \"SolverD\",\n            lambda: Solver(\n                model_client=model_client,\n                topic_type=\"SolverD\",\n                num_neighbors=2,\n                max_round=3,\n            ),\n        )\n        await Aggregator.register(runtime, \"Aggregator\", lambda: Aggregator(num_solvers=4))\n\n        # Subscriptions for topic published to by SolverA.\n        await runtime.add_subscription(TypeSubscription(\"SolverA\", \"SolverD\"))\n        await runtime.add_subscription(TypeSubscription(\"SolverA\", \"SolverB\"))\n\n        # Subscriptions for topic published to by SolverB.\n        await runtime.add_subscription(TypeSubscription(\"SolverB\", \"SolverA\"))\n        await runtime.add_subscription(TypeSubscription(\"SolverB\", \"SolverC\"))\n\n        # Subscriptions for topic published to by SolverC.\n        await runtime.add_subscription(TypeSubscription(\"SolverC\", \"SolverB\"))\n        await runtime.add_subscription(TypeSubscription(\"SolverC\", \"SolverD\"))\n\n        # Subscriptions for topic published to by SolverD.\n        await runtime.add_subscription(TypeSubscription(\"SolverD\", \"SolverC\"))\n        await runtime.add_subscription(TypeSubscription(\"SolverD\", \"SolverA\"))\n\n        # All solvers and the aggregator subscribe to the default topic.\n\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(runtime, \"output_result\", output_result, subscriptions=lambda: [result_topic])\n\n        runtime.start()\n        await runtime.publish_message(Question(content=task), DefaultTopicId())\n\n        # Keep processing messages until idle.\n        await runtime.stop_when_idle()\n\n        # Return the answer from the queue\n        return (await queue.get()).content\n\n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (16.7%, 33.3%), Median: 76.7%"
    },
    {
        "thought": "By using a tree search strategy, the model can explore multiple branches of thoughts, where at any step of the problem, multiple independent thoughts are generated and evaluated to find the most useful ones.",
        "name": "Tree of Thought",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import logging\n    from dataclasses import dataclass\n    from typing import List, Dict, Any\n    from autogen_core import SingleThreadedAgentRuntime, default_subscription, RoutedAgent, message_handler, ClosureAgent, ClosureContext, TypeSubscription, DefaultTopicId\n    from autogen_core.base import AgentId, AgentRuntime, MessageContext, TopicId\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        SystemMessage,\n        UserMessage,\n        AssistantMessage,\n        LLMMessage,\n    )\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n    from autogen_core.application.logging import TRACE_LOGGER_NAME \n\n    # Configure logging as per documentation \n    logging.basicConfig(level=logging.WARNING) \n    logger = logging.getLogger(TRACE_LOGGER_NAME) \n    logger.setLevel(logging.INFO)\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n \n    @dataclass \n    class Message: \n        content: str \n    \n    @dataclass \n    class FinalAnswer: \n        answer: str \n\n    @default_subscription\n    class TreeOfThoughtsAgent(RoutedAgent): \n        def __init__(self, model_client: ChatCompletionClient, max_depth: int = 3, beam_width: int = 3): \n            super().__init__(\"TreeOfThoughtsAgent\") \n            self._model_client = model_client \n            self._max_depth = max_depth \n            self._beam_width = beam_width \n            self._system_messages = [ \n                SystemMessage( \n                    content=\"You are a helpful assistant who reasons step-by-step to solve complex problems.\") \n            ] \n    \n        async def generate_thoughts(self, prompt: List[LLMMessage], num_thoughts: int, cancellation_token) -> List[str]: \n            # Generate multiple thoughts using the model \n            thoughts = [] \n            # Create multiple async tasks to generate thoughts in parallel \n            tasks = [] \n            for _ in range(num_thoughts): \n                tasks.append(self._model_client.create( \n                    prompt, \n                    extra_create_args={\"temperature\": 1.0},\n                    cancellation_token=cancellation_token, \n                )) \n            responses = await asyncio.gather(*tasks) \n            for response in responses: \n                thoughts.append(response.content.strip()) \n            return thoughts \n    \n        async def evaluate_thoughts(self, thoughts: List[str], ctx: MessageContext) -> List[str]: \n            # Batch evaluation of thoughts \n            eval_prompt = [ \n                SystemMessage(content=\"You are an assistant that evaluates reasoning steps for solving a problem.\"), \n                UserMessage( \n                    content=f\"Evaluate the following thoughts for their usefulness in solving the problem. Rank them from most useful to least useful and provide the rankings.\\n\\nThoughts:\\n\" + \"\\n\".join( \n                        [f\"{i+1}. {t}\" for i, t in enumerate(thoughts)]), \n                    source=\"user\" \n                ) \n            ] \n            eval_response = await self._model_client.create( \n                eval_prompt, \n                cancellation_token=ctx.cancellation_token, \n            ) \n            # Parse the response to extract rankings \n            rankings_text = eval_response.content.strip() \n            # For simplicity, assume the model outputs the rankings as a list of numbers \n            rankings = [] \n            for line in rankings_text.split('\\n'): \n                line = line.strip() \n                if line and line[0].isdigit(): \n                    rankings.append(int(line[0]) - 1)  # Subtract 1 to get index \n            # Select top-k thoughts \n            best_thoughts = [thoughts[i] for i in rankings[:self._beam_width]] \n            return best_thoughts \n    \n        @message_handler \n        async def handle_message(self, message: Message, ctx: MessageContext) -> None: \n            logger.info(f\"Received task: {message.content}\") \n            initial_prompt = self._system_messages + [UserMessage(content=message.content, source=\"user\")] \n            tree = [[]]  # Initialize the tree with an empty path \n            for depth in range(self._max_depth): \n                new_branches = [] \n                logger.info(f\"Depth {depth+1}\") \n                for path in tree: \n                    # Build the prompt up to this point \n                    prompt = initial_prompt.copy() \n                    for thought in path: \n                        prompt.append(AssistantMessage(content=thought, source=\"assistant\")) \n                    # Generate thoughts \n                    thoughts = await self.generate_thoughts(prompt, self._beam_width, ctx.cancellation_token) \n                    logger.info(f\"Generated thoughts: {thoughts}\") \n                    # Evaluate thoughts \n                    best_thoughts = await self.evaluate_thoughts(thoughts, ctx) \n                    logger.info(f\"Best thoughts: {best_thoughts}\") \n                    # Expand tree with best thoughts \n                    for thought in best_thoughts: \n                        new_path = path + [thought] \n                        new_branches.append(new_path) \n                # Update tree with new branches \n                if not new_branches: \n                    logger.info(\"No more branches to expand.\") \n                    break  # No more thoughts to expand \n                tree = new_branches \n            # After reaching max depth, select the best path \n            # For simplicity, select the first path \n            best_path = tree[0] \n            final_answer = best_path[-1] \n            logger.info(f\"Final answer: {final_answer}\") \n            # Publish the final answer \n            await self.publish_message( \n                FinalAnswer(answer=final_answer), \n                topic_id=TopicId(type=\"result\", source=self.id.key) \n            ) \n    \n    # Main function \n    async def main(): \n        # Create a queue to collect the final answer \n        queue = asyncio.Queue[FinalAnswer]() \n    \n        async def output_result(_agent: ClosureContext, message: FinalAnswer, ctx: MessageContext) -> None: \n            await queue.put(message) \n    \n        # Initialize runtime \n        runtime = SingleThreadedAgentRuntime() \n    \n        # Register TreeOfThoughtsAgent \n        await TreeOfThoughtsAgent.register( \n            runtime, \n            \"TreeOfThoughtsAgent\", \n            lambda: TreeOfThoughtsAgent(model_client) \n        ) \n    \n        # Register ClosureAgent with agent key matching self.id.key (default is \"default\") \n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\") \n        await ClosureAgent.register_closure( \n            runtime, \n            \"output_result\", \n            output_result, \n            subscriptions=lambda: [result_topic] \n        ) \n    \n        # Start the runtime \n        runtime.start() \n    \n        # Publish initial message to TreeOfThoughtsAgent\n        await runtime.publish_message( \n            Message(content=task), \n            topic_id=DefaultTopicId() \n        ) \n    \n        # Wait until idle \n        await runtime.stop_when_idle() \n    \n        # Return the final answer \n        final_message = await queue.get() \n        return final_message.answer\n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 6.4%), Median: 37.8%"
    },
    {
        "thought": "**Insights:**\nTo improve performance on DROP tasks requiring complex reasoning and calculations, allowing the LLM to generate and execute code can enhance accuracy. This aligns with the Program-Aided Language Models (PAL) method.\n**Overall Idea:**\nDesign an agent that instructs the LLM to generate Python code for the task. The code is then executed safely, and the output is used as the final answer. This leverages Python's computational abilities for precise calculations.\n**Implementation:**\n- Implement 'PALAgent' that prompts the LLM to generate Python code for the task.\n- Extract code from the LLM's response.\n- Use 'CodeExecutionAgent' to execute the code safely.\n- Capture the execution output and return it as the final answer.\n- Handle potential errors during code execution.\n- Ensure code execution is secure and isolated.",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import re\n    from dataclasses import dataclass\n    from autogen_core import SingleThreadedAgentRuntime, RoutedAgent, message_handler, ClosureAgent, ClosureContext, TypeSubscription\n    from autogen_core.base import AgentId, MessageContext, TopicId\n    from autogen_core.components.models import ChatCompletionClient, SystemMessage, UserMessage\n    from autogen_core.code_executor import CodeBlock\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True\n        }\n    )\n\n    @dataclass\n    class Message:\n        content: str\n\n    @dataclass\n    class FinalAnswer:\n        answer: str\n\n    @dataclass\n    class ExecuteCode:\n        code: str\n\n    @dataclass\n    class ExecutionResult:\n        output: str\n\n    class PALAgent(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient, code_agent_type: str):\n            super().__init__(\"PAL Agent\")\n            self._model_client = model_client\n            self._system_messages = [\n                SystemMessage(content=\"You are an expert reasoning agent. Solve the following task by generating Python code to compute the answer. Enclose your code in markdown code blocks. After execution, provide the final answer.\")\n            ]\n            self._code_agent_id = AgentId(code_agent_type, self.id.key)\n\n        @message_handler\n        async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n            user_message = UserMessage(content=message.content, source=\"user\")\n            messages = self._system_messages + [user_message]\n            response = await self._model_client.create(messages, cancellation_token=ctx.cancellation_token)\n            assert isinstance(response.content, str)\n            # Extract code from the response\n            code_blocks = re.findall(r'```(?:python)?\\n(.*?)```', response.content, re.DOTALL)\n            if code_blocks:\n                code = code_blocks[0]\n                # Send code to CodeExecutionAgent for execution\n                execution_result = await self.send_message(ExecuteCode(code=code), self._code_agent_id)\n                final_answer = execution_result.output.strip()\n                # Publish final answer\n                await self.publish_message(\n                    FinalAnswer(answer=final_answer),\n                    topic_id=TopicId(\"result\", self.id.key)\n                )\n            else:\n                # No code found, return the LLM's response as is\n                await self.publish_message(\n                    FinalAnswer(answer=response.content),\n                    topic_id=TopicId(\"result\", self.id.key)\n                )\n\n    class CodeExecutionAgent(RoutedAgent):\n        def __init__(self) -> None:\n            super().__init__(\"Code Execution Agent\")\n            self._code_executor = LocalCommandLineCodeExecutor()\n\n        async def start_code_executor(self):\n            # No initialization needed for LocalCommandLineCodeExecutor\n            pass\n\n        @message_handler\n        async def handle_execute_code(self, message: ExecuteCode, ctx: MessageContext) -> ExecutionResult:\n            code = message.code\n            result = await self._code_executor.execute_code_blocks(\n                code_blocks=[CodeBlock(language=\"python\", code=code)],\n                cancellation_token=ctx.cancellation_token,\n            )\n            output = result.output\n            return ExecutionResult(output=output)\n\n    async def create_code_execution_agent():\n        agent = CodeExecutionAgent()\n        await agent.start_code_executor()\n        return agent\n\n    async def main():\n        queue = asyncio.Queue[FinalAnswer]()\n\n        async def output_result(_agent: ClosureContext, message: FinalAnswer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        runtime = SingleThreadedAgentRuntime()\n\n        # Register agents\n        await PALAgent.register(runtime, \"pal_agent\", lambda: PALAgent(model_client, code_agent_type=\"code_execution_agent\"))\n        await CodeExecutionAgent.register(runtime, \"code_execution_agent\", create_code_execution_agent)\n\n        # Register ClosureAgent\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(runtime, \"output_result\", output_result, subscriptions=lambda: [result_topic])\n\n        runtime.start()\n        # Send initial message directly to PALAgent\n        pal_agent_id = AgentId(\"pal_agent\", \"default\")\n        await runtime.send_message(Message(content=task), pal_agent_id)\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer\n        final_message = await queue.get()\n        return final_message.answer\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 0.0%), Median: 0.4%",
        "generation": 2
    },
    {
        "thought": "**Insights:**\nComplex reasoning tasks can be effectively solved by decomposing them into sequential sub-questions. The \"Least-to-Most\" prompting strategy enables the model to tackle simpler questions first and build up to solving the original complex question.\n\n**Overall Idea:**\nDesign an agent that employs the \"Least-to-Most\" approach. The agent first prompts the LLM to decompose the main question into a sequence of sub-questions. It then iteratively answers each sub-question, using previous answers as context. Finally, it synthesizes these sub-answers to formulate the final answer.\n\n**Implementation:**\n- Implement a \"LeastToMostAgent\" that:\n  - Prompts the LLM to decompose the main question.\n  - Iteratively answers each sub-question, updating the context.\n  - Synthesizes the final answer based on all sub-answers.\n- Ensure correct message passing and topic subscriptions.\n- Publish the final answer to the topic that the \"ClosureAgent\" is subscribed to.",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    from dataclasses import dataclass\n    from typing import List\n    from autogen_core import SingleThreadedAgentRuntime, RoutedAgent, ClosureAgent, ClosureContext, message_handler, TypeSubscription\n    from autogen_core.base import MessageContext, AgentId, TopicId\n    from autogen_core.components.models import ChatCompletionClient, SystemMessage, UserMessage\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True\n        }\n    )\n\n    @dataclass\n    class TaskMessage:\n        content: str\n\n    @dataclass\n    class FinalAnswer:\n        content: str\n\n    class LeastToMostAgent(RoutedAgent):\n        def __init__(self):\n            super().__init__(\"Least-To-Most Agent\")\n            self._model_client = model_client\n            self._system_prompt = \"You are a helpful assistant who can decompose complex questions into simpler sub-questions and answer them sequentially to solve the original problem.\"\n\n        @message_handler\n        async def handle_task(self, message: TaskMessage, ctx: MessageContext) -> None:\n            # Step 1: Decompose the task into sub-questions\n            decomposition_prompt = f\"Decompose the following question into a sequence of simpler sub-questions that can help answer the original question:\\n\\nQuestion: {message.content}\\n\\nProvide the sub-questions as a numbered list.\"\n            msgs = [SystemMessage(content=self._system_prompt), UserMessage(content=decomposition_prompt, source=\"user\")]\n            decomposition_response = await self._model_client.create(msgs, cancellation_token=ctx.cancellation_token)\n            assert isinstance(decomposition_response.content, str)\n            # Extract the sub-questions\n            sub_questions = []\n            for line in decomposition_response.content.strip().split('\\n'):\n                if line.strip():\n                    # Remove numbering if any\n                    question = line.strip().lstrip('1234567890. ').strip()\n                    sub_questions.append(question)\n            # Initialize list for sub-answers\n            sub_answers = []\n            # Step 2: Iteratively answer each sub-question\n            for i, sub_question in enumerate(sub_questions):\n                sub_context = '\\n'.join([f\"Q: {q}\\nA: {a}\" for q, a in zip(sub_questions[:i], sub_answers)])\n                sub_question_prompt = f\"{sub_context}\\n\\nNow answer the following question:\\n\\n{sub_question}\"\n                msgs = [SystemMessage(content=self._system_prompt), UserMessage(content=sub_question_prompt, source=\"user\")]\n                answer_response = await self._model_client.create(msgs, cancellation_token=ctx.cancellation_token)\n                assert isinstance(answer_response.content, str)\n                sub_answers.append(answer_response.content.strip())\n            # Step 3: Synthesize the final answer\n            sub_qa_pairs = '\\n'.join([f\"Q{i+1}: {q}\\nA{i+1}: {a}\" for i, (q, a) in enumerate(zip(sub_questions, sub_answers))])\n            synthesis_prompt = f\"Original question: {message.content}\\n\\nSub-questions and answers:\\n{sub_qa_pairs}\\n\\nBased on the above, provide a final, comprehensive answer to the original question.\"\n            msgs = [SystemMessage(content=self._system_prompt), UserMessage(content=synthesis_prompt, source=\"user\")]\n            final_response = await self._model_client.create(msgs, cancellation_token=ctx.cancellation_token)\n            assert isinstance(final_response.content, str)\n            # Publish the final answer\n            await self.publish_message(\n                FinalAnswer(content=final_response.content.strip()),\n                topic_id=TopicId(\"result\", \"output_result\")\n            )\n\n    async def main():\n        # Create a queue to collect the final answer\n        queue = asyncio.Queue[FinalAnswer]()\n\n        async def output_result(_agent: ClosureContext, message: FinalAnswer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Register agents\n        await LeastToMostAgent.register(runtime, \"least_to_most_agent\", lambda: LeastToMostAgent())\n\n        # Register closure agent to collect the final answer\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(\n            runtime,\n            \"output_result\",\n            output_result,\n            subscriptions=lambda: [result_topic]\n        )\n\n        # Start the runtime\n        runtime.start()\n        # Send the initial message directly to the agent\n        least_to_most_agent_id = AgentId(\"least_to_most_agent\", \"default\")\n        await runtime.send_message(\n            TaskMessage(content=task),\n            least_to_most_agent_id\n        )\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer from the queue\n        final_message = await queue.get()\n        return final_message.content\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (3.0%, 3.4%), Median: 4.3%",
        "generation": 6
    },
    {
        "thought": "By implementing the ReAct prompting strategy, we enable the agent to interleave reasoning and actions, allowing it to interact with the passage or other external sources during problem-solving. This approach is novel compared to the existing methods in the archive and can enhance performance on complex tasks that require dynamic information retrieval and processing.\n\n**Implementation:**\n- Update the system prompt to clearly specify the expected output format, including Thought, Action, Observation, and Answer.\n- Implement robust parsing of the assistant's responses to handle multiple steps and loops.\n- Improve the 'perform_action' method to handle different actions and queries more effectively.\n- Ensure that the conversation history is maintained correctly, and observations are integrated properly.\n- The 'ReActAgent' will publish the final answer to the 'result' topic, which the 'ClosureAgent' subscribes to.",
        "name": "ReAct Prompting",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import re\n    from dataclasses import dataclass\n    from typing import List\n    from autogen_core import SingleThreadedAgentRuntime, RoutedAgent, ClosureAgent, ClosureContext, message_handler, TypeSubscription\n    from autogen_core.base import MessageContext, AgentId, TopicId\n    from autogen_core.components.models import SystemMessage, UserMessage, AssistantMessage, ChatCompletionClient, LLMMessage\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True\n        }\n    )\n\n    @dataclass\n    class TaskMessage:\n        content: str\n\n    @dataclass\n    class FinalAnswer:\n        content: str\n\n    class ReActAgent(RoutedAgent):\n        def __init__(self, passage: str, model_client: ChatCompletionClient):\n            super().__init__(\"ReAct Agent\")\n            self.passage = passage\n            self.model_client = model_client\n            self.system_prompt = (\n                \"You are an AI assistant that uses reasoning and actions to answer the question based on the provided passage. \"\n                \"At each step, follow this format:\\n\"\n                \"Thought: you should think about the problem\\n\"\n                \"Action: the action you want to take, choices are [Search], format \\\"Action: <action>[<query>]\\\"\\n\"\n                \"Observation: the result of the action\\n\"\n                \"... (this Thought/Action/Observation can repeat N times)\\n\"\n                \"Thought: I now have enough information to answer the question\\n\"\n                \"Answer: the final answer to the original question\\n\"\n                \"\\n\"\n                \"Begin.\"\n            )\n\n        @message_handler\n        async def handle_task(self, message: TaskMessage, ctx: MessageContext) -> None:\n            conversation: List[LLMMessage] = []\n            conversation.append(SystemMessage(content=self.system_prompt))\n            # Include the passage in the user's message\n            user_content = f\"Passage:\\n{self.passage}\\n\\nQuestion:\\n{message.content}\"\n            conversation.append(UserMessage(content=user_content, source=\"user\"))\n\n            while True:\n                response = await self.model_client.create(\n                    conversation,\n                    cancellation_token=ctx.cancellation_token\n                )\n                assert isinstance(response.content, str)\n                # Append assistant's message\n                assistant_message = response.content.strip()\n                conversation.append(AssistantMessage(content=assistant_message, source=\"assistant\"))\n\n                # Parse the assistant's message\n                actions = self.parse_actions(assistant_message)\n                if actions:\n                    for action in actions:\n                        # Perform action\n                        observation = self.perform_action(action)\n                        # Append observation to conversation\n                        conversation.append(AssistantMessage(content=f\"Observation: {observation}\", source=\"assistant\"))\n                elif \"Answer:\" in assistant_message:\n                    final_answer = assistant_message.split(\"Answer:\")[1].strip()\n                    await self.publish_message(\n                        FinalAnswer(content=final_answer),\n                        topic_id=TopicId(\"result\", \"output_result\")\n                    )\n                    break\n                else:\n                    # No action or answer, continue the loop\n                    continue\n\n        def parse_actions(self, assistant_message: str) -> List[str]:\n            # Find all occurrences of Action: ...\n            actions = re.findall(r\"Action:\\s*(.*)\", assistant_message)\n            return actions\n\n        def perform_action(self, action: str) -> str:\n            # For simplicity, define \"Search[<query>]\" action\n            if action.startswith(\"Search\"):\n                # Extract the query\n                query_match = re.match(r\"Search\\[(.*)\\]\", action)\n                if query_match:\n                    query = query_match.group(1)\n                    # Search the passage for the query\n                    if query.lower() in self.passage.lower():\n                        return f\"Found information about '{query}' in the passage.\"\n                    else:\n                        return f\"No information found about '{query}' in the passage.\"\n                else:\n                    return \"Invalid action format.\"\n            else:\n                return \"Unknown action.\"\n\n    async def main():\n        # Create a queue to collect the final answer\n        queue = asyncio.Queue()\n\n        async def output_result(_agent: ClosureContext, message: FinalAnswer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Extract passage and question from the task\n        task_content = str(task)\n\n        passage_match = re.search(r\"Passage:\\s*(.*?)\\nQuestion:\", task_content, re.DOTALL)\n        question_match = re.search(r\"Question:\\s*(.*)\", task_content, re.DOTALL)\n\n        passage = passage_match.group(1).strip() if passage_match else \"\"\n        question = question_match.group(1).strip() if question_match else \"\"\n\n        # Register the ReActAgent\n        await ReActAgent.register(runtime, \"react_agent\", lambda: ReActAgent(passage, model_client))\n\n        # Register closure agent to collect the final answer\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(\n            runtime,\n            \"output_result\",\n            output_result,\n            subscriptions=lambda: [result_topic]\n        )\n\n        # Start the runtime\n        runtime.start()\n\n        # Send the initial message to the agent\n        react_agent_id = AgentId(\"react_agent\", \"default\")\n        await runtime.send_message(\n            TaskMessage(content=question),\n            react_agent_id\n        )\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer from the queue\n        final_message = await queue.get()\n        return final_message.content\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (18.1%, 20.3%), Median: 24.9%",
        "generation": 9
    },
    {
        "thought": "**Insights:**\nComplex tasks can often be solved more effectively by drawing parallels with previously solved problems. This analogical reasoning allows for the application of insights and strategies from analogous cases to the current task.\n\n**Overall Idea:**\nDesign an 'Analogical Reasoning Agent' system where a 'RetrievalAgent' retrieves similar problems and their solutions from a knowledge base. A 'ReasoningAgent' then uses this information, along with the original task, to construct a well-informed answer.\n\n**Implementation:**\n- Implement a 'RetrievalAgent' that fetches analogous problems and their solutions based on the input task.\n- 'RetrievalAgent' sends this information to the 'ReasoningAgent'.\n- 'ReasoningAgent' uses the retrieved analogies and the original task to generate the final answer.\n- 'ReasoningAgent' publishes the final answer to the 'result' topic.\n- A 'ClosureAgent' subscribed to the 'result' topic collects the final answer.",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    from dataclasses import dataclass\n    from typing import List\n    from autogen_core import (\n        SingleThreadedAgentRuntime,\n        RoutedAgent,\n        ClosureAgent,\n        ClosureContext,\n        message_handler,\n        TypeSubscription\n    )\n    from autogen_core.base import MessageContext, AgentId, TopicId\n    from autogen_core.components.models import ChatCompletionClient, SystemMessage, UserMessage\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    # Initialize the token provider and model client\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\"vision\": True, \"function_calling\": True, \"json_output\": True}\n    )\n\n    # Define message types\n    @dataclass\n    class TaskMessage:\n        content: str\n\n    @dataclass\n    class Analogies:\n        analogies: List[str]\n\n    @dataclass\n    class FinalAnswer:\n        content: str\n\n    # Implement the RetrievalAgent\n    class RetrievalAgent(RoutedAgent):\n        def __init__(self):\n            super().__init__(\"Retrieval Agent\")\n            self._model_client = model_client\n            self._system_prompt = (\n                \"You are an assistant that retrieves analogies for a given problem from a knowledge base.\"\n            )\n\n        @message_handler\n        async def handle_task(self, message: TaskMessage, ctx: MessageContext) -> None:\n            prompt = (\n                f\"Problem: {message.content}\\n\\n\"\n                \"Retrieve similar problems and their solutions that could help solve this problem. \"\n                \"Present them as a list of analogies.\"\n            )\n            msgs = [\n                SystemMessage(content=self._system_prompt),\n                UserMessage(content=prompt, source=\"user\")\n            ]\n            response = await self._model_client.create(\n                msgs, cancellation_token=ctx.cancellation_token\n            )\n            assert isinstance(response.content, str)\n            analogies = []\n            for line in response.content.strip().split(\"\\n\"):\n                if line.strip():\n                    analogy = line.strip().lstrip(\"1234567890. \").strip()\n                    analogies.append(analogy)\n            reasoning_agent_id = AgentId(\"reasoning_agent\", self.id.key)\n            await self.send_message(\n                Analogies(analogies=analogies), reasoning_agent_id\n            )\n\n    # Implement the ReasoningAgent\n    class ReasoningAgent(RoutedAgent):\n        def __init__(self):\n            super().__init__(\"Reasoning Agent\")\n            self._model_client = model_client\n            self._system_prompt = (\n                \"You are an assistant that uses analogies to solve complex problems.\"\n            )\n            self._task_content = \"\"\n\n        @message_handler\n        async def handle_task(self, message: TaskMessage, ctx: MessageContext) -> None:\n            self._task_content = message.content\n            retrieval_agent_id = AgentId(\"retrieval_agent\", self.id.key)\n            await self.send_message(message, retrieval_agent_id)\n\n        @message_handler\n        async def handle_analogies(self, message: Analogies, ctx: MessageContext) -> None:\n            analogies_text = \"\\n\".join([\n                f\"Analogy {i+1}: {a}\" for i, a in enumerate(message.analogies)\n            ])\n            prompt = (\n                f\"Problem: {self._task_content}\\n\\n\"\n                f\"Analogies:\\n{analogies_text}\\n\\n\"\n                \"Using the analogies above, provide a detailed solution to the problem.\"\n            )\n            msgs = [\n                SystemMessage(content=self._system_prompt),\n                UserMessage(content=prompt, source=\"user\")\n            ]\n            response = await self._model_client.create(\n                msgs, cancellation_token=ctx.cancellation_token\n            )\n            assert isinstance(response.content, str)\n            await self.publish_message(\n                FinalAnswer(content=response.content.strip()),\n                topic_id=TopicId(\"result\", \"output_result\")\n            )\n\n    # Main function to run the agent system\n    async def main():\n        queue = asyncio.Queue()\n\n        # ClosureAgent to collect the final answer\n        async def output_result(\n            _agent: ClosureContext, message: FinalAnswer, ctx: MessageContext\n        ) -> None:\n            await queue.put(message)\n\n        runtime = SingleThreadedAgentRuntime()\n\n        # Register agents\n        await RetrievalAgent.register(runtime, \"retrieval_agent\", lambda: RetrievalAgent())\n        await ReasoningAgent.register(runtime, \"reasoning_agent\", lambda: ReasoningAgent())\n\n        # Register ClosureAgent to collect the final answer\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(\n            runtime,\n            \"output_result\",\n            output_result,\n            subscriptions=lambda: [result_topic]\n        )\n\n        # Start the runtime and send the initial message\n        runtime.start()\n        reasoning_agent_id = AgentId(\"reasoning_agent\", \"default\")\n        await runtime.send_message(\n            TaskMessage(content=task), reasoning_agent_id\n        )\n\n        # Wait until the system is idle and collect the final answer\n        await runtime.stop_when_idle()\n        final_message = await queue.get()\n        return final_message.content\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (2.2%, 2.4%), Median: 3.0%",
        "generation": 10
    },
    {
        "thought": "**Insights:**\nImplementing an explicit fact-checking step can enhance the reliability of the generated answers, ensuring they are grounded in the provided passage. This approach is novel compared to existing methods in the archive.\n\n**Overall Idea:**\nDesign an agent system where an \"AnswerGenerationAgent\" generates an initial answer, and a \"FactCheckingAgent\" verifies each assertion against the passage. If any assertions are unsupported, feedback is provided to refine the answer. This iterative process continues until the answer is fully supported or a maximum number of iterations is reached.\n\n**Implementation:**\n- Implement \"AnswerGenerationAgent\" that generates and refines the answer based on feedback.\n- Implement \"FactCheckingAgent\" that verifies each assertion and provides feedback.\n- Ensure robust communication between agents.\n- Publish the final verified answer to the \"result\" topic, which the \"ClosureAgent\" subscribes to.",
        "name": "Fact-Checking Agent",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import re\n    from dataclasses import dataclass\n    from typing import List\n    from autogen_core import SingleThreadedAgentRuntime, RoutedAgent, ClosureAgent, ClosureContext, message_handler, TypeSubscription\n    from autogen_core.base import MessageContext, AgentId, TopicId\n    from autogen_core.components.models import ChatCompletionClient, SystemMessage, UserMessage\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True\n        }\n    )\n\n    @dataclass\n    class TaskMessage:\n        passage: str\n        question: str\n\n    @dataclass\n    class FactCheckRequest:\n        passage: str\n        answer: str\n\n    @dataclass\n    class FactCheckResponse:\n        feedback: str\n        is_verified: bool\n\n    @dataclass\n    class FinalAnswer:\n        content: str\n\n    class AnswerGenerationAgent(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient, fact_checker_agent_type: str):\n            super().__init__(\"Answer Generation Agent\")\n            self.model_client = model_client\n            self.fact_checker_agent_id = AgentId(fact_checker_agent_type, self.id.key)\n            self.max_iterations = 3\n\n        @message_handler\n        async def handle_task(self, message: TaskMessage, ctx: MessageContext) -> None:\n            iteration = 0\n            passage = message.passage\n            question = message.question\n            answer = \"\"\n            while iteration < self.max_iterations:\n                # Generate answer\n                prompt = f\"Passage:\\n{passage}\\n\\nQuestion:\\n{question}\\n\\nAnswer the question based solely on the passage. Be concise and ensure all statements are directly supported by the passage.\"\n                msgs = [SystemMessage(content=\"You are a helpful assistant.\"), UserMessage(content=prompt, source=\"user\")]\n                response = await self.model_client.create(msgs, cancellation_token=ctx.cancellation_token)\n                assert isinstance(response.content, str)\n                answer = response.content.strip()\n                # Send to FactCheckingAgent\n                fact_check_request = FactCheckRequest(passage=passage, answer=answer)\n                fact_check_response = await self.send_message(fact_check_request, self.fact_checker_agent_id)\n                if fact_check_response.is_verified:\n                    # Publish final answer\n                    await self.publish_message(FinalAnswer(content=answer), topic_id=TopicId(\"result\", \"output_result\"))\n                    return\n                else:\n                    # Revise answer based on feedback\n                    feedback = fact_check_response.feedback\n                    revision_prompt = f\"Passage:\\n{passage}\\n\\nQuestion:\\n{question}\\n\\nPrevious Answer:\\n{answer}\\n\\nFeedback:\\n{feedback}\\n\\nBased on the feedback, please provide a new answer that is fully supported by the passage.\"\n                    msgs = [SystemMessage(content=\"You are a helpful assistant.\"), UserMessage(content=revision_prompt, source=\"user\")]\n                    response = await self.model_client.create(msgs, cancellation_token=ctx.cancellation_token)\n                    assert isinstance(response.content, str)\n                    answer = response.content.strip()\n                iteration += 1\n            # After max iterations, publish the last answer\n            await self.publish_message(FinalAnswer(content=answer), topic_id=TopicId(\"result\", \"output_result\"))\n\n    class FactCheckingAgent(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient):\n            super().__init__(\"Fact Checking Agent\")\n            self.model_client = model_client\n\n        @message_handler\n        async def handle_fact_check_request(self, message: FactCheckRequest, ctx: MessageContext) -> FactCheckResponse:\n            passage = message.passage\n            answer = message.answer\n            # Fact-check the answer\n            prompt = f\"Passage:\\n{passage}\\n\\nAnswer:\\n{answer}\\n\\nFor each statement in the answer, determine if it is directly supported by the passage. List any unsupported statements and explain why. If all statements are supported, simply state 'All statements are supported.'\"\n            msgs = [SystemMessage(content=\"You are a fact-checking assistant.\"), UserMessage(content=prompt, source=\"user\")]\n            response = await self.model_client.create(msgs, cancellation_token=ctx.cancellation_token)\n            assert isinstance(response.content, str)\n            verification = response.content.strip()\n            if \"All statements are supported.\" in verification:\n                return FactCheckResponse(feedback=verification, is_verified=True)\n            else:\n                return FactCheckResponse(feedback=verification, is_verified=False)\n\n    async def main():\n        # Create a queue to collect the final answer\n        queue = asyncio.Queue[FinalAnswer]()\n\n        async def output_result(_agent: ClosureContext, message: FinalAnswer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Extract passage and question from the task\n        task_content = str(task)\n\n        passage_match = re.search(r\"Passage:\\s*(.*?)\\nQuestion:\", task_content, re.DOTALL)\n        question_match = re.search(r\"Question:\\s*(.*)\", task_content, re.DOTALL)\n\n        passage = passage_match.group(1).strip() if passage_match else \"\"\n        question = question_match.group(1).strip() if question_match else \"\"\n\n        # Register agents\n        await AnswerGenerationAgent.register(runtime, \"answer_generation_agent\", lambda: AnswerGenerationAgent(model_client, fact_checker_agent_type=\"fact_checking_agent\"))\n        await FactCheckingAgent.register(runtime, \"fact_checking_agent\", lambda: FactCheckingAgent(model_client))\n\n        # Register ClosureAgent to collect the final answer\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(\n            runtime,\n            \"output_result\",\n            output_result,\n            subscriptions=lambda: [result_topic]\n        )\n\n        # Start the runtime\n        runtime.start()\n\n        # Send the initial task message to AnswerGenerationAgent\n        answer_generation_agent_id = AgentId(\"answer_generation_agent\", \"default\")\n        await runtime.send_message(\n            TaskMessage(passage=passage, question=question),\n            answer_generation_agent_id\n        )\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer from the queue\n        final_message = await queue.get()\n        return final_message.content\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (6.9%, 8.0%), Median: 10.7%",
        "generation": 11
    },
    {
        "thought": "**Insights:**\nEncouraging the agent to challenge its own answer by seeking potential refutations can enhance the accuracy and robustness of the final solution. This method promotes critical thinking and self-evaluation, helping to identify and correct errors that might have been overlooked.\n\n**Overall Idea:**\nDesign an agent that first generates an initial answer with detailed reasoning. Then, a 'RefutationAgent' attempts to find flaws, counterarguments, or errors in the reasoning and answer. Based on this feedback, the 'AnswerGenerationAgent' refines the answer to address the identified issues. This iterative process continues until the answer withstands the refutation attempts or a maximum number of iterations is reached.\n\n**Implementation:**\n- Implement an 'AnswerGenerationAgent' that generates an initial answer and reasoning.\n- Implement a 'RefutationAgent' that critiques the answer by finding potential errors or weaknesses.\n- The 'AnswerGenerationAgent' uses the feedback to refine the answer.\n- The process repeats for a set number of iterations.\n- The final, refined answer is published to the 'result' topic, which the 'ClosureAgent' subscribes to.",
        "name": "Conjecture and Refutation Agent",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import re\n    from dataclasses import dataclass\n    from typing import List\n    from autogen_core import (\n        SingleThreadedAgentRuntime,\n        RoutedAgent,\n        ClosureAgent,\n        ClosureContext,\n        message_handler,\n        TypeSubscription,\n    )\n    from autogen_core.base import MessageContext, AgentId, TopicId\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        SystemMessage,\n        UserMessage,\n    )\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(\n        DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n    )\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n\n    @dataclass\n    class TaskMessage:\n        content: str\n\n    @dataclass\n    class AnswerAndReasoning:\n        answer: str\n        reasoning: str\n\n    @dataclass\n    class RefutationRequest:\n        answer: str\n        reasoning: str\n        iteration: int\n\n    @dataclass\n    class RefutationResponse:\n        feedback: str\n        needs_revision: bool\n\n    @dataclass\n    class FinalAnswer:\n        content: str\n\n    class AnswerGenerationAgent(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient, refutation_agent_type: str):\n            super().__init__(\"Answer Generation Agent\")\n            self.model_client = model_client\n            self.refutation_agent_id = AgentId(refutation_agent_type, self.id.key)\n            self.max_iterations = 2\n\n        @message_handler\n        async def handle_task(self, message: TaskMessage, ctx: MessageContext) -> None:\n            iteration = 0\n            question = message.content\n            while iteration < self.max_iterations:\n                # Generate answer with reasoning\n                prompt = (\n                    \"You are an expert assistant. Please provide a detailed reasoning to solve the following question, and then provide a concise final answer.\\n\\n\"\n                    f\"Question:\\n{question}\\n\\n\"\n                    \"Present your reasoning and final answer in the following format:\\n\"\n                    \"Reasoning:\\n<Your detailed reasoning>\\n\\n\"\n                    \"Answer:\\n<Your concise final answer>\"\n                )\n                msgs = [\n                    SystemMessage(content=\"You are a helpful assistant.\"),\n                    UserMessage(content=prompt, source=\"user\"),\n                ]\n                response = await self.model_client.create(\n                    msgs, cancellation_token=ctx.cancellation_token\n                )\n                assert isinstance(response.content, str)\n                # Extract reasoning and answer\n                reasoning_match = re.search(\n                    r\"Reasoning:\\s*(.+?)\\n\\nAnswer:\", response.content, re.DOTALL\n                )\n                answer_match = re.search(r\"Answer:\\s*(.+)\", response.content, re.DOTALL)\n                if reasoning_match and answer_match:\n                    reasoning = reasoning_match.group(1).strip()\n                    answer = answer_match.group(1).strip()\n                else:\n                    # If parsing fails, consider the whole response as answer\n                    reasoning = \"\"\n                    answer = response.content.strip()\n                # Send to RefutationAgent for critique\n                refutation_request = RefutationRequest(\n                    answer=answer, reasoning=reasoning, iteration=iteration\n                )\n                refutation_response = await self.send_message(\n                    refutation_request, self.refutation_agent_id\n                )\n                if not refutation_response.needs_revision:\n                    # Publish final answer\n                    await self.publish_message(\n                        FinalAnswer(content=answer),\n                        topic_id=TopicId(\"result\", \"output_result\"),\n                    )\n                    return\n                else:\n                    # Refine answer based on feedback\n                    feedback = refutation_response.feedback\n                    revision_prompt = (\n                        f\"Your previous reasoning and answer were critiqued as follows:\\n{feedback}\\n\\n\"\n                        \"Please revise your reasoning and answer to address the critique.\"\n                    )\n                    msgs = [\n                        SystemMessage(content=\"You are a helpful assistant.\"),\n                        UserMessage(content=revision_prompt, source=\"user\"),\n                    ]\n                    response = await self.model_client.create(\n                        msgs, cancellation_token=ctx.cancellation_token\n                    )\n                    assert isinstance(response.content, str)\n                    # Update the answer and reasoning\n                    reasoning_match = re.search(\n                        r\"Reasoning:\\s*(.+?)\\n\\nAnswer:\", response.content, re.DOTALL\n                    )\n                    answer_match = re.search(\n                        r\"Answer:\\s*(.+)\", response.content, re.DOTALL\n                    )\n                    if reasoning_match and answer_match:\n                        reasoning = reasoning_match.group(1).strip()\n                        answer = answer_match.group(1).strip()\n                    else:\n                        reasoning = \"\"\n                        answer = response.content.strip()\n                iteration += 1\n            # After max iterations, publish the last answer\n            await self.publish_message(\n                FinalAnswer(content=answer),\n                topic_id=TopicId(\"result\", \"output_result\"),\n            )\n\n    class RefutationAgent(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient):\n            super().__init__(\"Refutation Agent\")\n            self.model_client = model_client\n\n        @message_handler\n        async def handle_refutation_request(\n            self, message: RefutationRequest, ctx: MessageContext\n        ) -> RefutationResponse:\n            # Attempt to find flaws or counterarguments\n            prompt = (\n                \"As a Refutation Agent, your task is to critically evaluate the given reasoning and answer, and attempt to find any flaws, errors, or weaknesses. If you find significant issues, provide a detailed critique. If the reasoning and answer are solid, state that they are acceptable.\\n\\n\"\n                f\"Reasoning:\\n{message.reasoning}\\n\\n\"\n                f\"Answer:\\n{message.answer}\\n\\n\"\n                \"Provide your critique in the following format:\\n\"\n                \"Critique:\\n<Your detailed critique or confirmation>\\n\\n\"\n                \"Needs Revision:\\n<Yes or No>\"\n            )\n            msgs = [\n                SystemMessage(content=\"You are a logical and critical assistant.\"),\n                UserMessage(content=prompt, source=\"user\"),\n            ]\n            response = await self.model_client.create(\n                msgs, cancellation_token=ctx.cancellation_token\n            )\n            assert isinstance(response.content, str)\n            # Parse the response\n            needs_revision_match = re.search(\n                r\"Needs Revision:\\s*(Yes|No)\", response.content, re.IGNORECASE\n            )\n            if needs_revision_match:\n                needs_revision = needs_revision_match.group(1).strip().lower() == \"yes\"\n            else:\n                needs_revision = True  # Assume needs revision if parsing fails\n            critique_match = re.search(\n                r\"Critique:\\s*(.+?)\\n\\nNeeds Revision:\", response.content, re.DOTALL\n            )\n            if critique_match:\n                feedback = critique_match.group(1).strip()\n            else:\n                feedback = response.content.strip()\n            return RefutationResponse(\n                feedback=feedback, needs_revision=needs_revision\n            )\n\n    async def main():\n        # Create a queue to collect the final answer\n        queue = asyncio.Queue[FinalAnswer]()\n\n        async def output_result(\n            _agent: ClosureContext, message: FinalAnswer, ctx: MessageContext\n        ) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Register agents\n        await AnswerGenerationAgent.register(\n            runtime,\n            \"answer_generation_agent\",\n            lambda: AnswerGenerationAgent(\n                model_client, refutation_agent_type=\"refutation_agent\"\n            ),\n        )\n        await RefutationAgent.register(\n            runtime, \"refutation_agent\", lambda: RefutationAgent(model_client)\n        )\n\n        # Register ClosureAgent to collect the final answer\n        result_topic = TypeSubscription(\n            topic_type=\"result\", agent_type=\"output_result\"\n        )\n        await ClosureAgent.register_closure(\n            runtime,\n            \"output_result\",\n            output_result,\n            subscriptions=lambda: [result_topic],\n        )\n\n        # Start the runtime\n        runtime.start()\n\n        # Send the initial task message to AnswerGenerationAgent\n        answer_generation_agent_id = AgentId(\"answer_generation_agent\", \"default\")\n        await runtime.send_message(\n            TaskMessage(content=task), answer_generation_agent_id\n        )\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer from the queue\n        final_message = await queue.get()\n        return final_message.content\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (56.4%, 60.9%), Median: 69.4%",
        "generation": 12
    },
    {
        "thought": "**Insights:**\nCombining ReAct prompting with tool usage allows the agent to perform precise computations and interact with external tools during reasoning. By correcting the implementation mistakes and enhancing error handling and security measures, we can improve the agent's performance on complex tasks requiring calculations.\n\n**Implementation:**\n- Correct the regular expression in `parse_actions` to properly extract actions and inputs.\n- Remove the unnecessary `start_code_executor` method from `CodeExecutionAgent`.\n- Add error handling in `CodeExecutionAgent` to manage code execution failures gracefully.\n- Execute code within a Docker container to ensure a secure and sandboxed environment.\n- Improve the extraction of `passage` and `question` from `task` for robustness.\n- Ensure observations are correctly added to the conversation history.\n- Update `CodeExecutionAgent` to override `on_started` and `on_stopped` methods to manage the code executor lifecycle.\n- Adjust agent registration to use lambda functions directly, removing unnecessary functions.\n",
        "name": "Tool-Augmented ReAct Agent",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import re\n    from dataclasses import dataclass\n    from typing import List\n    from autogen_core import SingleThreadedAgentRuntime, RoutedAgent, ClosureAgent, ClosureContext, message_handler, TypeSubscription\n    from autogen_core.base import MessageContext, AgentId, TopicId\n    from autogen_core.components.models import ChatCompletionClient, SystemMessage, UserMessage, AssistantMessage, LLMMessage\n    from autogen_core.code_executor import CodeBlock\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True\n        }\n    )\n\n    @dataclass\n    class TaskMessage:\n        content: str\n\n    @dataclass\n    class FinalAnswer:\n        content: str\n\n    @dataclass\n    class ExecuteCode:\n        code: str\n\n    @dataclass\n    class ExecutionResult:\n        output: str\n\n    class ToolAugmentedReActAgent(RoutedAgent):\n        def __init__(self, passage: str, model_client: ChatCompletionClient, code_agent_type: str):\n            super().__init__(\"Tool-Augmented ReAct Agent\")\n            self.passage = passage\n            self.model_client = model_client\n            self.code_agent_id = AgentId(code_agent_type, self.id.key)\n            self.system_prompt = (\n                \"You are an AI assistant that uses reasoning and actions to answer the question based on the provided passage. \"\n                \"At each step, follow this format:\\n\"\n                \"Thought: you should think about the problem\\n\"\n                \"Action: the action you want to take, choices are [Search, Calculate], format 'Action: <action>[<input>]'. For calculations, you can write Python code.\\n\"\n                \"Observation: the result of the action\\n\"\n                \"... (this Thought/Action/Observation can repeat N times)\\n\"\n                \"Thought: I now have enough information to answer the question\\n\"\n                \"Answer: the final answer to the original question\\n\"\n                \"\\n\"\n                \"Begin.\"\n            )\n\n        @message_handler\n        async def handle_task(self, message: TaskMessage, ctx: MessageContext) -> None:\n            conversation: List[LLMMessage] = []\n            conversation.append(SystemMessage(content=self.system_prompt))\n            # Include the passage in the user's message\n            user_content = f\"Passage:\\n{self.passage}\\n\\nQuestion:\\n{message.content}\"\n            conversation.append(UserMessage(content=user_content, source=\"user\"))\n\n            while True:\n                response = await self.model_client.create(\n                    conversation,\n                    cancellation_token=ctx.cancellation_token\n                )\n                assert isinstance(response.content, str)\n                # Append assistant's message\n                assistant_message = response.content.strip()\n                conversation.append(AssistantMessage(content=assistant_message, source=\"assistant\"))\n\n                # Parse the assistant's message\n                actions = self.parse_actions(assistant_message)\n                if actions:\n                    for action_type, action_input in actions:\n                        if action_type == \"Search\":\n                            observation = self.perform_search(action_input)\n                        elif action_type == \"Calculate\":\n                            # Send code to CodeExecutionAgent for execution\n                            execution_result = await self.send_message(\n                                ExecuteCode(code=action_input), self.code_agent_id\n                            )\n                            observation = execution_result.output.strip()\n                        else:\n                            observation = \"Unknown action.\"\n                        # Append observation to conversation\n                        conversation.append(AssistantMessage(content=f\"Observation: {observation}\", source=\"assistant\"))\n                elif \"Answer:\" in assistant_message:\n                    final_answer = assistant_message.split(\"Answer:\", 1)[1].strip()\n                    await self.publish_message(\n                        FinalAnswer(content=final_answer),\n                        topic_id=TopicId(\"result\", \"output_result\")\n                    )\n                    break\n                else:\n                    # No action or answer, continue the loop\n                    continue\n\n        def parse_actions(self, assistant_message: str):\n            # Find all occurrences of Action: ...\n            action_pattern = r\"Action:\\s*(\\w+)\\[(.*?)\\]\"\n            actions = re.findall(action_pattern, assistant_message, re.DOTALL)\n            return actions\n\n        def perform_search(self, query: str) -> str:\n            # Search the passage for the query\n            if query.lower() in self.passage.lower():\n                return f\"Found information about '{query}' in the passage.\"\n            else:\n                return f\"No information found about '{query}' in the passage.\"\n\n    class CodeExecutionAgent(RoutedAgent):\n        def __init__(self):\n            super().__init__(\"Code Execution Agent\")\n            self.code_executor = DockerCommandLineCodeExecutor()\n\n        async def on_started(self) -> None:\n            await self.code_executor.start()\n\n        async def on_stopped(self) -> None:\n            await self.code_executor.stop()\n\n        @message_handler\n        async def handle_execute_code(self, message: ExecuteCode, ctx: MessageContext) -> ExecutionResult:\n            code = message.code\n            try:\n                result = await self.code_executor.execute_code_blocks(\n                    code_blocks=[CodeBlock(language=\"python\", code=code)],\n                    cancellation_token=ctx.cancellation_token,\n                )\n                output = result.output\n            except Exception as e:\n                output = f\"Error during code execution: {str(e)}\"\n            return ExecutionResult(output=output)\n\n    async def main():\n        # Create a queue to collect the final answer\n        queue = asyncio.Queue()\n\n        async def output_result(_agent: ClosureContext, message: FinalAnswer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Extract passage and question from the task\n        task_content = str(task)\n\n        # Improved passage and question extraction\n        try:\n            passage_start = task_content.index('Passage:') + len('Passage:')\n            question_start = task_content.index('Question:')\n            passage = task_content[passage_start:question_start].strip()\n            question = task_content[question_start + len('Question:'):].strip()\n        except ValueError:\n            passage = \"\"\n            question = task_content.strip()\n\n        # Register agents\n        await ToolAugmentedReActAgent.register(\n            runtime,\n            \"react_agent\",\n            lambda: ToolAugmentedReActAgent(passage, model_client, code_agent_type=\"code_execution_agent\")\n        )\n        await CodeExecutionAgent.register(runtime, \"code_execution_agent\", lambda: CodeExecutionAgent())\n\n        # Register closure agent to collect the final answer\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(\n            runtime,\n            \"output_result\",\n            output_result,\n            subscriptions=lambda: [result_topic]\n        )\n\n        # Start the runtime\n        runtime.start()\n\n        # Send the initial message to the agent\n        react_agent_id = AgentId(\"react_agent\", \"default\")\n        await runtime.send_message(\n            TaskMessage(content=question),\n            react_agent_id\n        )\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer from the queue\n        final_message = await queue.get()\n        return final_message.content\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (20.6%, 22.8%), Median: 27.8%",
        "generation": 17
    },
    {
        "thought": "To improve performance on tasks requiring detailed comprehension and reasoning, we can design an agent that constructs a knowledge graph from the passage and uses it to answer the question. By extracting key entities and relationships, the agent can leverage a structured representation of the information, enhancing its ability to reason over the content.",
        "name": "Knowledge Graph Augmented Reasoning Agent",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import re\n    from dataclasses import dataclass\n    from typing import List\n    from autogen_core import SingleThreadedAgentRuntime, RoutedAgent, ClosureAgent, ClosureContext, message_handler, TypeSubscription\n    from autogen_core.base import MessageContext, AgentId, TopicId\n    from autogen_core.components.models import ChatCompletionClient, SystemMessage, UserMessage\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    # Initialize token provider for Azure OpenAI\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True\n        }\n    )\n\n    @dataclass\n    class TaskMessage:\n        passage: str\n        question: str\n\n    @dataclass\n    class FinalAnswer:\n        content: str\n\n    class KnowledgeGraphAgent(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient):\n            super().__init__(\"Knowledge Graph Agent\")\n            self.model_client = model_client\n\n        @message_handler\n        async def handle_task(self, message: TaskMessage, ctx: MessageContext) -> None:\n            passage = message.passage\n            question = message.question\n\n            # Step 1: Extract knowledge graph\n            system_prompt_extract = (\n                \"You are an AI assistant that extracts key entities and relationships from a passage to build a knowledge graph. \"\n                \"Extract the information as a list of triples in the format (subject, relation, object).\"\n            )\n            user_prompt_extract = f\"Passage:\\n{passage}\\n\\nExtract the knowledge graph as a list of triples.\"\n            messages_extract = [\n                SystemMessage(content=system_prompt_extract),\n                UserMessage(content=user_prompt_extract, source=\"user\")\n            ]\n            response_extract = await self.model_client.create(\n                messages_extract,\n                cancellation_token=ctx.cancellation_token\n            )\n            knowledge_graph_text = response_extract.content.strip()\n\n            # Step 2: Use knowledge graph to answer question\n            system_prompt_answer = (\n                \"You are an AI assistant that answers questions based on the provided knowledge graph. \"\n                \"Use the knowledge graph to reason and provide a detailed answer.\"\n            )\n            user_prompt_answer = f\"Knowledge Graph:\\n{knowledge_graph_text}\\n\\nQuestion:\\n{question}\"\n            messages_answer = [\n                SystemMessage(content=system_prompt_answer),\n                UserMessage(content=user_prompt_answer, source=\"user\")\n            ]\n            response_answer = await self.model_client.create(\n                messages_answer,\n                cancellation_token=ctx.cancellation_token\n            )\n            final_answer = response_answer.content.strip()\n\n            # Publish final answer\n            await self.publish_message(\n                FinalAnswer(content=final_answer),\n                topic_id=TopicId(\"result\", \"output_result\")\n            )\n\n    async def main():\n        # Create a queue to collect the final answer\n        queue = asyncio.Queue()\n\n        async def output_result(_agent: ClosureContext, message: FinalAnswer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Extract passage and question from the task\n        task_content = str(task)\n        try:\n            passage_match = re.search(r\"Passage:\\s*(.*?)\\nQuestion:\", task_content, re.DOTALL)\n            question_match = re.search(r\"Question:\\s*(.*)\", task_content, re.DOTALL)\n            passage = passage_match.group(1).strip() if passage_match else \"\"\n            question = question_match.group(1).strip() if question_match else task_content.strip()\n        except Exception:\n            passage = \"\"\n            question = task_content.strip()\n\n        # Register the agent\n        await KnowledgeGraphAgent.register(\n            runtime,\n            \"knowledge_graph_agent\",\n            lambda: KnowledgeGraphAgent(model_client)\n        )\n\n        # Register closure agent to collect the final answer\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register_closure(\n            runtime,\n            \"output_result\",\n            output_result,\n            subscriptions=lambda: [result_topic]\n        )\n\n        # Start the runtime\n        runtime.start()\n\n        # Send the initial task message to the agent\n        agent_id = AgentId(\"knowledge_graph_agent\", \"default\")\n        await runtime.send_message(\n            TaskMessage(passage=passage, question=question),\n            agent_id\n        )\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer from the queue\n        final_message = await queue.get()\n        return final_message.content\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (5.3%, 6.1%), Median: 7.9%",
        "generation": 19
    }
]