[
    {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import logging\n    import json\n    from dataclasses import dataclass\n    import sys\n    from autogen_core.application import SingleThreadedAgentRuntime\n    from autogen_core.base import AgentId, AgentRuntime, MessageContext\n    from autogen_core.components import DefaultTopicId, RoutedAgent, message_handler, ClosureAgent, DefaultSubscription\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        LLMMessage,\n        SystemMessage,\n        UserMessage,\n    )\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from typing import List\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n\n    # Define message types as data classes\n    @dataclass\n    class ChainOfThoughtTask:\n        task: str\n\n\n    @dataclass\n    class FinalResult:\n        result: str\n\n\n    # Define the Chain-of-Thought Agent\n    class ChainOfThoughtAgent(RoutedAgent):\n        def __init__(self, description: str,\n                    model_client: ChatCompletionClient,\n                    system_prompt: str,\n                    instruction: str,\n            ) -> None:\n            super().__init__(description)\n            self._system_messages: List[LLMMessage] = [\n                SystemMessage(\n                    content=system_prompt,\n                )\n            ]\n            self._model_client = model_client\n            self._instruction = instruction\n\n        @message_handler\n        async def handle_task(self, message: ChainOfThoughtTask, ctx: MessageContext) -> None:\n\n            logging.info(f\"{self._description} received message: {message.task}\")\n            user_prompt = message.task + \"\\n\" + self._instruction\n            msgs = self._system_messages + [UserMessage(content=user_prompt, source=self.metadata[\"type\"])]\n            model_result = await self._model_client.create(msgs)\n            assert isinstance(model_result.content, str)\n\n            await self.publish_message(\n                message=FinalResult(model_result.content),\n                topic_id=DefaultTopicId(),\n            )\n\n\n    # Define the main function to set up and run the agent system\n    async def main():\n\n        # Create a queue to collect final answer\n        queue = asyncio.Queue[FinalResult]()\n        async def output_result(_runtime: AgentRuntime, id: AgentId, message: FinalResult, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Create the chain-of-thought agent\n        agent_id = AgentId(\"COTAgent\", \"default\")\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        await ChainOfThoughtAgent.register(\n            runtime, \"COTAgent\", lambda: ChainOfThoughtAgent(\n                description='Chain-of-Thought Agent',\n                model_client=model_client,\n                system_prompt=\"You are a helpful assistant. Directly answer the question. Keep it very concise.\",\n                instruction=cot_instruction,\n            )\n        )\n        # Create closure agent to collect final output result\n        await ClosureAgent.register(runtime, \"output_result\", output_result, subscriptions=lambda: [DefaultSubscription()])\n\n        # Start the runtime, and publish the first message\n        runtime.start()\n        initial_message = ChainOfThoughtTask(task=task)\n        await runtime.send_message(initial_message, agent_id) # publish_message\n\n        # Keep processing messages until idle.\n        await runtime.stop_when_idle()\n\n        # Return the first answer from the queue\n        return (await queue.get()).result\n\n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (8.4%, 9.6%), Median: 12.4%"
    },
    {
        "thought": "While an LLM can arrive at the correct answer, its reasoning may vary. By repeatedly asking the same question with high temperature settings, we can generate different reasoning paths. We then combine multiple answers from these Chain-of-Thought (CoT) agents to produce a more accurate final answer through ensembling.",
        "name": "Self-Consistency with Chain-of-Thought",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import logging\n    import json\n    from dataclasses import dataclass\n    import sys\n    from autogen_core.application import SingleThreadedAgentRuntime\n    from autogen_core.base import AgentId, AgentRuntime, MessageContext\n    from autogen_core.components import DefaultTopicId, RoutedAgent, message_handler, ClosureAgent, DefaultSubscription\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        LLMMessage,\n        SystemMessage,\n        UserMessage,\n    )\n    from typing import List\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n\n    @dataclass\n    class WorkerTask:\n        task: str\n        previous_results: List[str]\n\n\n    @dataclass\n    class WorkerTaskResult:\n        result: str\n\n\n    @dataclass\n    class UserTask:\n        task: str\n\n\n    @dataclass\n    class FinalResult:\n        result: str\n\n\n    class WorkerAgent(RoutedAgent):\n        def __init__(\n            self,\n            model_client: ChatCompletionClient,\n            instruction: str,\n        ) -> None:\n            super().__init__(description=\"Worker Agent\")\n            self._model_client = model_client\n            self._instruction = instruction\n\n        @message_handler\n        async def handle_task(self, message: WorkerTask, ctx: MessageContext) -> WorkerTaskResult:\n            user_prompt = message.task + \"\\n\" + self._instruction\n\n            if message.previous_results:\n                # If previous results are provided, we need to synthesize them to create a single prompt.\n                # system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n                system_prompt = \"Given all the solutions, reason over them carefully and provide a final answer.\"\n                system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(message.previous_results)])\n                model_result = await self._model_client.create(\n                    [SystemMessage(system_prompt), UserMessage(content=user_prompt, source=\"user\")]\n                )\n            else:\n                # If no previous results are provided, we can simply pass the user query to the model.\n                model_result = await self._model_client.create([UserMessage(content=user_prompt, source=\"user\")])\n            assert isinstance(model_result.content, str)\n            print(f\"{'-'*80}\\nWorker-{self.id}:\\n{model_result.content}\")\n            return WorkerTaskResult(result=model_result.content)\n\n\n    class OrchestratorAgent(RoutedAgent):\n        def __init__(\n            self,\n            model_client: ChatCompletionClient,\n            worker_agent_types: List[str],\n            num_layers: int,\n        ) -> None:\n            super().__init__(description=\"Aggregator Agent\")\n            self._model_client = model_client\n            self._worker_agent_types = worker_agent_types\n            self._num_layers = num_layers\n\n\n        @message_handler\n        async def handle_task(self, message: UserTask, ctx: MessageContext) -> FinalResult:\n            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived task: {message.task}\")\n            # Create task for the first layer.\n            worker_task = WorkerTask(task=message.task, previous_results=[])\n            # Iterate over layers.\n            for i in range(self._num_layers):\n                # Assign workers for this layer.\n                worker_ids = [\n                    AgentId(worker_type, f\"{self.id.key}/layer_{i}/worker_{j}\")\n                    for j, worker_type in enumerate(self._worker_agent_types)\n                ]\n                # Dispatch tasks to workers.\n                print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nDispatch to workers at layer {i}\")\n                results = await asyncio.gather(*[self.send_message(worker_task, worker_id) for worker_id in worker_ids])\n                print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived results from workers at layer {i}\")\n                # Prepare task for the next layer.\n                worker_task = WorkerTask(task=message.task, previous_results=[r.result for r in results])\n            # Perform final aggregation.\n            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nPerforming final aggregation\")\n            # system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n            system_prompt = \"Given all the above solutions, reason over them carefully and provide a final answer.\"\n            system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(worker_task.previous_results)])\n            model_result = await self._model_client.create(\n                [SystemMessage(system_prompt), UserMessage(content=message.task, source=\"user\")]\n            )\n            assert isinstance(model_result.content, str)\n            return FinalResult(result=model_result.content)\n\n    # Define the main function to set up and run the agent system\n    async def main():\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Create the agents\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        await WorkerAgent.register(\n            runtime, \"worker\", lambda: WorkerAgent(model_client=model_client, instruction=cot_instruction)\n        )\n        await OrchestratorAgent.register(\n            runtime,\n            \"orchestrator\",\n            lambda: OrchestratorAgent(\n                model_client=model_client, worker_agent_types=[\"worker\"] * 5, num_layers=1\n            ),\n        )\n\n        # Start the runtime, and publish the first message\n        runtime.start()\n        result = await runtime.send_message(UserTask(task=task), AgentId(\"orchestrator\", \"default\"))\n\n        # Return the result\n        return result.result\n\n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (9.3%, 11.1%), Median: 15.7%"
    },
    {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import json\n    import logging\n    import re\n    import sys\n    import uuid\n    from dataclasses import dataclass\n    from typing import Dict, List, Union\n    from autogen_core.base import MessageContext, TopicId, AgentId, AgentRuntime\n    from autogen_core.components import RoutedAgent, default_subscription, message_handler, TypeSubscription\n    from autogen_core.components.models import (\n        AssistantMessage,\n        ChatCompletionClient,\n        LLMMessage,\n        SystemMessage,\n        UserMessage,\n    )\n    from autogen_core.application import SingleThreadedAgentRuntime\n    from autogen_core.components import DefaultTopicId, RoutedAgent, message_handler, ClosureAgent\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n\n    @dataclass\n    class WritingTask:\n        task: str\n\n\n    @dataclass\n    class WritingResult:\n        task: str\n        answer: str\n        review: str\n\n\n    @dataclass\n    class ReviewTask:\n        session_id: str\n        writing_task: str\n        answer_scratchpad: str\n        answer: str\n\n\n    @dataclass\n    class ReviewResult:\n        review: str\n        session_id: str\n        approved: bool\n\n\n    @default_subscription\n    class WorkerAgent(RoutedAgent):\n        \"An agent that performs writing tasks.\"\n\n        def __init__(self,\n                    model_client: ChatCompletionClient,\n                    instruction: str,\n        ) -> None:\n            super().__init__(\"A helpful assistant\")\n            self._system_messages: List[LLMMessage] = [\n                SystemMessage(\n                    content=\"\"\"You are a helpful assistant. Work with the critic to improve your answer.\n                    Make sure to directly answer the question. Keep it very concise.\n                    Respond using the following format:\n\n    Thoughts: <Your comments>\n    Answer: <Your answer>\n    \"\"\",\n                )\n            ]\n            self._model_client = model_client\n            self._session_memory: Dict[str, List[WritingTask | ReviewTask | ReviewResult]] = {}\n            self._instruction = instruction\n\n        @message_handler\n        async def handle_writing_task(self, message: WritingTask, ctx: MessageContext) -> None:\n            # Store the messages in a temporary memory for this request only.\n            session_id = str(uuid.uuid4())\n            self._session_memory.setdefault(session_id, []).append(message)\n            # Generate a response using the chat completion API.\n            response = await self._model_client.create(\n                self._system_messages + [UserMessage(content=message.task + self._instruction, source=self.metadata[\"type\"])],\n                cancellation_token=ctx.cancellation_token,\n            )\n            assert isinstance(response.content, str)\n            # Extract the answer from the response.\n            answer = self._extract_answer(response.content)\n            # Create a review task.\n            review_task = ReviewTask(\n                session_id=session_id,\n                writing_task=message.task,\n                answer_scratchpad=response.content,\n                answer=answer,\n            )\n            # Store the review task in the session memory.\n            self._session_memory[session_id].append(review_task)\n            # Publish a review task.\n            await self.publish_message(review_task, topic_id=TopicId(\"default\", self.id.key))\n\n        @message_handler\n        async def handle_review_result(self, message: ReviewResult, ctx: MessageContext) -> None:\n            # Store the review result in the session memory.\n            self._session_memory[message.session_id].append(message)\n            # Obtain the request from previous messages.\n            review_request = next(\n                m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, ReviewTask)\n            )\n            assert review_request is not None\n            # Check if the is approved.\n            if message.approved:\n                # Publish the writing result.\n                await self.publish_message(\n                    WritingResult(\n                        answer=review_request.answer,\n                        task=review_request.writing_task,\n                        review=message.review,\n                    ),\n                    topic_id=TopicId(\"result\", self.id.key),\n                )\n                print(\"Writing Result:\")\n                print(\"-\" * 80)\n                print(f\"Task:\\n{review_request.writing_task}\")\n                print(\"-\" * 80)\n                print(f\"Answer:\\n{review_request.answer}\")\n                print(\"-\" * 80)\n                print(f\"Review:\\n{message.review}\")\n                print(\"-\" * 80)\n            else:\n                # Create a list of LLM messages to send to the model.\n                messages: List[LLMMessage] = [*self._system_messages]\n                for m in self._session_memory[message.session_id]:\n                    if isinstance(m, ReviewResult):\n                        messages.append(UserMessage(content=m.review, source=\"Reviewer\"))\n                    elif isinstance(m, ReviewTask):\n                        messages.append(AssistantMessage(content=m.answer_scratchpad, source=\"Worker\"))\n                    elif isinstance(m, WritingTask):\n                        messages.append(UserMessage(content=m.task, source=\"User\"))\n                    else:\n                        raise ValueError(f\"Unexpected message type: {m}\")\n                # Generate a revision using the chat completion API.\n                response = await self._model_client.create(messages, cancellation_token=ctx.cancellation_token)\n                assert isinstance(response.content, str)\n                # Extract the answer from the response.\n                answer = self._extract_answer(response.content)\n                # Create a new review task.\n                review_task = ReviewTask(\n                    session_id=message.session_id,\n                    writing_task=review_request.writing_task,\n                    answer_scratchpad=response.content,\n                    answer=answer,\n                )\n                # Store the review task in the session memory.\n                self._session_memory[message.session_id].append(review_task)\n                # Publish a new review task.\n                await self.publish_message(review_task, topic_id=TopicId(\"default\", self.id.key))\n\n\n        def _extract_answer(self, text: str) -> Union[str, None]:\n            pattern = \"(?<=Answer: ).*\"\n            # Search for the pattern in the markdown text\n            match = re.search(pattern, text, re.DOTALL)\n            # Extract the language and code block if a match is found\n            if match:\n                return match.group(0)\n            return None\n\n    @default_subscription\n    class ReviewerAgent(RoutedAgent):\n        \"\"\"An agent that critiques tasks.\"\"\"\n\n        def __init__(self, model_client: ChatCompletionClient) -> None:\n            super().__init__(\"A critic agent.\")\n            self._system_messages: List[LLMMessage] = [\n                SystemMessage(\n                    content=\"\"\"You are a critic. Review answers and criticize on where it might be wrong.\n    Respond using the following JSON format:\n    {\n        \"correctness\": \"<Your comments>\",\n        \"approval\": \"<APPROVE or REVISE>\",\n        \"suggested_changes\": \"<Your comments>\"\n    }\n    \"\"\",\n                )\n            ]\n            self._session_memory: Dict[str, List[ReviewTask | ReviewResult]] = {}\n            self._model_client = model_client\n\n        @message_handler\n        async def handle_review_task(self, message: ReviewTask, ctx: MessageContext) -> None:\n            # Format the prompt for the review.\n            # Gather the previous feedback if available.\n            previous_feedback = \"\"\n            if message.session_id in self._session_memory:\n                previous_review = next(\n                    (m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, ReviewResult)),\n                    None,\n                )\n                if previous_review is not None:\n                    previous_feedback = previous_review.review\n            # Store the messages in a temporary memory for this request only.\n            self._session_memory.setdefault(message.session_id, []).append(message)\n            prompt = f\"\"\"The problem statement is: {message.writing_task}\n    The answer is:\n    ```\n    {message.answer}\n    ```\n\n    Previous feedback:\n    {previous_feedback}\n\n    Please review the answer. If previous feedback was provided, see if it was addressed.\n    \"\"\"\n            # Generate a response using the chat completion API.\n            response = await self._model_client.create(\n                self._system_messages + [UserMessage(content=prompt, source=self.metadata[\"type\"])],\n                cancellation_token=ctx.cancellation_token,\n                json_output=True,\n            )\n            assert isinstance(response.content, str)\n            # TODO: use structured generation library e.g. guidance to ensure the response is in the expected format.\n            # Parse the response JSON.\n            review = json.loads(response.content)\n            # Construct the review text.\n            review_text = \"Review:\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in review.items()])\n            approved = review[\"approval\"].lower().strip() == \"approve\"\n            result = ReviewResult(\n                review=review_text,\n                session_id=message.session_id,\n                approved=approved,\n            )\n            # Store the review result in the session memory.\n            self._session_memory[message.session_id].append(result)\n            # Publish the review result.\n            await self.publish_message(result, topic_id=TopicId(\"default\", self.id.key))\n\n\n    # Define the main function to set up and run the agent system\n    async def main():\n        # Create a queue to collect final answer\n        queue = asyncio.Queue[WritingResult]()\n        async def output_result(_runtime: AgentRuntime, id: AgentId, message: WritingResult, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize the agent runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Create agents\n        await ReviewerAgent.register(\n            runtime, \"ReviewerAgent\", lambda: ReviewerAgent(model_client=model_client)\n        )\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        await WorkerAgent.register(\n            runtime, \"WorkerAgent\", lambda: WorkerAgent(model_client=model_client, instruction=cot_instruction)\n        )\n        # Create closure agent to collect final output result\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register(runtime, \"output_result\", output_result, subscriptions=lambda: [result_topic])\n\n        # Start the runtime, and publish the first message\n        runtime.start()\n        await runtime.publish_message(\n            message=WritingTask(task=task),\n            topic_id=DefaultTopicId(),\n        )\n\n        # Keep processing messages until idle.\n        await runtime.stop_when_idle()\n\n        # Return the first answer from the queue\n        print(f\"queue {queue}\")\n        return (await queue.get()).answer\n    \n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (34.7%, 38.3%), Median: 46.0%"
    },
    {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import json\n    import logging\n    import re\n    import sys\n    import uuid\n    from dataclasses import dataclass\n    from typing import Dict, List, Union\n    from autogen_core.base import MessageContext, TopicId, AgentId, AgentRuntime\n    from autogen_core.components import RoutedAgent, default_subscription, message_handler, TypeSubscription\n    from autogen_core.components.models import (\n        AssistantMessage,\n        ChatCompletionClient,\n        LLMMessage,\n        SystemMessage,\n        UserMessage,\n    )\n    from autogen_core.application import SingleThreadedAgentRuntime\n    from autogen_core.components import DefaultTopicId, RoutedAgent, message_handler, ClosureAgent\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n    \n    @dataclass\n    class Question:\n        content: str\n\n\n    @dataclass\n    class Answer:\n        content: str\n\n\n    @dataclass\n    class SolverRequest:\n        content: str\n        question: str\n\n\n    @dataclass\n    class IntermediateSolverResponse:\n        content: str\n        question: str\n        answer: str\n        round: int\n\n\n    @dataclass\n    class FinalSolverResponse:\n        answer: str\n\n    @default_subscription\n    class Solver(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient, topic_type: str, num_neighbors: int, max_round: int) -> None:\n            super().__init__(\"A debator.\")\n            self._topic_type = topic_type\n            self._model_client = model_client\n            self._num_neighbors = num_neighbors\n            self._history: List[LLMMessage] = []\n            self._buffer: Dict[int, List[IntermediateSolverResponse]] = {}\n            self._system_messages = [\n                SystemMessage(\n                    (\n                        \"You are a helpful assistant with expertise in reasoning. \"\n                        \"Your task is to assist in solving a reasoning problem by providing \"\n                        \"a clear and detailed solution. Limit your output within 100 words, \"\n                        \"and your final answer should be a single string.\"\n                    )\n                )\n            ]\n            self._round = 0\n            self._max_round = max_round\n\n        @message_handler\n        async def handle_request(self, message: SolverRequest, ctx: MessageContext) -> None:\n            # Add the question to the memory.\n            self._history.append(UserMessage(content=message.content, source=\"user\"))\n            # Make an inference using the model.\n            model_result = await self._model_client.create(self._system_messages + self._history)\n            assert isinstance(model_result.content, str)\n            # Add the response to the memory.\n            self._history.append(AssistantMessage(content=model_result.content, source=self.metadata[\"type\"]))\n            print(f\"{'-'*80}\\nSolver {self.id} round {self._round}:\\n{model_result.content}\")\n            # Increment the counter.\n            self._round += 1\n            if self._round == self._max_round:\n                # If the counter reaches the maximum round, publishes a final response.\n                await self.publish_message(FinalSolverResponse(answer=model_result.content), topic_id=DefaultTopicId())\n            else:\n                # Publish intermediate response to the topic associated with this solver.\n                print(\"publish IntermediateSolverResponse\")\n                await self.publish_message(\n                    IntermediateSolverResponse(\n                        content=model_result.content,\n                        question=message.question,\n                        answer=model_result.content,\n                        round=self._round,\n                    ),\n                    topic_id=DefaultTopicId(type=self._topic_type),\n                )\n\n        @message_handler\n        async def handle_response(self, message: IntermediateSolverResponse, ctx: MessageContext) -> None:\n            # Add neighbor's response to the buffer.\n            self._buffer.setdefault(message.round, []).append(message)\n            # Check if all neighbors have responded.\n            if len(self._buffer[message.round]) == self._num_neighbors:\n                print(\n                    f\"{'-'*80}\\nSolver {self.id} round {message.round}:\\nReceived all responses from {self._num_neighbors} neighbors.\"\n                )\n                # Prepare the prompt for the next question.\n                prompt = \"These are the solutions to the problem from other agents:\\n\"\n                for resp in self._buffer[message.round]:\n                    prompt += f\"One agent solution: {resp.content}\\n\"\n                prompt += (\n                    \"Using the solutions from other agents as additional information, \"\n                    \"can you provide your answer to the problem? \"\n                    f\"The original problem is {message.question}. \"\n                    \"Your final answer should be a single string.\"\n                )\n                # Send the question to the agent itself to solve.\n                await self.send_message(SolverRequest(content=prompt, question=message.question), self.id)\n                # Clear the buffer.\n                self._buffer.pop(message.round)\n\n\n    @default_subscription\n    class Aggregator(RoutedAgent):\n        def __init__(self, num_solvers: int) -> None:\n            super().__init__(\"Aggregator\")\n            self._num_solvers = num_solvers\n            self._buffer: List[FinalSolverResponse] = []\n\n        @message_handler\n        async def handle_question(self, message: Question, ctx: MessageContext) -> None:\n            print(f\"{'-'*80}\\nAggregator {self.id} received question:\\n{message.content}\")\n            prompt = (\n                f\"Can you solve the following problem?\\n{message.content}\\n\"\n                \"Explain your reasoning. Your final answer should be a single string.\"\n            )\n            print(f\"{'-'*80}\\nAggregator {self.id} publishes initial solver request.\")\n            await self.publish_message(SolverRequest(content=prompt, question=message.content), topic_id=DefaultTopicId())\n\n        @message_handler\n        async def handle_final_solver_response(self, message: FinalSolverResponse, ctx: MessageContext) -> None:\n            self._buffer.append(message)\n            if len(self._buffer) == self._num_solvers:\n                print(f\"{'-'*80}\\nAggregator {self.id} received all final answers from {self._num_solvers} solvers.\")\n                # Find the majority answer.\n                answers = [resp.answer for resp in self._buffer]\n                majority_answer = max(set(answers), key=answers.count)\n                # Publish the aggregated response.\n                await self.publish_message(Answer(content=majority_answer), topic_id=TopicId(\"result\", self.id.key))\n                # Clear the responses.\n                self._buffer.clear()\n                print(f\"{'-'*80}\\nAggregator {self.id} publishes final answer:\\n{majority_answer}\")\n\n\n    # Define the main function to set up and run the agent system\n    async def main():\n        queue = asyncio.Queue[Answer]()\n        async def output_result(_runtime: AgentRuntime, id: AgentId, message: Answer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        runtime = SingleThreadedAgentRuntime()\n        await Solver.register(\n            runtime,\n            \"SolverA\",\n            lambda: Solver(\n                model_client=model_client,\n                topic_type=\"SolverA\",\n                num_neighbors=2,\n                max_round=3,\n            ),\n        )\n        await Solver.register(\n            runtime,\n            \"SolverB\",\n            lambda: Solver(\n                model_client=model_client,\n                topic_type=\"SolverB\",\n                num_neighbors=2,\n                max_round=3,\n            ),\n        )\n        await Solver.register(\n            runtime,\n            \"SolverC\",\n            lambda: Solver(\n                model_client=model_client,\n                topic_type=\"SolverC\",\n                num_neighbors=2,\n                max_round=3,\n            ),\n        )\n        await Solver.register(\n            runtime,\n            \"SolverD\",\n            lambda: Solver(\n                model_client=model_client,\n                topic_type=\"SolverD\",\n                num_neighbors=2,\n                max_round=3,\n            ),\n        )\n        await Aggregator.register(runtime, \"Aggregator\", lambda: Aggregator(num_solvers=4))\n\n        # Subscriptions for topic published to by SolverA.\n        await runtime.add_subscription(TypeSubscription(\"SolverA\", \"SolverD\"))\n        await runtime.add_subscription(TypeSubscription(\"SolverA\", \"SolverB\"))\n\n        # Subscriptions for topic published to by SolverB.\n        await runtime.add_subscription(TypeSubscription(\"SolverB\", \"SolverA\"))\n        await runtime.add_subscription(TypeSubscription(\"SolverB\", \"SolverC\"))\n\n        # Subscriptions for topic published to by SolverC.\n        await runtime.add_subscription(TypeSubscription(\"SolverC\", \"SolverB\"))\n        await runtime.add_subscription(TypeSubscription(\"SolverC\", \"SolverD\"))\n\n        # Subscriptions for topic published to by SolverD.\n        await runtime.add_subscription(TypeSubscription(\"SolverD\", \"SolverC\"))\n        await runtime.add_subscription(TypeSubscription(\"SolverD\", \"SolverA\"))\n\n        # All solvers and the aggregator subscribe to the default topic.\n\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register(runtime, \"output_result\", output_result, subscriptions=lambda: [result_topic])\n\n        runtime.start()\n        await runtime.publish_message(Question(content=task), DefaultTopicId())\n\n        # Keep processing messages until idle.\n        await runtime.stop_when_idle()\n\n        # Return the answer from the queue\n        res = (await queue.get()).content\n        print(f\"res {res}\")\n        return res\n\n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (77.2%, 80.6%), Median: 87.3%"
    },
    {
        "thought": "By using a tree search strategy, the model can explore multiple branches of thoughts, where at any step of the problem, multiple independent thoughts are generated and evaluated to find the most useful ones.",
        "name": "Tree of Thought",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import logging\n    from dataclasses import dataclass\n    from typing import List, Dict, Any\n    from autogen_core.application import SingleThreadedAgentRuntime\n    from autogen_core.base import AgentId, AgentRuntime, MessageContext, TopicId\n    from autogen_core.components import default_subscription, RoutedAgent, message_handler, ClosureAgent, TypeSubscription, DefaultTopicId\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        SystemMessage,\n        UserMessage,\n        AssistantMessage,\n        LLMMessage,\n    )\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n    from autogen_core.application.logging import TRACE_LOGGER_NAME \n\n    # Configure logging as per documentation \n    logging.basicConfig(level=logging.WARNING) \n    logger = logging.getLogger(TRACE_LOGGER_NAME) \n    logger.setLevel(logging.INFO)\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs['model'],\n        api_version=model_client_kwargs['api_version'],\n        azure_endpoint=model_client_kwargs['azure_endpoint'],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n \n    @dataclass \n    class Message: \n        content: str \n    \n    @dataclass \n    class FinalAnswer: \n        answer: str \n\n    @default_subscription\n    class TreeOfThoughtsAgent(RoutedAgent): \n        def __init__(self, model_client: ChatCompletionClient, max_depth: int = 3, beam_width: int = 3): \n            super().__init__(\"TreeOfThoughtsAgent\") \n            self._model_client = model_client \n            self._max_depth = max_depth \n            self._beam_width = beam_width \n            self._system_messages = [ \n                SystemMessage( \n                    content=\"You are a helpful assistant who reasons step-by-step to solve complex problems.\") \n            ] \n    \n        async def generate_thoughts(self, prompt: List[LLMMessage], num_thoughts: int, cancellation_token) -> List[str]: \n            # Generate multiple thoughts using the model \n            thoughts = [] \n            # Create multiple async tasks to generate thoughts in parallel \n            tasks = [] \n            for _ in range(num_thoughts): \n                tasks.append(self._model_client.create( \n                    prompt, \n                    extra_create_args={\"temperature\": 1.0},\n                    cancellation_token=cancellation_token, \n                )) \n            responses = await asyncio.gather(*tasks) \n            for response in responses: \n                thoughts.append(response.content.strip()) \n            return thoughts \n    \n        async def evaluate_thoughts(self, thoughts: List[str], ctx: MessageContext) -> List[str]: \n            # Batch evaluation of thoughts \n            eval_prompt = [ \n                SystemMessage(content=\"You are an assistant that evaluates reasoning steps for solving a problem.\"), \n                UserMessage( \n                    content=f\"Evaluate the following thoughts for their usefulness in solving the problem. Rank them from most useful to least useful and provide the rankings.\\n\\nThoughts:\\n\" + \"\\n\".join( \n                        [f\"{i+1}. {t}\" for i, t in enumerate(thoughts)]), \n                    source=\"user\" \n                ) \n            ] \n            eval_response = await self._model_client.create( \n                eval_prompt, \n                cancellation_token=ctx.cancellation_token, \n            ) \n            # Parse the response to extract rankings \n            rankings_text = eval_response.content.strip() \n            # For simplicity, assume the model outputs the rankings as a list of numbers \n            rankings = [] \n            for line in rankings_text.split('\\n'): \n                line = line.strip() \n                if line and line[0].isdigit(): \n                    rankings.append(int(line[0]) - 1)  # Subtract 1 to get index \n            # Select top-k thoughts \n            best_thoughts = [thoughts[i] for i in rankings[:self._beam_width]] \n            return best_thoughts \n    \n        @message_handler \n        async def handle_message(self, message: Message, ctx: MessageContext) -> None: \n            logger.info(f\"Received task: {message.content}\") \n            initial_prompt = self._system_messages + [UserMessage(content=message.content, source=\"user\")] \n            tree = [[]]  # Initialize the tree with an empty path \n            for depth in range(self._max_depth): \n                new_branches = [] \n                logger.info(f\"Depth {depth+1}\") \n                for path in tree: \n                    # Build the prompt up to this point \n                    prompt = initial_prompt.copy() \n                    for thought in path: \n                        prompt.append(AssistantMessage(content=thought, source=\"assistant\")) \n                    # Generate thoughts \n                    thoughts = await self.generate_thoughts(prompt, self._beam_width, ctx.cancellation_token) \n                    logger.info(f\"Generated thoughts: {thoughts}\") \n                    # Evaluate thoughts \n                    best_thoughts = await self.evaluate_thoughts(thoughts, ctx) \n                    logger.info(f\"Best thoughts: {best_thoughts}\") \n                    # Expand tree with best thoughts \n                    for thought in best_thoughts: \n                        new_path = path + [thought] \n                        new_branches.append(new_path) \n                # Update tree with new branches \n                if not new_branches: \n                    logger.info(\"No more branches to expand.\") \n                    break  # No more thoughts to expand \n                tree = new_branches \n            # After reaching max depth, select the best path \n            # For simplicity, select the first path \n            best_path = tree[0] \n            final_answer = best_path[-1] \n            logger.info(f\"Final answer: {final_answer}\") \n            # Publish the final answer \n            await self.publish_message( \n                FinalAnswer(answer=final_answer), \n                topic_id=TopicId(type=\"result\", source=self.id.key) \n            ) \n    \n    # Main function \n    async def main(): \n        # Create a queue to collect the final answer \n        queue = asyncio.Queue[FinalAnswer]() \n    \n        async def output_result(_runtime: AgentRuntime, id: AgentId, message: FinalAnswer, ctx: MessageContext) -> None: \n            await queue.put(message) \n    \n        # Initialize runtime \n        runtime = SingleThreadedAgentRuntime() \n    \n        # Register TreeOfThoughtsAgent \n        await TreeOfThoughtsAgent.register( \n            runtime, \n            \"TreeOfThoughtsAgent\", \n            lambda: TreeOfThoughtsAgent(model_client) \n        ) \n    \n        # Register ClosureAgent with agent key matching self.id.key (default is \"default\") \n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\") \n        await ClosureAgent.register( \n            runtime, \n            \"output_result\", \n            output_result, \n            subscriptions=lambda: [result_topic] \n        ) \n    \n        # Start the runtime \n        runtime.start() \n    \n        # Publish initial message to TreeOfThoughtsAgent\n        await runtime.publish_message( \n            Message(content=task), \n            topic_id=DefaultTopicId() \n        ) \n    \n        # Wait until idle \n        await runtime.stop_when_idle() \n    \n        # Return the final answer \n        final_message = await queue.get() \n        return final_message.answer\n    return asyncio.run(main())\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (32.7%, 37.2%), Median: 46.2%"
    },
    {
        "thought": "**Insights:**\nIntegrating tool use into the agent's reasoning process through the ReAct framework can enhance performance on tasks requiring complex computations. By interleaving reasoning steps with actions, the agent can utilize external tools like a calculator to perform tasks beyond the capabilities of the language model alone.\n\n**Overall Idea:**\nImplement a `ReActAgent` that uses chain-of-thought reasoning interleaved with tool use. The agent decides when to call tools (e.g., a calculator) during its reasoning process. A `ToolAgent` will execute the requested tool actions. The `ReActAgent` will communicate with the `ToolAgent` via direct messages. The final answer will be published to a 'result' topic, and a `ClosureAgent` will collect the final answer.\n\n**Implementation:**\n- Use the `@default_subscription` decorator for the `ReActAgent` to subscribe to the default topic.\n- Modify the `ReActAgent` to use `tool_agent_caller_loop` for tool interactions.\n- Adjust the `ClosureAgent` to subscribe to `TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")`.\n- Ensure that the agents communicate properly and that the message handling follows the framework's guidelines.",
        "name": "ReAct (Reasoning and Acting)",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import logging\n    from dataclasses import dataclass\n    from typing import List\n    from autogen_core.application import SingleThreadedAgentRuntime\n    from autogen_core.base import AgentId, AgentRuntime, MessageContext, TopicId\n    from autogen_core.components import RoutedAgent, message_handler, ClosureAgent, default_subscription, TypeSubscription, DefaultTopicId\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        SystemMessage,\n        UserMessage,\n        AssistantMessage,\n        LLMMessage,\n    )\n    from autogen_core.components.tool_agent import ToolAgent, tool_agent_caller_loop\n    from autogen_core.components.tools import FunctionTool\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    # Set up Azure OpenAI credentials\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create the AzureOpenAI model client\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"function_calling\": True,\n            \"vision\": True,\n            \"json_output\": True,\n        },\n    )\n\n    # Message class\n    @dataclass\n    class Message:\n        content: str\n\n    # Calculator tool\n    async def calculator(expression: str) -> str:\n        try:\n            result = eval(expression)\n            return str(result)\n        except Exception as e:\n            return \"Error: \" + str(e)\n\n    calculator_tool = FunctionTool(\n        func=calculator,\n        description=\"A calculator that can evaluate mathematical expressions.\",\n    )\n\n    # ReActAgent\n    @default_subscription\n    class ReActAgent(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient, tool_agent_id: AgentId, tools: List[FunctionTool]):\n            super().__init__(\"ReAct Agent\")\n            self._model_client = model_client\n            self._tool_agent_id = tool_agent_id\n            self._tools = tools\n            self._system_messages = [\n                SystemMessage(\n                    content=(\n                        \"You are an intelligent agent that can reason and act. \"\n                        \"Use tools to perform actions when necessary. \"\n                        \"You should use the following format:\\n\\n\"\n                        \"Thought: what you think\\n\"\n                        \"Action: the action to perform, must be one of [calculator]\\n\"\n                        \"Action Input: the input to the action\\n\"\n                        \"Observation: the result of the action\\n\"\n                        \"... (this Thought/Action/Action Input/Observation can repeat N times)\\n\"\n                        \"Thought: I now know the final answer\\n\"\n                        \"Answer: the final answer to the original question\"\n                    )\n                )\n            ]\n            self._conversation: List[LLMMessage] = []\n\n        @message_handler\n        async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n            user_message = UserMessage(content=message.content, source=\"user\")\n            self._conversation.append(user_message)\n\n            # Use tool_agent_caller_loop to manage tool calls\n            messages = await tool_agent_caller_loop(\n                self,\n                tool_agent_id=self._tool_agent_id,\n                model_client=self._model_client,\n                input_messages=self._conversation,\n                tool_schema=[tool.schema for tool in self._tools],\n                cancellation_token=ctx.cancellation_token,\n            )\n\n            # Extract final answer from the last assistant message\n            final_answer = messages[-1].content\n\n            # Publish final answer to 'result' topic with topic_source 'output_result'\n            await self.publish_message(\n                Message(content=final_answer),\n                topic_id=TopicId(\"result\", \"output_result\"),\n            )\n\n    # Main function\n    async def main():\n        # Create a queue to collect the final answer\n        queue = asyncio.Queue[Message]()\n\n        async def output_result(_runtime: AgentRuntime, id: AgentId, message: Message, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Register ToolAgent\n        tools = [calculator_tool]\n        await ToolAgent.register(\n            runtime,\n            \"ToolAgent\",\n            lambda: ToolAgent(\"Tool Agent\", tools),\n        )\n\n        # Register ReActAgent\n        tool_agent_id = AgentId(\"ToolAgent\", \"default\")\n        await ReActAgent.register(\n            runtime,\n            \"ReActAgent\",\n            lambda: ReActAgent(model_client, tool_agent_id, tools),\n        )\n\n        # Register ClosureAgent with corrected subscription\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register(runtime, \"output_result\", output_result, subscriptions=lambda: [result_topic])\n\n        # Start the runtime\n        runtime.start()\n\n        # Publish initial message to ReActAgent\n        await runtime.publish_message(\n            Message(content=task),\n            topic_id=DefaultTopicId(),\n        )\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer\n        final_message = await queue.get()\n        return final_message.content\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (10.7%, 12.2%), Median: 15.9%",
        "generation": 4
    },
    {
        "thought": "We will improve the 'Tree of Thought' agent by enhancing the thought evaluation process to make it more robust and by refining the final answer selection mechanism to choose the best path based on cumulative evaluation scores. Specifically, we'll modify the `evaluate_thoughts` method to have the model output JSON-formatted evaluations for each thought, ensuring reliable parsing. We'll also keep track of cumulative scores for each path and select the one with the highest total score.",
        "name": "Improved Tree of Thought",
        "code": "def forward(self, task, model_client_kwargs):\n    import asyncio\n    import logging\n    from dataclasses import dataclass\n    from typing import List, Dict, Any\n    from autogen_core.application import SingleThreadedAgentRuntime\n    from autogen_core.base import AgentId, AgentRuntime, MessageContext, TopicId\n    from autogen_core.components import default_subscription, RoutedAgent, message_handler, ClosureAgent, TypeSubscription, DefaultTopicId\n    from autogen_core.components.models import (\n        ChatCompletionClient,\n        SystemMessage,\n        UserMessage,\n        AssistantMessage,\n        LLMMessage,\n    )\n    from autogen_ext.models import AzureOpenAIChatCompletionClient\n    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n    from autogen_core.application.logging import TRACE_LOGGER_NAME\n\n    # Configure logging as per documentation\n    logging.basicConfig(level=logging.WARNING)\n    logger = logging.getLogger(TRACE_LOGGER_NAME)\n    logger.setLevel(logging.INFO)\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n\n    # Create an AzureOpenAI model client.\n    model_client = AzureOpenAIChatCompletionClient(\n        model=model_client_kwargs[\"model\"],\n        api_version=model_client_kwargs[\"api_version\"],\n        azure_endpoint=model_client_kwargs[\"azure_endpoint\"],\n        azure_ad_token_provider=token_provider,\n        model_capabilities={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n        },\n    )\n\n    @dataclass\n    class Message:\n        content: str\n\n    @dataclass\n    class FinalAnswer:\n        answer: str\n\n    @default_subscription\n    class TreeOfThoughtsAgent(RoutedAgent):\n        def __init__(self, model_client: ChatCompletionClient, max_depth: int = 3, beam_width: int = 3):\n            super().__init__(\"TreeOfThoughtsAgent\")\n            self._model_client = model_client\n            self._max_depth = max_depth\n            self._beam_width = beam_width\n            self._system_messages = [\n                SystemMessage(\n                    content=\"You are a helpful assistant who reasons step-by-step to solve complex problems.\")\n            ]\n\n        async def generate_thoughts(self, prompt: List[LLMMessage], num_thoughts: int, cancellation_token) -> List[str]:\n            # Generate multiple thoughts using the model\n            thoughts = []\n            # Create multiple async tasks to generate thoughts in parallel\n            tasks = []\n            for _ in range(num_thoughts):\n                tasks.append(self._model_client.create(\n                    prompt,\n                    extra_create_args={\"temperature\": 1.0},\n                    cancellation_token=cancellation_token,\n                ))\n            responses = await asyncio.gather(*tasks)\n            for response in responses:\n                thoughts.append(response.content.strip())\n            return thoughts\n\n        async def evaluate_thoughts(self, thoughts: List[str], ctx: MessageContext) -> List[Dict[str, Any]]:\n            # Evaluate thoughts with the model outputting JSON-formatted scores\n            eval_prompt = [\n                SystemMessage(content=\"You are an assistant that evaluates reasoning steps for solving a problem.\"),\n                UserMessage(\n                    content=(\n                        \"Evaluate the following thoughts for their usefulness in solving the problem. \"\n                        \"Provide a JSON array of objects with 'thought' and 'score' (from 1 to 10).\\n\\nThoughts:\\n\" + \"\\n\".join(\n                            [f\"- {t}\" for t in thoughts])\n                    ),\n                    source=\"user\"\n                )\n            ]\n            eval_response = await self._model_client.create(\n                eval_prompt,\n                cancellation_token=ctx.cancellation_token,\n            )\n            # Parse the JSON response\n            import json\n            try:\n                evaluations = json.loads(eval_response.content.strip())\n                # Each evaluation should be a dict with 'thought' and 'score'\n                return evaluations\n            except json.JSONDecodeError:\n                # If parsing fails, assign default scores\n                return [{\"thought\": t, \"score\": 5} for t in thoughts]\n\n        @message_handler\n        async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n            logger.info(f\"Received task: {message.content}\")\n            initial_prompt = self._system_messages + [UserMessage(content=message.content, source=\"user\")]\n            tree = [[{\"thought\": \"\", \"score\": 0, \"cumulative_score\": 0}]]  # Initialize the tree with an empty path\n            for depth in range(self._max_depth):\n                new_branches = []\n                logger.info(f\"Depth {depth+1}\")\n                for path in tree:\n                    # Build the prompt up to this point\n                    prompt = initial_prompt.copy()\n                    for node in path:\n                        if node[\"thought\"]:\n                            prompt.append(AssistantMessage(content=node[\"thought\"], source=\"assistant\"))\n                    # Generate thoughts\n                    thoughts = await self.generate_thoughts(prompt, self._beam_width, ctx.cancellation_token)\n                    logger.info(f\"Generated thoughts: {thoughts}\")\n                    # Evaluate thoughts\n                    evaluations = await self.evaluate_thoughts(thoughts, ctx)\n                    logger.info(f\"Evaluations: {evaluations}\")\n                    # Expand tree with evaluated thoughts\n                    for eval in evaluations:\n                        new_path = path + [{\n                            \"thought\": eval[\"thought\"],\n                            \"score\": eval[\"score\"],\n                            \"cumulative_score\": path[-1][\"cumulative_score\"] + eval[\"score\"]\n                        }]\n                        new_branches.append(new_path)\n                # Select top-k paths based on cumulative_score\n                if not new_branches:\n                    logger.info(\"No more branches to expand.\")\n                    break  # No more thoughts to expand\n                # Sort paths by cumulative score\n                new_branches.sort(key=lambda p: p[-1][\"cumulative_score\"], reverse=True)\n                tree = new_branches[:self._beam_width]\n            # After reaching max depth, select the best path\n            best_path = tree[0]\n            final_answer = best_path[-1][\"thought\"]\n            logger.info(f\"Final answer: {final_answer}\")\n            # Publish the final answer to topic_id=TopicId(type=\"result\", source=\"default\")\n            await self.publish_message(\n                FinalAnswer(answer=final_answer),\n                topic_id=TopicId(type=\"result\", source=\"default\")\n            )\n\n    # Main function\n    async def main():\n        # Create a queue to collect the final answer\n        queue = asyncio.Queue()\n\n        async def output_result(_runtime: AgentRuntime, id: AgentId, message: FinalAnswer, ctx: MessageContext) -> None:\n            await queue.put(message)\n\n        # Initialize runtime\n        runtime = SingleThreadedAgentRuntime()\n\n        # Register TreeOfThoughtsAgent\n        await TreeOfThoughtsAgent.register(\n            runtime,\n            \"TreeOfThoughtsAgent\",\n            lambda: TreeOfThoughtsAgent(model_client)\n        )\n\n        # Register ClosureAgent\n        result_topic = TypeSubscription(topic_type=\"result\", agent_type=\"output_result\")\n        await ClosureAgent.register(\n            runtime,\n            \"output_result\",\n            output_result,\n            subscriptions=lambda: [result_topic]\n        )\n\n        # Start the runtime\n        runtime.start()\n\n        # Publish initial message to TreeOfThoughtsAgent\n        await runtime.publish_message(\n            Message(content=task),\n            topic_id=DefaultTopicId()\n        )\n\n        # Wait until idle\n        await runtime.stop_when_idle()\n\n        # Return the final answer\n        final_message = await queue.get()\n        return final_message.answer\n\n    return asyncio.run(main())",
        "fitness": "95% Bootstrap Confidence Interval: (35.0%, 39.6%), Median: 48.6%",
        "generation": 6
    }
]